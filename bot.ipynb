{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading file art_writing_content.txt: 'utf-8' codec can't decode byte 0xd3 in position 10: invalid continuation byte\n"
     ]
    }
   ],
   "source": [
    "from Gpt import file_detcher\n",
    "data_array = file_detcher()\n",
    "data_Call = []\n",
    "# Example usage:\n",
    "for data in data_array:\n",
    "    data_Call.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MAKE-A-VIDEO:\\nTEXT-TO-VIDEO\\nGENERATION\\nWITHOUT TEXT-VIDEO DATA\\nUriel Singer +\\nAdam Polyak +\\nThomas Hayes +\\nXi Yin +\\nJie An\\nSongyang Zhang\\nQiyuan Hu\\nHarry Yang\\nOron Ashual\\nOran Gafni\\nDevi Parikh +\\nSonal Gupta +\\nYaniv Taigman +\\nMeta AI\\nABSTRACT\\nWe propose Make-A-Video – an approach for directly translating the tremendous\\nrecent progress in Text-to-Image (T2I) generation to Text-to-Video (T2V). Our\\nintuition is simple: learn what the world looks like and how it is described from\\npaired text-image data, and learn how the world moves from unsupervised video\\nfootage. Make-A-Video has three advantages: (1) it accelerates training of the\\nT2V model (it does not need to learn visual and multimodal representations from\\nscratch), (2) it does not require paired text-video data, and (3) the generated\\nvideos inherit the vastness (diversity in aesthetic, fantastical depictions, etc.)\\nof today’s image generation models. We design a simple yet effective way to\\nbuild on T2I models with novel and effective spatial-temporal modules. First, we\\ndecompose the full temporal U-Net and attention tensors and approximate them\\nin space and time. Second, we design a spatial temporal pipeline to generate\\nhigh resolution and frame rate videos with a video decoder, interpolation model\\nand two super resolution models that can enable various applications besides\\nT2V. In all aspects, spatial and temporal resolution, faithfulness to text, and\\nquality, Make-A-Video sets the new state-of-the-art in text-to-video generation,\\nas determined by both qualitative and quantitative measures.\\n1\\nINTRODUCTION\\nThe Internet has fueled collecting billions of (alt-text, image) pairs from HTML pages (Schuhmann\\net al., 2022), enabling the recent breakthroughs in Text-to-Image (T2I) modeling. However, repli-\\ncating this success for videos is limited since a similarly sized (text, video) dataset cannot be easily\\ncollected. It would be wasteful to train Text-to-Video (T2V) models from scratch when there already\\nexist models that can generate images. Moreover, unsupervised learning enables networks to learn\\nfrom orders of magnitude more data. This large quantity of data is important to learn representa-\\ntions of more subtle, less common concepts in the world. Unsupervised learning has long had great\\nsuccess in advancing the ﬁeld of natural language processing (NLP) (Liu et al., 2019a; Brown et al.,\\n2020). Models pre-trained this way yield considerably higher performance than when solely trained\\nin a supervised manner.\\nInspired by these motivations, we propose Make-A-Video. Make-A-Video leverages T2I models\\nto learn the correspondence between text and the visual world, and uses unsupervised learning on\\nunlabeled (unpaired) video data, to learn realistic motion. Together, Make-A-Video generates videos\\nfrom text without leveraging paired text-video data.\\nClearly, text describing images does not capture the entirety of phenomena observed in videos. That\\nsaid, one can often infer actions and events from static images (e.g. a woman drinking coffee, or an\\n+ Core Contributors. Corresponding author: urielsinger@meta.com. Jie and Songyang are from\\nUniversity of Rochester (work done during internship at Meta).\\n1\\narXiv:2209.14792v1  [cs.CV]  29 Sep 2022\\n(a) A dog wearing a superhero outfit with red cape flying through the sky. \\n(b) There is a table by a window with sunlight streaming through illuminating a pile of books. \\n(c) Robot dancing in times square.\\n(d) Unicorns running along a beach, highly detailed.\\nFigure 1: T2V generation examples. Our model can generate high-quality videos with coherent\\nmotion for a diverse set of visual concepts. In example (a), there are large and realistic motion for\\nthe dog. In example (b), the books are almost static but the scene changes with the camera motion.\\nVideo samples are available at make-a-video.github.io\\nelephant kicking a football) as done in image-based action recognition systems (Girish et al., 2020).\\nMoreover, even without text descriptions, unsupervised videos are sufﬁcient to learn how different\\nentities in the world move and interact (e.g. the motion of waves at the beach, or of an elephant’s\\ntrunk). As a result, a model that has only seen text describing images is surprisingly effective at\\ngenerating short videos, as demonstrated by our temporal diffusion-based method. Make-A-Video\\nsets the new state-of-the-art in T2V generation.\\nUsing function-preserving transformations, we extend the spatial layers at the model initialization\\nstage, to include temporal information. The extended spatial-temporal network includes new at-\\ntention modules that learn temporal world dynamics from a collection of videos. This procedure\\nsigniﬁcantly accelerates the T2V training process by instantaneously transferring the knowledge\\nfrom a previously trained T2I network to a new T2V one. To enhance the visual quality, we train\\nspatial super-resolution models as well as frame interpolation models. This increases the resolution\\nof the generated videos, as well as enables a higher (controllable) frame rate.\\nOur main contributions are:\\n• We present Make-A-Video – an effective method that extends a diffusion-based T2I model\\nto T2V through a spatiotemporally factorized diffusion model.\\n• We leverage joint text-image priors to bypass the need for paired text-video data, which in\\nturn allows us to potentially scale to larger quantities of video data.\\n• We present super-resolution strategies in space and time that, for the ﬁrst time, generate\\nhigh-deﬁnition, high frame-rate videos given a user-provided textual input.\\n• We evaluate Make-A-Video against existing T2V systems and present: (a) State-of-the-art\\nresults in quantitative as well as qualitative measures, and (b) A more thorough evaluation\\nthan existing literature in T2V. We also collect a test set of 300 prompts for zero-shot T2V\\nhuman evaluation which we plan to release.\\n2\\n2\\nPREVIOUS WORK\\nText-to-Image Generation.\\n(Reed et al., 2016) is among the ﬁrst methods to extend uncondi-\\ntional Generative Adversairal Network (GAN) (Goodfellow et al., 2014) to T2I generation. Later\\nGAN variants have focused on progressive generation (Zhang et al., 2017; Hong et al., 2018), or\\nbetter text-image alignment (Xu et al., 2018; Zhang et al., 2021). The pioneering work of DALL-\\nE (Ramesh et al., 2021) considers T2I generation as a sequence-to-sequence translation problem us-\\ning a discrete variational auto-encoder (VQVAE) and Transformer (Vaswani et al., 2017). Additional\\nvariants (Ding et al., 2022) have been proposed since then. For example, Make-A-Scene (Gafni\\net al., 2022) explores controllable T2I generation using semantic maps. Parti (Yu et al., 2022a)\\naims for more diverse content generation through an encoder-decoder architecture and an improved\\nimage tokenizer (Yu et al., 2021). On the other hand, Denoising Diffusion Probabilistic Models\\n(DDPMs) (Ho et al., 2020) are successfully leveraged for T2I generation. GLIDE (Nichol et al.,\\n2021) trained a T2I and an upsampling diffusion model for cascade generation. GLIDE’s proposed\\nclassiﬁer-free guidance has been widely adopted in T2I generation to improve image quality and\\ntext faithfulness. DALLE-2 (Ramesh et al., 2022) leverages the CLIP (Radford et al., 2021) latent\\nspace and a prior model. VQ-diffusion (Gu et al., 2022) and stable diffusion (Rombach et al., 2022)\\nperforms T2I generation in the latent space instead of pixel space to improve efﬁciency.\\nText-to-Video Generation. While there is remarkable progress in T2I generation, the progress of\\nT2V generation lags behind largely due to two main reasons: the lack of large-scale datasets with\\nhigh-quality text-video pairs, and the complexity of modeling higher-dimensional video data. Early\\nworks (Mittal et al., 2017; Pan et al., 2017; Marwah et al., 2017; Li et al., 2018; Gupta et al., 2018;\\nLiu et al., 2019b) are mainly focused on video generation in simple domains, such as moving digits\\nor speciﬁc human actions. To our knowledge, Sync-DRAW (Mittal et al., 2017) is the ﬁrst T2V\\ngeneration approach that leverages a VAE with recurrent attention. (Pan et al., 2017) and (Li et al.,\\n2018) extend GANs from image generation to T2V generation.\\nMore recently, GODIVA (Wu et al., 2021a) is the ﬁrst to use 2D VQVAE and sparse attention for\\nT2V generation supporting more realistic scenes. N ¨UWA (Wu et al., 2021b) extends GODIVA, and\\npresents a uniﬁed representation for various generation tasks in a multitask learning scheme. To\\nfurther improve the performance of T2V generation, CogVideo (Hong et al., 2022) is built on top of\\na frozen CogView-2 (Ding et al., 2022) T2I model by adding additional temporal attention modules.\\nVideo Diffusion Models (VDM) (Ho et al., 2022) uses a space-time factorized U-Net with joint\\nimage and video data training. While both CogVideo and VDM collected 10M private text-video\\npairs for training, our work uses solely open-source datasets, making it easier to reproduce.\\nLeveraging Image Priors for Video Generation. Due to the complexity of modeling videos and the\\nchallenges in high-quality video data collection, it is natural to consider leveraging image priors for\\nvideos to simplifying the learning process. After all, an image is a video with a single frame (Bain\\net al., 2021). In unconditional video generation, MoCoGAN-HD (Tian et al., 2021) formulates\\nvideo generation as the task of ﬁnding a trajectory in the latent space of a pre-trained and ﬁxed image\\ngeneration model. In T2V generation, N ¨UWA (Wu et al., 2021b) combines image and video datasets\\nin a multitask pre-training stage to improve model generalization for ﬁne-tuning. CogVideo (Hong\\net al., 2022) uses a pre-trained and ﬁxed T2I model for T2V generation with only a small number\\nof trainable parameters to reduce memory usage during training. But the ﬁxed autoencoder and T2I\\nmodels can be restrictive for T2V generation. The architecture of VDM (Ho et al., 2022) can enable\\njoint image and video generation. However, they sample random independent images from random\\nvideos as their source of images, and do not leverage the massive text-image datasets.\\nMake-A-Video differs from previous works in several aspects. First, our architecture breaks the\\ndependency on text-video pairs for T2V generation. This is a signiﬁcant advantage compared to\\nprior work, that has to be restricted to narrow domains (Mittal et al., 2017; Gupta et al., 2018; Ge\\net al., 2022; Hayes et al., 2022), or require large-scale paired text-video data (Hong et al., 2022;\\nHo et al., 2022). Second, we ﬁne-tune the T2I model for video generation, gaining the advantage\\nof adapting the model weights effectively, compared to freezing the weights as in CogVideo (Hong\\net al., 2022). Third, motivated from prior work on efﬁcient architectures for video and 3D vision\\ntasks (Ye et al., 2019; Qiu et al., 2017; Xie et al., 2018), our use of pseudo-3D convolution (Qiu\\net al., 2017) and temporal attention layers not only better leverage a T2I architecture, it also allows\\nfor better temporal information fusion compared to VDM (Ho et al., 2022).\\n3\\nFigure 2: Make-A-Video high-level architecture. Given input text x translated by the prior P into\\nan image embedding, and a desired frame rate fps, the decoder Dt generates 16 64 × 64 frames,\\nwhich are then interpolated to a higher frame rate by ↑F , and increased in resolution to 256 × 256\\nby SRt\\nl and 768 × 768 by SRh, resulting in a high-spatiotemporal-resolution generated video ˆy.\\n3\\nMETHOD\\nMake-A-Video consists of three main components: (i) A base T2I model trained on text-image pairs\\n(Sec. 3.1), (ii) spatiotemporal convolution and attention layers that extend the networks’ building\\nblocks to the temporal dimension (Sec. 3.2), and (iii) spatiotemporal networks that consist of both\\nspatiotemporal layers, as well as another crucial element needed for T2V generation - a frame inter-\\npolation network for high frame rate generation (Sec. 3.3).\\nMake-A-Video’s ﬁnal T2V inference scheme (depicted in Fig. 2) can be formulated as:\\nˆyt = SRh ◦ SRt\\nl ◦ ↑F ◦ Dt ◦ P ◦(ˆx, Cx(x)),\\n(1)\\nwhere ˆyt is the generated video, SRh, SRl are the spatial and spatiotemporal super-resolution net-\\nworks (Sec. 3.2), ↑F is a frame interpolation network (Sec. 3.3), Dt is the spatiotemporal decoder\\n(Sec. 3.2), P is the prior (Sec. 3.1), ˆx is the BPE-encoded text, Cx is the CLIP text encoder (Rad-\\nford et al., 2021), and x is the input text. The three main components are described in detail in the\\nfollowing sections.\\n3.1\\nTEXT-TO-IMAGE MODEL\\nPrior to the addition of the temporal components, we train the backbone of our method: a T2I model\\ntrained on text-image pairs, sharing the core components with the work of (Ramesh et al., 2022).\\nWe use the following networks to produce high-resolution images from text: (i) A prior network P,\\nthat during inference generates image embeddings ye given text embeddings xe and BPE encoded\\ntext tokens ˆx, (ii) a decoder network D that generates a low-resolution 64 × 64 RGB image ˆyl,\\nconditioned on the image embeddings ye, and (iii) two super-resolution networks SRl,SRh that\\nincrease the generated image ˆyl resolution to 256 × 256 and 768 × 768 pixels respectively, resulting\\nin the ﬁnal1 generated image ˆy.\\n3.2\\nSPATIOTEMPORAL LAYERS\\nIn order to expand the two-dimensional (2D) conditional network into the temporal dimension, we\\nmodify the two key building blocks that now require not just spatial but also temporal dimensions in\\norder to generate videos: (i) Convolutional layers (Sec. 3.2.1), and (ii) attention layers (Sec. 3.2.2),\\ndiscussed in the following two subsections. Other layers, such as fully-connected layers, do not\\nrequire speciﬁc handling when adding an additional dimension, as they are agnostic to structured\\nspatial and temporal information. Temporal modiﬁcations are made in most U-Net-based diffusion\\nnetworks: the spatiotemporal decoder Dt now generating 16 RGB frames, each of size 64 × 64, the\\n1We then downsample to 512 using bicubic interpolation for a cleaner aesthetic. Maintaining a clean aes-\\nthetic for high deﬁnition videos is part of future work.\\n4\\nFigure 3: The architecture and initialization scheme of the Pseudo-3D convolutional and at-\\ntention layers, enabling the seamless transition of a pre-trained Text-to-Image model to the\\ntemporal dimension. (left) Each spatial 2D conv layer is followed by a temporal 1D conv layer.\\nThe temporal conv layer is initialized with an identity function. (right) Temporal attention layers are\\napplied following the spatial attention layers by initializing the temporal projection to zero, resulting\\nin an identity function of the temporal attention blocks.\\nnewly added frame interpolation network ↑F , increasing the effective frame rate by interpolating\\nbetween the 16 generated frames (as depicted in Fig. 2), and the super-resolution networks SRt\\nl.\\nNote that super resolution involves hallucinating information. In order to not have ﬂickering ar-\\ntifacts, the hallucination must be consistent across frames. As a result, our SRt\\nl module operates\\nacross spatial and temporal dimensions. In qualitative inspection we found this to signiﬁcantly out-\\nperform per-frame super resolution. It is challenging to extend SRh to the temporal dimension due\\nto memory and compute constraints, as well as a scarcity of high resolution video data. So SRh\\noperates only along the spatial dimensions. But to encourage consistent detail hallucination across\\nframes, we use the same noise initialization for each frame.\\n3.2.1\\nPSEUDO-3D CONVOLUTIONAL LAYERS\\nMotivated by separable convolutions (Chollet, 2017), we stack a 1D convolution following each\\n2D convolutional (conv) layer, as shown in Fig. 3. This facilitates information sharing between\\nthe spatial and temporal axes, without succumbing to the heavy computational load of 3D conv\\nlayers. In addition, it creates a concrete partition between the pre-trained 2D conv layers and the\\nnewly initialized 1D conv layers, allowing us to train the temporal convolutions from scratch, while\\nretaining the previously learned spatial knowledge in the spatial convolutions’ weights.\\nGiven an input tensor h ∈ RB×C×F ×H×W , where B, C, F, H, W are the batch, channels, frames,\\nheight, and width dimensions respectively, the Pseudo-3D convolutional layer is deﬁned as:\\nConvP 3D(h) := Conv1D(Conv2D(h) ◦ T) ◦ T,\\n(2)\\nwhere the transpose operator ◦T swaps between the spatial and temporal dimensions. For smooth\\ninitialization, while the Conv2D layer is initialized from the pre-trained T2I model, the Conv1D\\nlayer is initialized as the identity function, enabling a seamless transition from training spatial-only\\nlayers, to spatiotemporal layers. Note that at initialization, the network will generate K different\\nimages (due to random noise), each faithful to the input text but lacking temporal coherence.\\n3.2.2\\nPSEUDO-3D ATTENTION LAYERS\\nA crucial component of T2I networks is the attention layer, where in addition to self-attending to ex-\\ntracted features, text information is injected to several network hierarchies, alongside other relevant\\ninformation, such as the diffusion time-step. While using 3D convolutional layers is computationally\\nheavy, adding the temporal dimension to attention layers is outright infeasible in terms of memory\\nconsumption. Inspired by the work of (Ho et al., 2022), we extend our dimension decomposition\\nstrategy to attention layers as well. Following each (pre-trained) spatial attention layer, we stack a\\ntemporal attention layer, which as with the convolutional layers, approximates a full spatiotemporal\\nattention layer. Speciﬁcally, given an input tensor h, we deﬁne flatten as a matrix operator that\\n5\\nﬂattens the spatial dimension into h′ ∈ RB×C×F ×HW . unflatten is deﬁned as the inverse matrix\\noperator. The Pseudo-3D attention layer therefore is therefore deﬁned as:\\nATTNP 3D(h) = unflatten(ATTN1D(ATTN2D(flatten(h)) ◦ T) ◦ T).\\n(3)\\nSimilarly to ConvP 3D, to allow for smooth spatiotemporal initialization, the ATTN2D layer is ini-\\ntialized from the pre-trained T2I model and the ATTN1D layer is initialized as the identity function.\\nFactorized space-time attention layers have also been used in VDM (Ho et al., 2022) and\\nCogVideo (Hong et al., 2022). CogVideo has added temporal layers to each (frozen) spatial layers\\nwhereas we train them jointly. In order to force their network to train for images and videos inter-\\nchangeably, VDM has extended their 2D U-Net to 3D through unﬂattened 1x3x3 convolution ﬁlters,\\nsuch that the subsequent spatial attention remains 2D, and added 1D temporal attention through rel-\\native position embeddings. In contrast, we apply an additional 3x1x1 convolution projection (after\\neach 1x3x3) such that the temporal information will also be passed through each convolution layer.\\nFrame rate conditioning. In addition to the T2I conditionings, similar to CogVideo (Hong et al.,\\n2022), we add an additional conditioning parameter fps, representing the number of frames-per-\\nsecond in a generated video. Conditioning on a varying number of frames-per-second, enables an\\nadditional augmentation method to tackle the limited volume of available videos at training time,\\nand provides additional control on the generated video at inference time.\\n3.3\\nFRAME INTERPOLATION NETWORK\\nIn addition to the spatiotemporal modiﬁcations discussed in Sec. 3.2, we train a new masked frame\\ninterpolation and extrapolation network ↑F , capable of increasing the number of frames of the gen-\\nerated video either by frame interpolation for a smoother generated video, or by pre/post frame\\nextrapolation for extending the video length. In order to increase the frame rate within memory and\\ncompute constraints, we ﬁne-tune a spatiotemporal decoder Dt on the task of masked frame inter-\\npolation, by zero-padding the masked input frames, enabling video upsampling. When ﬁne-tuning\\non masked frame interpolation, we add an additional 4 channels to the input of the U-Net: 3 chan-\\nnels for the RGB masked video input and an additional binary channel indicating which frames are\\nmasked. We ﬁne-tune with variable frame-skips and fps conditioning to enable multiple temporal\\nupsample rates at inference time. We denote ↑F as the operator that expands the given video tensor\\nthrough masked frame interpolation. For all of our experiments we applied ↑F with frame skip 5 to\\nupsample a 16 frame video to 76 frames ((16-1)×5+1). Note that we can use the same architecture\\nfor video extrapolation or image animation by masking frames at the beginning or end of a video.\\n3.4\\nTRAINING\\nThe different components of Make-A-Video described above are trained independently. The only\\ncomponent that receives text as input is the prior P. We train it on paired text-image data and do not\\nﬁne-tune it on videos. The decoder, prior, and two super-resolution components are ﬁrst trained on\\nimages alone (no aligned text). Recall that the decoder receives CLIP image embedding as input,\\nand the super-resolution components receive downsampled images as input during training. After\\ntraining on images, we add and initialize the new temporal layers and ﬁne-tune them over unlabeled\\nvideo data. 16 frames are sampled from the original video with random fps ranging from 1 to\\n30. We use the beta function for sampling and while training the decoder, start from higher FPS\\nranges (less motion) and then transition to lower FPS ranges (more motion). The masked-frame-\\ninterpolation component is ﬁne-tuned from the temporal decoder.\\n4\\nEXPERIMENTS\\n4.1\\nDATASETS AND SETTINGS\\nDatasets. To train the image models, we use a 2.3B subset of the dataset from (Schuhmann et al.)\\nwhere the text is English. We ﬁlter out sample pairs with NSFW images 2, toxic words in the text,\\nor images with a watermark probability larger than 0.5. We use WebVid-10M (Bain et al., 2021)\\n2We used this model: https://github.com/GantMan/nsfw model\\n6\\nTable 1: T2V generation evaluation on MSR-VTT. Zero-Shot means no training is conducted on\\nMSR-VTT. Samples/Input means how many samples are generated (and then ranked) for each input.\\nMethod\\nZero-Shot\\nSamples/Input\\nFID (↓)\\nCLIPSIM (↑)\\nGODIVA (Wu et al., 2021a)\\nNo\\n30\\n−\\n0.2402\\nN ¨UWA (Wu et al., 2021b)\\nNo\\n−\\n47.68\\n0.2439\\nCogVideo (Hong et al., 2022) (Chinese)\\nYes\\n1\\n24.78\\n0.2614\\nCogVideo (Hong et al., 2022) (English)\\nYes\\n1\\n23.59\\n0.2631\\nMake-A-Video (ours)\\nYes\\n1\\n13.17\\n0.3049\\nand a 10M subset from HD-VILA-100M (Xue et al., 2022) 3 to train our video generation models.\\nNote that only the videos (no aligned text) are used. The decoder Dt and the interpolation model\\nis trained on WebVid-10M. SRt\\nl is trained on both WebVid-10M and HD-VILA-10M. While prior\\nwork (Hong et al., 2022; Ho et al., 2022) have collected private text-video pairs for T2V generation,\\nwe use only public datasets (and no paired text for videos). We conduct automatic evaluation on\\nUCF-101 (Soomro et al., 2012) and MSR-VTT (Xu et al., 2016) in a zero-shot setting.\\nAutomatic Metrics. For UCF-101, we write one template sentence for each class (without generat-\\ning any video) and ﬁx it for evaluation. We report Frechet Video Distance (FVD) and Inception Score\\n(IS) on 10K samples following (Ho et al., 2022). We generate samples that follow the same class\\ndistribution as the training set. For MSR-VTT, we report Frechet Inception Distance (FID) (Parmar\\net al., 2022) and CLIPSIM (average CLIP similarity between video frames and text) (Wu et al.,\\n2021a), where all 59, 794 captions from the test set are used, following (Wu et al., 2021b).\\nHuman Evaluation Set and Metrics. We collect an evaluation set from Amazon Mechanical Turk\\n(AMT) that consists of 300 prompts. We asked annotators what they would be interested in gener-\\nating if there were a T2V system. We ﬁltered out prompts that were incomplete (e.g., “jump into\\nwater”), too abstract (e.g., “climate change”), or offensive. We then identiﬁed 5 categories (animals,\\nfantasy, people, nature and scenes, food and beverage) and selected prompts for these categories.\\nThese prompts were selected without generating any videos for them, and were kept ﬁxed. In addi-\\ntion, we also used the DrawBench prompts from Imagen (Saharia et al., 2022) for human evaluation.\\nWe evaluate video quality and text-video faithfulness. For video quality, we show two videos in ran-\\ndom order and ask annotators which one is of higher quality. For faithfulness, we additionally show\\nthe text and ask annotators which video has a better correspondence with the text (we suggest them\\nto ignore quality issues). In addition, we also conducted human evaluation to compare video motion\\nrealism of our interpolation model and FILM (Reda et al., 2022). For each comparison, we use the\\nmajority vote from 5 different annotators as the ﬁnal result.\\n4.2\\nQUANTITATIVE RESULTS\\nAutomatic Evaluation on MSR-VTT. In addition to GODIVA and N ¨UWA that report on MSR-\\nVTT, we also perform inference on the ofﬁcially released CogVideo model with both Chinese and\\nEnglish inputs for comparison. For CogVideo and Make-A-Video, we only generate one sample\\nfor each prompt in a zero-shot setting. We only generate videos that are at 16 × 256 × 256 as the\\nevaluation models do not expect higher resolutions and frame rate. The results are shown in Table 1.\\nMake-A-Video’s zero-shot performance is much better than GODIVA and N ¨UWA which are trained\\non MSR-VTT. We also outperform CogVideo in both Chinese and English settings. Thus, Make-A-\\nVideo has signiﬁcantly better generalization capabilities than prior work.\\nAutomatic Evaluation on UCF-101. UCF-101 is a popular benchmark to evaluate video generation\\nand has been recently used in T2V models. CogVideo performed ﬁnetuning of their pretrained\\nmodel for class-conditional video generation. VDM (Ho et al., 2022) performed unconditional video\\ngeneration and trained from scratch on UCF-101. We argue that both settings are not ideal and is\\nnot a direct evaluation of the T2V generation capabilities. Moreover, the FVD evaluation model\\nexpects the videos to be 0.5 second (16 frames), which is too short to be used for video generation in\\npractice. Nevertheless, in order to compare to prior work, we conducted evaluation on UCF-101 in\\n3These 100M clips are sourced from 3.1M videos. We randomly downloaded 3 clips per video to form our\\nHD-VILA-10M subset.\\n7\\nTable 2: Video generation evaluation on UCF-101 for both zero-shot and ﬁne-tuning settings.\\nMethod\\nPretrain\\nClass\\nResolution\\nIS (↑)\\nFVD (↓)\\nZero-Shot Setting\\nCogVideo (Chinese)\\nNo\\nYes\\n480 × 480\\n23.55\\n751.34\\nCogVideo (English)\\nNo\\nYes\\n480 × 480\\n25.27\\n701.59\\nMake-A-Video (ours)\\nNo\\nYes\\n256 × 256\\n33.00\\n367.23\\nFinetuning Setting\\nTGANv2(Saito et al., 2020)\\nNo\\nNo\\n128 × 128\\n26.60 ± 0.47\\n-\\nDIGAN(Yu et al., 2022b)\\nNo\\nNo\\n32.70 ± 0.35\\n577 ± 22\\nMoCoGAN-HD(Tian et al., 2021)\\nNo\\nNo\\n256 × 256\\n33.95 ± 0.25\\n700 ± 24\\nCogVideo (Hong et al., 2022)\\nYes\\nYes\\n160 × 160\\n50.46\\n626\\nVDM (Ho et al., 2022)\\nNo\\nNo\\n64 × 64\\n57.80 ± 1.3\\n-\\nTATS-base(Ge et al., 2022)\\nNo\\nYes\\n128 × 128\\n79.28 ± 0.38\\n278 ± 11\\nMake-A-Video (ours)\\nYes\\nYes\\n256 × 256\\n82.55\\n81.25\\nTable 3: Human evaluation results compared to CogVideo (Hong et al., 2022) on DrawBench and\\nour test set, and to VDM (Ho et al., 2022) on the 28 examples from their website. The numbers\\nshow the percentage of raters that prefer the results of our Make-A-Video model.\\nComparison\\nBenchmark\\nQuality\\nFaithfulness\\nMake-A-Video (ours) vs. VDM\\nVDM prompts (28)\\n84.38\\n78.13\\nMake-A-Video (ours) vs. CogVideo (Chinese)\\nDrawBench (200)\\n76.88\\n73.37\\nMake-A-Video (ours) vs. CogVideo (English)\\nDrawBench (200)\\n74.48\\n68.75\\nMake-A-Video (ours) vs. CogVideo (Chinese)\\nOur Eval. Set (300)\\n73.44\\n75.74\\nMake-A-Video (ours) vs. CogVideo (English)\\nOur Eval. Set (300)\\n77.15\\n71.19\\nboth zero-shot and ﬁnetuning settings. As shown in Table 2, Make-A-Video’s zero-shot performance\\nis already competitive than other approaches that are trained on UCF-101, and is much better than\\nCogVideo, which indicates that Make-A-Video can generalize better even to such a speciﬁc domain.\\nOur ﬁnetuning setting achieves state-of-the-art results with a signiﬁcant reduction in FVD, which\\nsuggests that Make-A-Video can generate more coherent videos than prior work.\\nHuman Evaluation. We compare to CogVideo (the only public zero-shot T2V generation model) on\\nDrawBench and our test set. We also evaluate on the 28 videos shown on the webpage of VDM (Ho\\net al., 2022) (which may be biased towards showcasing the model’s strengths). Since this is a very\\nsmall test set, we randomly generate 8 videos for each input and perform evaluation 8 times and\\nreport the average results. We generate videos at 76 × 256 × 256 resolution for human evaluation.\\nThe results are shown in Table 3. Make-A-Video achieves much better performance in both video\\nquality and text-video faithfulness in all benchmarks and comparisons. For CogVideo, the results are\\nsimilar on DrawBench and our evaluation set. For VDM, it is worth noting that we have achieved\\nsigniﬁcantly better results without any cherry-picking. We also evaluate our frame interpolation\\nnetwork in comparison to FILM (Reda et al., 2022). We ﬁrst generate low frame rate videos (1 FPS)\\nfrom text prompts in DrawBench and our evaluation set, then use each method to upsample to 4\\nFPS. Raters choose our method for more realistic motion 62% of the time on our evaluation set and\\n54% of the time on DrawBench. We observe that our method excels when there are large differences\\nbetween frames where having real-world knowledge of how objects move is crucial.\\n4.3\\nQUALITATIVE RESULTS\\nExamples of Make-A-Video’s generations are shown in Figure 1. In this section, we will show\\nT2V generation comparison to CogVideo (Hong et al., 2022) and VDM (Ho et al., 2022), and video\\ninterpolation comparison to FILM (Reda et al., 2022). In addition, our models can be used for\\na variety of other tasks such as image animation, video variation, etc. Due to space constraint,\\nwe only show a single example of each. Figure 4 (a) shows the comparison of Make-A-Video\\nto CogVideo and VDM. Make-A-Video can generate richer content with motion consistency and\\n8\\n(a) T2V Generation: comparison between VDM (top), CogVideo (mid), and Ours (bottom) for input “Busy freeway at night”. \\n(b) Image Animation: leftmost shows the input image, and we animated it to be a video.\\n(c) Image Interpolation: given two images (leftmost and rightmost), we interpolate frames. Comparing FILM (left) and Ours (right).\\n(d) Video Variation: we can generate a new video (bottom) as a variant to the original video (top).\\nFigure 4: Qualitative results for various comparisons and applications.\\ntext correspondence. Figure 4 (b) shows an example of image animation where we condition the\\nmasked frame interpolation and extrapolation network ↑F on the image and CLIP image embedding\\nto extrapolate the rest of the video. This allows a user to generate a video using their own image\\n– giving them the opportunity to personalize and directly control the generated video. Figure 4\\n(c) shows a comparison of our approach to FILM (Reda et al., 2022) on the task of interpolation\\nbetween two images. We achieve this by using the interpolation model that takes the two images as\\nthe beginning and end frames and masks 14 frames in between for generation. Our model generates\\nmore semantically meaningful interpolation while FILM seems to primarily smoothly transition\\nbetween frames without semantic real-world understanding of what is moving. Figure 4 (d) shows\\nan example for video variation. We take the average CLIP embedding of all frames from a video\\nas the condition to generate a semantically similar video. More video generation examples and\\napplications can be found here: make-a-video.github.io.\\n5\\nDISCUSSION\\nLearning from the world around us is one of the greatest strengths of human intelligence. Just as we\\nquickly learn to recognize people, places, things, and actions through observation, generative sys-\\ntems will be more creative and useful if they can mimic the way humans learn. Learning world dy-\\nnamics from orders of magnitude more videos using unsupervised learning helps researchers break\\n9\\naway from the reliance on labeled data. The presented work has shown how labeled images com-\\nbined effectively with unlabeled video footage can achieve that.\\nAs a next step we plan to address several of the technical limitations. As discussed earlier, our\\napproach can not learn associations between text and phenomenon that can only be inferred in\\nvideos. How to incorporate these (e.g., generating a video of a person waving their hand left-to-right\\nor right-to-left), along with generating longer videos, with multiple scenes and events, depicting\\nmore detailed stories, is left for future work.\\nAs with all large-scale models trained on data from the web, our models have learnt and likely\\nexaggerated social biases, including harmful ones. Our T2I generation model was trained on data\\nthat removed NSFW content and toxic words. All our data (image as well as videos) is publicly\\navailable, adding a layer of transparency to our models, and making it possible for the community\\nto reproduce our work.\\nACKNOWLEDGMENTS\\nMustafa Said Mehmetoglu, Jacob Xu, Katayoun Zand, Jia-Bin-Huang, Jiebo Luo, Shelly Sheynin,\\nAngela Fan, Kelly Freed. Thank you for your contributions!\\nREFERENCES\\nMax Bain, Arsha Nagrani, G¨ul Varol, and Andrew Zisserman. Frozen in time: A joint video and\\nimage encoder for end-to-end retrieval. In ICCV, pp. 1728–1738, 2021.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\\nAriel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz\\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\\nRadford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. CoRR,\\nabs/2005.14165, 2020. URL https://arxiv.org/abs/2005.14165.\\nFranc¸ois Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings\\nof the IEEE conference on computer vision and pattern recognition, pp. 1251–1258, 2017.\\nMing Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image\\ngeneration via hierarchical transformers. arXiv preprint arXiv:2204.14217, 2022.\\nOran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-\\nscene: Scene-based text-to-image generation with human priors, 2022. URL https://arxiv.\\norg/abs/2203.13131.\\nSongwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and\\nDevi Parikh. Long video generation with time-agnostic vqgan and time-sensitive transformer.\\nECCV, 2022.\\nDeeptha Girish, Vineeta Singh, and Anca Ralescu. Understanding action recognition in still images.\\npp. 1523–1529, 06 2020. doi: 10.1109/CVPRW50498.2020.00193.\\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\\nAaron Courville, and Yoshua Bengio. Generative adversarial networks. NIPS, 2014.\\nShuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and\\nBaining Guo. Vector quantized diffusion model for text-to-image synthesis. In CVPR, pp. 10696–\\n10706, 2022.\\nTanmay Gupta, Dustin Schwenk, Ali Farhadi, Derek Hoiem, and Aniruddha Kembhavi. Imagine\\nthis! scripts to compositions to videos. In ECCV, pp. 598–613, 2018.\\nThomas Hayes, Songyang Zhang, Xi Yin, Guan Pang, Sasha Sheng, Harry Yang, Songwei Ge, Is-\\nabelle Hu, and Devi Parikh. Mugen: A playground for video-audio-text multimodal understanding\\nand generation. ECCV, 2022.\\n10\\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020. URL\\nhttps://arxiv.org/abs/2006.11239.\\nJonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J.\\nFleet. Video diffusion models, 2022. URL https://arxiv.org/abs/2204.03458.\\nSeunghoon Hong, Dingdong Yang, Jongwook Choi, and Honglak Lee. Inferring semantic layout for\\nhierarchical text-to-image synthesis. In CVPR, pp. 7986–7994, 2018.\\nWenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pre-\\ntraining for text-to-video generation via transformers, 2022. URL https://arxiv.org/\\nabs/2205.15868.\\nYitong Li, Martin Min, Dinghan Shen, David Carlson, and Lawrence Carin. Video generation from\\ntext. In AAAI, volume 32, 2018.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining\\napproach. CoRR, abs/1907.11692, 2019a. URL http://arxiv.org/abs/1907.11692.\\nYue Liu, Xin Wang, Yitian Yuan, and Wenwu Zhu. Cross-modal dual learning for sentence-to-\\nvideo generation. In Proceedings of the 27th ACM International Conference on Multimedia, pp.\\n1239–1247, 2019b.\\nTanya Marwah, Gaurav Mittal, and Vineeth N Balasubramanian. Attentive semantic video genera-\\ntion using captions. In ICCV, pp. 1426–1434, 2017.\\nGaurav Mittal, Tanya Marwah, and Vineeth N Balasubramanian. Sync-draw: Automatic video gen-\\neration using deep recurrent attentive architectures. In Proceedings of the 25th ACM international\\nconference on Multimedia, pp. 1096–1104, 2017.\\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with\\ntext-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\\nYingwei Pan, Zhaofan Qiu, Ting Yao, Houqiang Li, and Tao Mei. To create what you tell: Generat-\\ning videos from captions. In Proceedings of the 25th ACM international conference on Multime-\\ndia, pp. 1789–1798, 2017.\\nGaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On aliased resizing and surprising subtleties in\\ngan evaluation. In CVPR, 2022.\\nZhaofan Qiu, Ting Yao, and Tao Mei. Learning spatio-temporal representation with pseudo-3d\\nresidual networks. In ICCV, pp. 5533–5541, 2017.\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\\nmodels from natural language supervision. In ICML, pp. 8748–8763. PMLR, 2021.\\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\\nand Ilya Sutskever. Zero-shot text-to-image generation. In ICML, pp. 8821–8831. PMLR, 2021.\\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\\nconditional image generation with clip latents, 2022.\\nURL https://arxiv.org/abs/\\n2204.06125.\\nFitsum Reda, Janne Kontkanen, Eric Tabellion, Deqing Sun, Caroline Pantofaru, and Brian Curless.\\nFilm: Frame interpolation for large motion. arXiv preprint arXiv:2202.04901, 2022.\\nScott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee.\\nGenerative adversarial text to image synthesis. In ICML, pp. 1060–1069. PMLR, 2016.\\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High-\\nresolution image synthesis with latent diffusion models. In CVPR, pp. 10684–10695, 2022.\\n11\\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kam-\\nyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Sal-\\nimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image dif-\\nfusion models with deep language understanding, 2022. URL https://arxiv.org/abs/\\n2205.11487.\\nMasaki Saito, Shunta Saito, Masanori Koyama, and Sosuke Kobayashi. Train sparsely, generate\\ndensely: Memory-efﬁcient unsupervised training of high-resolution temporal gan. International\\nJournal of Computer Vision, 128(10):2586–2606, 2020.\\nChristoph Schuhmann, Romain Beaumont, Cade W Gordon, Ross Wightman, Theo Coombes,\\nAarush Katta, Clayton Mullis, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson,\\net al. Laion-5b: An open large-scale dataset for training next generation image-text models.\\nChristoph Schuhmann,\\nRichard Vencu,\\nRomain Beaumont,\\nTheo Coombes,\\nCade Gor-\\ndon,\\nAarush\\nKatta,\\nRobert\\nKaczmarczyk,\\nand\\nJenia\\nJitsev.\\nLAION-5B:\\nlaion-\\n5b:\\nA new era of open large-scale multi-modal datasets.\\nhttps://laion.ai/\\nlaion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/,\\n2022.\\nKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions\\nclasses from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.\\nYu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N Metaxas, and Sergey\\nTulyakov. A good image generator is what you need for high-resolution video synthesis. ICLR,\\n2021.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2017. URL https://arxiv.\\norg/abs/1706.03762.\\nChenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and\\nNan Duan. Godiva: Generating open-domain videos from natural descriptions. arXiv preprint\\narXiv:2104.14806, 2021a.\\nChenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. N ¨Uwa: Visual\\nsynthesis pre-training for neural visual world creation, 2021b. URL https://arxiv.org/\\nabs/2111.12417.\\nSaining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethinking spatiotem-\\nporal feature learning: Speed-accuracy trade-offs in video classiﬁcation. In ECCV, pp. 305–321,\\n2018.\\nJun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging\\nvideo and language. In CVPR, pp. 5288–5296, 2016.\\nTao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong\\nHe. Attngan: Fine-grained text to image generation with attentional generative adversarial net-\\nworks. In CVPR, pp. 1316–1324, 2018.\\nHongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and\\nBaining Guo. Advancing high-resolution video-language representation with large-scale video\\ntranscriptions. In CVPR, pp. 5036–5045, 2022.\\nRongtian Ye, Fangyu Liu, and Liqiang Zhang. 3d depthwise convolution: Reducing model parame-\\nters in 3d vision tasks. In Canadian Conference on Artiﬁcial Intelligence, pp. 186–199. Springer,\\n2019.\\nJiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong\\nXu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan.\\narXiv preprint arXiv:2110.04627, 2021.\\n12\\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\\nAlexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin\\nLi, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich\\ntext-to-image generation, 2022a. URL https://arxiv.org/abs/2206.10789.\\nSihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin.\\nGenerating videos with dynamics-aware implicit generative adversarial networks. ICLR, 2022b.\\nHan Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dim-\\nitris N Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adver-\\nsarial networks. In ICCV, pp. 5907–5915, 2017.\\nHan Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and Yinfei Yang. Cross-modal contrastive\\nlearning for text-to-image generation. In CVPR, pp. 833–842, 2021.\\n13\\n',\n",
       " 'AppAgent: Multimodal Agents as Smartphone Users\\nChi Zhang∗\\nZhao Yang∗\\nJiaxuan Liu∗\\nYucheng Han\\nXin Chen\\nZebiao Huang\\nBin Fu\\nGang Yu†\\nTencent\\n{johnczhang, jayzyang, jiaxuanliu, yuchenghan, shingxchen, zebiaohuang, brianfu, skicyyu}@tencent.com\\nhttps://appagent-official.github.io/\\nClock\\nTEMU\\nObservation: The screenshot shows a photo editing app on \\na smartphone with an image displayed on the screen...\\nThought: To complete this task, I should adjust the noise \\nreduction level using the slider. \\nAction: Action: swipe(23, \"right\", \"medium\")\\nObservation: The image shows a screenshot of an \\nalarm application on a smartphone...\\nThought: To complete this task, I should disable the \\nvibration feature by tapping on the UI element. \\nAction: tap(17)\\nObservation: The screenshot displays a shopping app \\nwith a variety of headsets listed ...\\nThought: To complete this task, I should add the \\nselected headset into my shopping cart.\\nAction: tap(14)\\nImprove the quality of \\nthis image until it looks \\ngood to you\\nObservation: The screenshot shows the Telegram app, \\na conversation from \"Agent Liu\" with a message...\\nThought: To complete this task, I should input the text with \\nthis answer into the input field labeled with the tag ‘7’\\nAction: text(\"The Toronto Raptors won the 2019 NBA ... \")\\nSet an alarm at 12:30 pm \\nevery Friday and Sunday, \\nand disable the vibration\\nSearch for a gaming \\nheadset and add it to my \\nshopping cart.\\nSend an email to \\njanedoe@email.com to \\nask her about her new job\\nSearch music video song \\nWonderful Tonight and \\nleave a praising comment \\nChat with the user \\nAgent Liu and \\nanswer his question\\nObservation: The screenshot shows a YouTube search \\nresult for \"Wonderful Tonight\" ...\\nThought: To complete this task, I should insert a \\npraising comment into the text input field labeled ‘2’.\\nAction: text(“This is such a timeless piece...\")\\nObservation: The screenshot shows a smartphone\\'s \\nemail application ...\\nThought: To complete this task, I should type the \\ncontent of the email asking Jane Doe about her new job.\\nAction: text(“Dear Jane Doe,\\\\n\\\\nI am writing to ...\")\\nFigure 1: Diverse applications of our multimodal agent framework for smartphone App operation. We\\nevaluate the effectiveness of our agent model on 50 tasks across 10 different Apps, highlighting its adaptability and\\neffectiveness in a real-world context.\\nAbstract\\nRecent advancements in large language models\\n(LLMs) have led to the creation of intelligent\\nagents capable of performing complex tasks.\\nThis paper introduces a novel LLM-based mul-\\ntimodal agent framework designed to operate\\nsmartphone applications. Our framework en-\\nables the agent to operate smartphone applica-\\n*Equal contributions.\\n†Corresponding Author.\\ntions through a simplified action space, mim-\\nicking human-like interactions such as tapping\\nand swiping. This novel approach bypasses\\nthe need for system back-end access, thereby\\nbroadening its applicability across diverse apps.\\nCentral to our agent’s functionality is its in-\\nnovative learning method. The agent learns\\nto navigate and use new apps either through\\nautonomous exploration or by observing hu-\\nman demonstrations. This process generates\\n1\\narXiv:2312.13771v1  [cs.CV]  21 Dec 2023\\na knowledge base that the agent refers to for\\nexecuting complex tasks across different appli-\\ncations. To demonstrate the practicality of our\\nagent, we conducted extensive testing over 50\\ntasks in 10 different applications, including so-\\ncial media, email, maps, shopping, and sophis-\\nticated image editing tools. The results affirm\\nour agent’s proficiency in handling a diverse\\narray of high-level tasks.\\n1\\nIntroduction\\nThe emergence of large language models (LLMs),\\nsuch as ChatGPT (OpenAI, 2021) and GPT-4 (Ope-\\nnAI, 2023), marks a significant milestone in the\\nfield of artificial intelligence and natural language\\nprocessing. These advanced models represent a\\nfundamental change in how machines understand\\nand generate human language, exhibiting a level of\\nsophistication and versatility previously unattain-\\nable. One of the most exciting developments in\\nthis field is the capability of LLMs to function not\\njust as language processors, but as agents capable\\nof performing complex tasks. This evolution is ev-\\nident in initiatives such as AutoGPT (Yang et al.,\\n2023a) and MetaGPT (Hong et al., 2023), which\\nshowcase the practical applications of LLMs in\\ntasks requiring advanced cognitive functions like\\nreasoning, planning, and collaboration. The signifi-\\ncance of these developments cannot be overstated,\\nas they extend the utility of LLMs beyond simple\\nlanguage tasks, revolutionizing various aspects of\\ntechnology and daily life.\\nHowever, a key limitation of these LLM-based\\nagents has been their reliance solely on text-based\\ninformation. This restriction has historically cur-\\ntailed their perception and interaction with their\\nenvironment. The introduction of models equipped\\nwith vision capabilities, such as the latest itera-\\ntion of GPT-4, marks a pivotal breakthrough. By\\nintegrating the ability to process and interpret vi-\\nsual information, these models can now understand\\naspects of their surroundings that are difficult or\\nimpossible to convey through text alone. This ex-\\ntended capability enables LLMs to interpret con-\\ntext, recognize patterns, and respond to visual cues,\\nthus providing a more holistic and interactive expe-\\nrience with the world.\\nIn our work, we focus on building a multimodal\\nagent leveraging the vision capabilities of multi-\\nmodal large language models to undertake tasks\\npreviously unachievable by text-only agents. In\\nparticular, we explore an interesting but challeng-\\ning application that builds an agent to operate any\\nsmartphone application (App) in the mobile oper-\\nating system. Our approach differs significantly\\nfrom existing intelligent phone assistants like Siri,\\nwhich operate through system back-end access and\\nfunction calls. Instead, our agent interacts with\\nsmartphone apps in a human-like manner, using\\nlow-level operations such as tapping and swiping\\non the graphical user interface (GUI). The proposed\\nagent offers multiple advantages. Firstly, it elimi-\\nnates the need for system back-end access, making\\nour agent universally applicable across various ap-\\nplications. Additionally, this approach enhances\\nsecurity and privacy, as the agent does not require\\ndeep system integration. Furthermore, by operating\\non the GUI level, our agent can adapt to changes\\nin app interfaces and updates, ensuring long-term\\napplicability and flexibility.\\nHowever, creating a multimodal agent capable\\nof operating diverse smartphone apps presents sig-\\nnificant challenges. Existing research indicates that\\nadapting current models for embodied tasks ne-\\ncessitates extensive training data, and collecting a\\nlarge dataset of app demonstrations for training is\\na formidable task. Moreover, different apps have\\nunique GUIs with varying icon meanings and op-\\nerational logic, and it remains uncertain whether\\nthese adapted models can effectively generalize to\\nunseen apps.\\nIn this paper, we introduce a multimodal agent\\nframework aimed at operating any smartphone app\\nlike human users. The learning of our framework\\ninvolves an exploration phase where the agent in-\\nteracts autonomously with apps through a set of\\npre-defined actions and learns from their outcomes.\\nThese interactions are documented, which assists\\nthe agent in navigating and operating the apps. This\\nlearning process can be accelerated by observing\\na few human demonstrations. Following this ex-\\nploratory phase, the agent can operate the app by\\nconsulting the constructed document based on its\\ncurrent state, eliminating the need to adapt the pa-\\nrameters of the LLMs or collect extensive training\\ndata for each app.\\nTo validate its effectiveness, we tested our agent\\non 50 tasks across 10 different apps, ranging from\\nsocial media and messaging to email, maps, shop-\\nping, and even complex image editing apps. Both\\nquantitative results and user studies underscore the\\nadvantages of our design, particularly its adapt-\\nability, user-friendliness, and efficient learning and\\n2\\noperating capabilities across a wide range of appli-\\ncations. This underlines the potential of our agent\\nas a versatile and effective tool in the realm of\\nsmartphone app operation.\\nIn summary, this paper makes the following con-\\ntributions:\\n• We open-source a multimodal agent frame-\\nwork, focusing on operating smartphone ap-\\nplications with our developed action space.\\n• We propose an innovative exploration strategy,\\nwhich enables the agent to learn to use novel\\napps.\\n• Through extensive experiments across multi-\\nple apps, we validate the advantages of our\\nframework, demonstrating its potential in the\\nrealm of AI-assisted smartphone app opera-\\ntion.\\n2\\nRelated Work\\n2.1\\nLarge language models\\nThe development of ChatGPT (OpenAI, 2021) and\\nGPT-4 (OpenAI, 2023) represents a crucial ad-\\nvancement in natural language processing. Unlike\\nearlier large language models (LLMs), these new\\nmodels (Touvron et al., 2023a,b; Zeng et al., 2022;\\nTaori et al., 2023; Zheng et al., 2023) enable multi-\\nround conversations and have the impressive ability\\nto follow complex instructions. The integration of\\nvision capabilities in GPT-4V (Yang et al., 2023b)\\nis a further milestone, enabling the language model\\nto process and interpret visual data. This addition\\nhas broadened the scope of potential AI applica-\\ntions, allowing GPT-4 to undertake diverse tasks\\nsuch as problem-solving, logical reasoning, tool\\nusage, API calls, and coding. Recent studies (Yang\\net al., 2023c; Yan et al., 2023) have shown that\\nGPT-4V can understand various types of images,\\nincluding simple user interfaces (UIs) in popular\\nsmartphone apps. However, challenges arise when\\nthe apps are new and their UIs are less typical,\\nwhich highlights a major problem that our work\\naims to address. Among open-source efforts from\\nthe industry and research community, the LLaMA\\nseries (Touvron et al., 2023a,b) are the most pop-\\nular equivalents and have been fine-tuned to ac-\\nquire conversational abilities, employing a decoder-\\nonly architecture similar to ChatGPT (Taori et al.,\\n2023; Zheng et al., 2023). Building upon LLaMA,\\nmany multimodal LLMs, such as LLaVA (Liu et al.,\\n2023b,a), ChartLlama (Han et al., 2023), and Sta-\\nbleLLaVA (Li et al., 2023), also demonstrate vision\\nunderstanding capabilities akin to those of GPT-4V.\\nNevertheless, a performance gap persists between\\nthese open-source models and GPT-4V, suggesting\\npotential areas for further development.\\n2.2\\nLLMs as agents\\nThe use of LLMs as agents for executing com-\\nplex tasks has gained increasing attention. Ini-\\ntiatives like AutoGPT (Yang et al., 2023a), Hug-\\ngingGPT (Shen et al., 2023), and MetaGPT (Hong\\net al., 2023) illustrate this trend, and many projects\\ndemonstrate impressive capabilities, moving be-\\nyond basic language tasks to engaging in activi-\\nties requiring higher cognitive functions, such as\\nsoftware development (Qian et al., 2023; Chen\\net al., 2021) and gaming (FAIR et al., 2022; Park\\net al., 2023; Xu et al., 2023).\\nIn this context,\\nYao et al. (Yao et al., 2023) introduce an innova-\\ntive approach that synergizes reasoning and acting\\nin LLMs, significantly enhancing their decision-\\nmaking and interactive capabilities. LLM-based\\nagents are designed to utilize the advanced lan-\\nguage and reasoning skills of LLMs to interact\\nwith and manipulate their environment (Liu et al.,\\n2023c; Gur et al., 2023; Xie et al., 2023). This in-\\ncludes performing tasks that require understanding\\ncontext, making decisions, and learning from inter-\\nactions (Xi et al., 2023; Hu and Shu, 2023). Such\\nagents are pivotal in applications where human-like\\ncognitive abilities are essential.\\nThe\\nemergence\\nof\\nmultimodal\\nLLM\\nagents (Wang et al., 2023; Furuta et al., 2023;\\nBrohan et al., 2022, 2023; Reed et al., 2022),\\ncapable of processing various inputs including text,\\nimages, audio, and video, has further broadened\\nthe scope of LLM applications. This versatility\\nis particularly beneficial for LLM-based agents,\\nenabling them to interact more effectively with\\ntheir environment and complete more complex\\ntasks, be it completing household tasks in a\\nphysical world (Ahn et al., 2022), generating 3D\\nassets via procedural tool use (Sun et al., 2023), or\\nmastering over 600 tasks across different domains\\nat the same time (Reed et al., 2022). Our research\\ncontributes to this area by focusing on an agent\\ndesigned to operate smartphone applications. This\\nagent’s ability to interpret screenshots from the\\noperating system demonstrates its flexibility and\\nadaptability, making it a valuable tool in a wide\\n3\\nTap\\n👀\\n👀\\n“Tapping this UI element opens \\nthe color settings to adjust the \\ncolor of the image.”\\n“Swiping this UI element allows the \\nuser to adjust the color \\ntemperature of the image.”\\n🤔\\n…\\n…\\nExploration Phase\\nDeployment Phase: “beautify this photo for me…”\\n…\\n…\\nObserve, Think, Act, and Summarize\\nSwipe\\nDoc for App\\nFigure 2: Overview of our multimodal agent framework designed to operate smartphone applications. The\\nfigure illustrates the two-phase approach of our framework. In the exploration phase, the agent interacts with a\\nsmartphone application and learns from their outcomes to create a comprehensive reference document. In the\\ndeployment phase, the agent utilizes the information compiled in this document to operate and navigate the apps\\neffectively.\\nrange of applications.\\n3\\nMethod\\nThis section details the methodology behind our in-\\nnovative multimodal agent framework. This frame-\\nwork enables an agent to interact with smartphone\\napplications in a manner akin to human behavior.\\nWe first describe the experimental environment and\\naction space, which are foundational elements of\\nour system. Next, we discuss the exploration phase,\\nwhere the agent learns app functionalities either\\nthrough autonomous interactions or by observing\\nhuman demonstrations. Finally, we outline the de-\\nployment phase, explaining how the agent applies\\nits acquired knowledge to execute high-level tasks.\\n3.1\\nEnvironment and Action Space\\nExperimental Environment: Our experimental\\nenvironment is built on a command-line interface\\n(CLI), allowing the agent to interact with smart-\\nphone apps. We chose the Android operating sys-\\ntem for our experiments. The agent receives two\\nkey inputs: a real-time screenshot showing the\\napp’s interface and an XML file detailing the in-\\nteractive elements. To enhance the agent’s ability\\nto identify and interact with these elements seam-\\nlessly, we assign each element a unique identifier.\\nThese identifiers are derived either from the re-\\nsource ID in the XML file (if provided) or are con-\\nstructed by combining the class name, size, and\\ncontent of the element. These elements are over-\\nlaid as semi-transparent numbers on the screenshot.\\nThis helps the agent to interact accurately without\\nneeding to specify exact positions on the screen\\nand enhances the agent’s precision in controlling\\nthe phone.\\nAction Space: Our agent’s action space mir-\\nrors common human interactions with smartphones:\\ntaps and swipes. We designed four basic functions:\\n• Tap(element : int) : This function simu-\\nlates a tap on the UI element numbered on\\nthe screen. For example, tap(5) would tap\\nthe element labeled ‘5’.\\n• Long_press(element : int) : This func-\\ntion emulates a long press (for 1 second) on a\\nUI element.\\n• Swipe ( element : int, direction : str,\\ndist : str): It allows the agent to swipe on\\nan element in a specified direction (up, down,\\nleft, right) and distance (short, medium, long).\\nFor instance,\\nswipe(21, “up”, “medium”)\\nwould swipe up on element ‘21’ for a medium\\ndistance.\\n• Text(text : str) : To bypass inefficient vir-\\ntual keyboard typing, this function inputs\\ntext directly into an input field when a\\nvirtual keyboard is visible.\\nFor exam-\\nple, text(“Hello, world!”) inputs the string\\n“Hello, world!\".\\n• Back() : A system-level function that helps\\nthe agent return to the previous UI page, espe-\\ncially useful for exiting irrelevant pages.\\n4\\n• Exit() : A specialized function is employed\\nto conclude processes, typically invoked upon\\nsuccessful task completion.\\nThese predefined actions are designed to sim-\\nplify the agent’s interactions, particularly by elim-\\ninating the need for precise screen coordinates,\\nwhich can pose challenges for language models\\nin accurately predicting.\\n3.2\\nExploration Phase\\nExploring by autonomous interactions. The Ex-\\nploration Phase is central to our framework. Here,\\nthe agent learns about the functionalities and fea-\\ntures of smartphone apps through trial and error. In\\nthis phase, the agent is assigned a task and starts\\ninteracting autonomously with the UI elements. It\\nuses different actions and observes the resulting\\nchanges in the app interface to understand how\\nit works. The agent, driven by a large language\\nmodel, attempts to figure out the functions of UI\\nelements and the effects of specific actions by an-\\nalyzing screenshots before and after each action.\\nThis information is compiled into a document that\\nrecords the effects of actions applied to different el-\\nements. When a UI element is acted upon multiple\\ntimes, the agent will update the document based\\non past documents and current observations to im-\\nprove quality. To make exploration more efficient,\\nthe agent stops further exploring UI elements if the\\ncurrent UI page seems unrelated to the main tasks\\nof the app, like advertisement pages. In such cases,\\nit uses the Android system’s Back() function to\\nreturn to the previous UI page. Compared with\\nrandom exploration, such as Depth-First Search\\nand Breadth-First Search, this goal-oriented explo-\\nration approach ensures that the agent focuses on\\nelements crucial for the effective operation of the\\napp. The agent also utilizes the LLM’s existing\\nknowledge about user interfaces to improve explo-\\nration efficiency. The exploration stops when the\\nagent completes the assigned task.\\nExploring by watching demos. An alternative\\nand often more effective exploration method in-\\nvolves the agent observing human demonstrations.\\nThese demonstrations provide the agent with ex-\\namples of efficient app usage, especially for un-\\nderstanding complex functionalities that might be\\nchallenging to discover through autonomous inter-\\nactions. In this method, a human user operates\\nthe apps while the agent observes, recording only\\nthe elements and actions employed by the human.\\nThis strategy narrows down the exploration space\\nand prevents the agent from engaging with irrele-\\nvant app pages, making it a more streamlined and\\nefficient approach compared to autonomous inter-\\nactions.\\n3.3\\nDeployment Phase\\nFollowing the exploration phase, the agent is well-\\nequipped to execute complex tasks based on its\\naccrued experience. The agent adheres to a step-\\nby-step approach when given a task, with each step\\nencompassing access to a screenshot of the current\\nUI and a dynamically generated document detail-\\ning the functions of UI elements and the actions’\\neffects on the current UI page. The prompts also\\nprovide detailed explanations of all available ac-\\ntions. In each step, the agent is first tasked with\\nproviding its observations of the current UI, fol-\\nlowed by articulating its thought process concern-\\ning the task and current observations. Subsequently,\\nthe agent proceeds to execute actions by invoking\\navailable functions. After each action, the agent\\nsummarizes the interaction history and the actions\\ntaken during the current step. This information is\\nincorporated into the next prompt, which provides\\nthe agent with a form of memory. This meticulous\\napproach enhances the reliability and interpretabil-\\nity of the agent’s actions, thereby facilitating more\\ninformed decision-making. The deployment phase\\nstops when the agent determines that the task has\\nbeen accomplished, at which point it can exit the\\nprocess by taking the Exit() action.\\n4\\nExperiments\\nIn this section, we will present our evaluation of\\nthe multimodal agent framework through a combi-\\nnation of quantitative and qualitative experiments.\\nOur primary goal is to assess the agent’s perfor-\\nmance and its ability to operate a diverse set of\\nsmartphone applications effectively.\\n4.1\\nExperimental Setup\\nTo comprehensively evaluate our method, we con-\\nstruct a benchmark that includes 10 popular appli-\\ncations, each serving various purposes. These ap-\\nplications include Google Maps, Twitter, Telegram,\\nYouTube, Spotify, Yelp, Gmail, TEMU, Clock, and\\nLightroom. We have intentionally chosen this di-\\nverse set of apps to test the agent’s adaptability\\nacross various functions and interfaces. In particu-\\nlar, to gain a more comprehensive insight into the\\n5\\nThe search field is focused and a keyboard is \\npresent on the lower half of the screen. UI \\nelement labeled \\'6\\' appears to be the search input \\nfield where text can be input to find locations, \\naddresses, or points of interest.\\nObservation\\nThought\\nAction\\ntext(\"Tencent Shanghai Branch\")\\nTo complete the given task, I should enter the \\ntext \"Tencent Shanghai Branch\" in the search \\nfield to proceed with the search for the \\ndestination.\\nA keyboard and a search result for \"Tencent Shanghai \\nBranch\" with address details and a distance measurement \\nare displayed.\\nObservation\\nThought\\nAction\\ntap(1)\\nTo complete the given task, I should confirm the search \\nfor \"Tencent Shanghai Branch\". In this screenshot, the \\nsearch confirm button is represented by the blue button\\nwith the text “\\\\u641c\\\\u7d22” (“Search” in Chinese) on it.\\nThe screenshot shows a map application with the \\nlocation \"Tencent Shanghai Branch\" pinpointed \\nwith a red marker. There are two buttons at the \\nbottom of the screen, one labeled \"Directions\" and \\nthe other labeled \"Start\", with numeric tags 1 and \\n3 respectively.\\nObservation\\nThought\\nAction\\ntap(2)\\nTo complete the given task, which is to \\nnavigate to Tencent Shanghai Branch, I should \\ntap the \"Directions\" button to initiate the \\nroute finding process.\\nObservation\\nThe recipient\\'s email address is already filled \\nin, and the subject line reads \"Inquiry About \\nYour New Position.\" The body of the email is\\nnot visible, and there\\'s a UI element labeled \\n\\'11\\' which is used to collapse the keyboard.\\nThought\\nAction\\ntap(11)\\nTo complete the task, the next step should be \\nto collapse the keyboard so that I can access \\nthe body of the email to type the message to \\nJane Doe asking about her new job.\\nObservation\\nThe recipient\\'s email address is already filled in as \\njanedoe@email.com and the subject line is \"Inquiry \\nAbout Your New Position.\" The cursor is blinking in \\nthe main body area of the email, indicating that it is \\nready to receive text input. The on-screen keyboard is \\nalso visible.\\nThought\\nAction\\ntext(\"Dear Jane Doe,\\\\n\\\\nI am writing to inquire \\nabout your new position that you recently started…\")\\nTo complete the given task, the next step I should do is \\nto type the content of the email asking Jane Doe about \\nher new job.\\nObservation\\nThe email body is typed with a message asking \\nJane Doe about her recent job and expressing \\neagerness to hear from her. There is a send \\nbutton labeled with the numeric tag \\'3’.\\nThought\\nAction\\ntap(3)\\nTo complete the given task, the next step I \\nshould take is to send the email by tapping \\nthe send button.\\nObservation\\nThe screenshot shows a photo editing application \\ninterface with various adjustment settings for \\nexposure, contrast, highlights, and shadows. \\nThese adjustments have numeric tags 20, 21, 22, \\nand 23 respectively. The settings are currently at \\ntheir default values, and the image appears \\nunderexposed, needing improvement in lighting.\\nThought\\nAction\\nswipe(20, \"right\", \"medium\")\\nTo improve the image quality and correct the \\nunderexposure, I need to increase the \\nexposure. The slider for the exposure setting \\nis labeled with numeric tag 20, which I can \\ninteract with to adjust the exposure level. I will \\nswipe right on this slider to increase the \\nexposure.\\nObservation\\nThe exposure level has been increased, as indicated by the \\nslider tagged with number 20 being to the right. The \\ncontrast, highlights, and shadows are yet to be adjusted, \\nwith their sliders at the neutral position. There are also \\nother adjustment options available, such as color and detail, \\nwhich could further improve the image quality if needed.\\nThought\\nAction\\nswipe(21, \"right\", \"short\")\\nThe next step to improve the quality of the image could be \\nto adjust the contrast, as increasing contrast can often \\nmake an image appear more vivid and can complement \\nthe increased exposure. Based on the documentation, \\ninteracting with UI element labeled 21 will adjust the \\ncontrast level of the image.\\nObservation\\nThe exposure and contrast sliders are visible, \\nwith the exposure increased to +2.20 and the \\ncontrast increased to +24. The highlights and\\nshadows sliders are at 0, indicating no \\nadjustment has been made to them.\\nThought\\nAction\\ntap(22)\\nSince I have previously increased the exposure \\nand adjusted the contrast slightly, the next step \\ncould be to adjust the highlights or shadows to \\nfurther improve the image quality. Adjusting \\nthe highlights might help in bringing out more \\ndetail in the brighter parts of the image \\nwithout affecting the overall exposure too much.\\nGoogle Maps: navigate to Tencent Shanghai Branch\\nGmail: send an email to janedoe@email.com to ask her about her new job\\nLightroom: improve the quality of this image until it looks good to you\\nFigure 3: Qualitative Task Evaluation Across Three Apps.\\nThis figure presents qualitative results for three\\ndistinct tasks conducted on Google Maps, Gmail, and Lightroom. It showcases AppAgent’s ability to accurately\\nperceive, reason, and execute tasks, demonstrating its competence in various application contexts. Due to space\\nconstraints, some less critical details have been omitted from the description.\\nvision capabilities of our agent, we conducted an\\nin-depth case study using Adobe Lightroom, an\\nimage-editing application. This specific case study\\nallowed us to evaluate the agent’s proficiency in\\nhandling visual tasks and its ability to interpret\\nand manipulate images within the app. For the\\nexploration phase, we capped the maximum num-\\nber of steps at 40. During testing, we limited the\\nmaximum number of steps to 10. For these experi-\\nments, we utilized the state-of-the-art multimodal\\nlarge language model, GPT-4. GPT-4 is equipped\\nto process interleaved image-and-text inputs effec-\\ntively. This unique capability enables our agent to\\ninterpret and interact with both visual and textual\\ninformation seamlessly within the applications.\\n4.2\\nDesign and Analysis\\nBaselines. To comprehensively evaluate our mul-\\ntimodal agent framework, we considered various\\ndesign choices and their impact on performance.\\nWe conducted experiments using different configu-\\nrations to provide valuable insights into the agent’s\\nbehavior. We started with GPT-4 without any ref-\\nerence documents during testing and examined its\\nperformance both with the raw action API and\\nour simplified action space. Next, we explored\\ndifferent ways to generate guiding documents for\\nthe agent. These included documents generated\\nthrough autonomous exploration, watching human\\ndemonstrations, and the manually crafted docu-\\nment as an oracle benchmark.\\nTo effectively compare the performance of dif-\\n6\\nMethod\\nDocument\\nAction Space\\nSR ↑\\nReward ↑\\nAvg. Steps\\nGPT4 (Baseline)\\nNone\\nRaw\\n2.2%\\n0.6\\n4.0\\nNone\\nOurs\\n48.9%\\n3.5\\n6.9\\nAppAgent\\nAuto. Exploration\\nOurs\\n73.3%\\n5.1\\n4.4\\nWatching Demos\\nOurs\\n84.4%\\n4.7\\n5.1\\nManually Crafted\\nOurs\\n95.6%\\n5.5\\n5.5\\nTable 1: Evaluating Design Choices in AppAgent Performance. This table contrasts different design elements\\nwithin AppAgent. Key findings include: our custom-developed action space surpasses the raw action space in effi-\\nciency; the exploration phase, incorporating both autonomous interaction and observation of human demonstrations,\\nsignificantly enhances agent performance; and the auto-generated documentation yields outcomes on par with those\\nderived from manually crafted documents.\\nMethod\\nDocument\\nAction Space\\nAvg. Rank ↓\\nNum. Tools\\nGPT4 (Baseline)\\nNone\\nOurs\\n2.30\\n2.4\\nAppAgent\\nWatching Demos\\nOurs\\n1.95\\n5.8\\nManually Crafted\\nOurs\\n1.75\\n4.0\\nTable 2: Case study on image editing tasks with Lightroom App. We conduct a user study to rank the image\\nediting results of different methods. Our agents produce better results than the GPT-4 baseline.\\nferent methods, we employed three key metrics:\\nSuccessful Rate (SR): This metric measures the\\naverage rate at which the agent successfully com-\\npletes tasks within an app. If the agent fails to\\nfinish the task in 10 steps, it is considered a failure.\\nReward: To provide a more fine-grained measure-\\nment, we developed a reward model to assess per-\\nformance. For each task within an app, we scored\\ndifferent UI pages. The closer the UI page was to\\nthe objective, the higher the score received. This\\nmeans that even if the agent failed to complete the\\ntask, it would still receive credit based on its final\\nstate.\\nAverage Steps: We also reported the average num-\\nber of steps required to successfully finish tasks\\nacross the selected applications.\\nResults. The comparison of our experimental re-\\nsults is presented in Table 1. We report the average\\nperformance of 45 tasks on 9 of the 10 previously\\ndescribed apps. Notably, we excluded Lightroom\\nfrom this evaluation, as assessing task completion\\nin this application presented inherent ambiguities.\\nAs demonstrated, our simplified action space sig-\\nnificantly improves the performance of the GPT-4\\nbaseline. Our observations indicate that LLM strug-\\ngles with producing accurate xy coordinates, while\\nour simplified action space eliminates this challeng-\\ning requirement. Additionally, documents gener-\\nated through autonomous exploration and observ-\\ning human demonstrations proved to be highly ef-\\nfective. Their results consistently outperformed the\\nGPT-4 baseline and are comparable to the results\\nof human-written documents, which highlights the\\nefficacy of our design in enhancing the agent’s per-\\nformance across a diverse set of applications.\\nQualitative results. In Fig. 3, we provide ex-\\namples showcasing the agent’s execution process\\nfor various tasks. This qualitative analysis serves\\nto demonstrate the agent’s capacity to accurately\\nperceive, reason, and act in response to given tasks.\\nFor a more comprehensive understanding of our\\nagent’s capabilities, please refer to our project page,\\nwhich includes additional demonstration videos.\\n4.3\\nCase Study\\nTo gain deeper insights into the vision capabili-\\nties of our agent, we conducted an extensive case\\nstudy using Adobe Lightroom, an image-editing\\napplication. This specific case study allowed us\\nto evaluate the agent’s proficiency in handling vi-\\nsual tasks, which was previously impossible for\\ntext-only agent models. Lightroom, as an image-\\nediting app with various editing tools, demands a\\nwide range of operations, such as selecting appro-\\npriate tools and manipulating image parameters.\\nThis case study provides a robust evaluation of the\\nagent’s overall capabilities. Additionally, the open-\\nended nature of image editing tasks allows us to\\n7\\nassess the agent’s problem-solving abilities. We\\nprepared five images with visual issues, such as\\nlow contrast and overexposure. Various variants\\nof our model, as previously illustrated, were used\\nto edit these images. A user study was conducted\\nto rank the editing results produced by different\\nmethods. We also reported the average number of\\ntools used for image editing, providing an addi-\\ntional reference to the editing process’s complexity.\\nAll models were assigned the task of “fix this image\\nuntil it looks good to you” without specifying the\\nimage’s problems. The comparison of the results\\nis presented in Table 2. As we can see, our agent\\nmodel with documents yields consistently better\\nresults than the GPT-4 baseline, which emphasizes\\nthe influence of documents in our design. The gen-\\nerated documents by watching the demonstration\\nproduced comparable results with the results of\\nmanually crafted documents, which suggests the\\neffectiveness of the exploration phase. We also\\nfind that with a document, the agent tends to use\\nvarious tools to improve the image quality, while\\nthe GPT-4 baseline uses fewer tools.\\n5\\nConclusion\\nIn this paper, we have introduced a novel multi-\\nmodal agent framework that leverages the vision\\ncapabilities of large language models to operate\\nsmartphone applications in a human-like manner.\\nOur approach eliminates the need for system back-\\nend access and offers security, adaptability, and\\nflexibility advantages. Our exploration-based learn-\\ning strategy allows the agent to quickly adapt to\\nnew applications with unfamiliar user interfaces,\\nmaking it a versatile tool for various tasks. Our\\nextensive experiments across various apps high-\\nlight our agent’s ability to handle diverse high-level\\ntasks and underscore its adaptability and learning\\nefficiency.\\nLimitation. We have adopted a simplified action\\nspace for smartphone operations, which means that\\nadvanced controls such as multi-touch and irregu-\\nlar gestures are not supported. This limitation may\\nrestrict the agent’s applicability in some challeng-\\ning scenarios. Nevertheless, we recognize this as\\nan avenue for future research and development.\\nReferences\\nMichael Ahn, Anthony Brohan, Noah Brown, Yev-\\ngen Chebotar, Omar Cortes, Byron David, Chelsea\\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol\\nHausman, Alex Herzog, Daniel Ho, Jasmine Hsu,\\nJulian Ibarz, Brian Ichter, Alex Irpan, Eric Jang,\\nRosario Jauregui Ruano, Kyle Jeffrey, Sally Jes-\\nmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalash-\\nnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey\\nLevine, Yao Lu, Linda Luu, Carolina Parada, Pe-\\nter Pastor, Jornell Quiambao, Kanishka Rao, Jarek\\nRettinghouse, Diego Reyes, Pierre Sermanet, Nico-\\nlas Sievers, Clayton Tan, Alexander Toshev, Vincent\\nVanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,\\nMengyuan Yan, and Andy Zeng. 2022. Do as i can\\nand not as i say: Grounding language in robotic af-\\nfordances. In arXiv preprint arXiv:2204.01691.\\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen\\nChebotar, Xi Chen, Krzysztof Choromanski, Tianli\\nDing, Danny Driess, Avinava Dubey, Chelsea Finn,\\net al. 2023. Rt-2: Vision-language-action models\\ntransfer web knowledge to robotic control. arXiv\\npreprint arXiv:2307.15818.\\nAnthony Brohan, Noah Brown, Justice Carbajal, Yev-\\ngen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana\\nGopalakrishnan, Karol Hausman, Alex Herzog, Jas-\\nmine Hsu, et al. 2022. Rt-1: Robotics transformer\\nfor real-world control at scale.\\narXiv preprint\\narXiv:2212.06817.\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\\nGreg Brockman, et al. 2021.\\nEvaluating large\\nlanguage models trained on code. arXiv preprint\\narXiv:2107.03374.\\nMeta FAIR, Anton Bakhtin, Noam Brown, Emily Di-\\nnan, Gabriele Farina, Colin Flaherty, Daniel Fried,\\nAndrew Goff, Jonathan Gray, Hengyuan Hu, et al.\\n2022. Human-level play in the game of diplomacy\\nby combining language models with strategic reason-\\ning. Science, 378(6624):1067–1074.\\nHiroki Furuta, Kuang-Huei Lee, Ofir Nachum, Yutaka\\nMatsuo, Aleksandra Faust, Shixiang Shane Gu, and\\nIzzeddin Gur. 2023. Multimodal web navigation with\\ninstruction-finetuned foundation models.\\nIzzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa\\nSafdari, Yutaka Matsuo, Douglas Eck, and Aleksan-\\ndra Faust. 2023. A real-world webagent with plan-\\nning, long context understanding, and program syn-\\nthesis.\\nYucheng Han, Chi Zhang, Xin Chen, Xu Yang, Zhibin\\nWang, Gang Yu, Bin Fu, and Hanwang Zhang. 2023.\\nChartllama: A multimodal llm for chart understand-\\ning and generation.\\nSirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu\\nZheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang,\\nZili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang\\nZhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu,\\nand Jürgen Schmidhuber. 2023. Metagpt: Meta pro-\\ngramming for a multi-agent collaborative framework.\\n8\\nZhiting Hu and Tianmin Shu. 2023. Language mod-\\nels, agent models, and world models: The law for\\nmachine reasoning and planning.\\narXiv preprint\\narXiv:2312.05230.\\nYanda Li, Chi Zhang, Gang Yu, Zhibin Wang, Bin\\nFu, Guosheng Lin, Chunhua Shen, Ling Chen, and\\nYunchao Wei. 2023. Stablellava: Enhanced visual\\ninstruction tuning with synthesized image-dialogue\\ndata.\\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae\\nLee. 2023a. Improved baselines with visual instruc-\\ntion tuning.\\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\\nLee. 2023b. Visual instruction tuning.\\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu\\nLei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen\\nMen, Kejuan Yang, Shudan Zhang, Xiang Deng, Ao-\\nhan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng\\nShen, Tianjun Zhang, Yu Su, Huan Sun, Minlie\\nHuang, Yuxiao Dong, and Jie Tang. 2023c. Agent-\\nBench: Evaluating LLMs as agents. arXiv preprint\\narXiv: 2308.03688.\\nOpenAI. 2021.\\nChatgpt.\\nhttps://openai.com/\\nresearch/chatgpt.\\nOpenAI. 2023. Gpt-4 technical report.\\nJoon Sung Park, Joseph O’Brien, Carrie Jun Cai, Mered-\\nith Ringel Morris, Percy Liang, and Michael S Bern-\\nstein. 2023. Generative agents: Interactive simulacra\\nof human behavior. In Proceedings of the 36th An-\\nnual ACM Symposium on User Interface Software\\nand Technology, pages 1–22.\\nChen Qian, Xin Cong, Cheng Yang, Weize Chen,\\nYusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong\\nSun. 2023. Communicative agents for software de-\\nvelopment. arXiv preprint arXiv:2307.07924.\\nScott Reed, Konrad Zolna, Emilio Parisotto, Ser-\\ngio Gomez Colmenarejo,\\nAlexander Novikov,\\nGabriel Barth-Maron, Mai Gimenez, Yury Sulsky,\\nJackie Kay, Jost Tobias Springenberg, et al. 2022. A\\ngeneralist agent. arXiv preprint arXiv:2205.06175.\\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\\nWeiming Lu, and Yueting Zhuang. 2023. Hugging-\\ngpt: Solving ai tasks with chatgpt and its friends\\nin huggingface. In Advances in Neural Information\\nProcessing Systems.\\nChunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang,\\nZishan Qin, and Stephen Gould. 2023. 3d-gpt: Proce-\\ndural 3d modeling with large language models. arXiv\\npreprint arXiv:2310.12945.\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\\nAn instruction-following llama model.\\nhttps://\\ngithub.com/tatsu-lab/stanford_alpaca.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. 2023a. Llama: Open\\nand efficient foundation language models.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\\nScialom. 2023b. Llama 2: Open foundation and\\nfine-tuned chat models.\\nZihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jin-\\nbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He,\\nZilong Zheng, Yaodong Yang, Xiaojian Ma, and\\nYitao Liang. 2023.\\nJarvis-1: Open-world multi-\\ntask agents with memory-augmented multimodal lan-\\nguage models.\\nZhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen\\nDing, Boyang Hong, Ming Zhang, Junzhe Wang,\\nSenjie Jin, Enyu Zhou, et al. 2023. The rise and\\npotential of large language model based agents: A\\nsurvey. arXiv preprint arXiv:2309.07864.\\nTianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Lu-\\noxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao,\\nQian Liu, Che Liu, Leo Z. Liu, Yiheng Xu, Hongjin\\nSu, Dongchan Shin, Caiming Xiong, and Tao Yu.\\n2023. Openagents: An open platform for language\\nagents in the wild.\\nYuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xi-\\naolong Wang, Weidong Liu, and Yang Liu. 2023.\\nExploring large language models for communica-\\ntion games: An empirical study on werewolf. arXiv\\npreprint arXiv:2309.04658.\\nAn Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin,\\nLinjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong,\\nJulian McAuley, Jianfeng Gao, Zicheng Liu, and\\nLijuan Wang. 2023. Gpt-4v in wonderland: Large\\nmultimodal models for zero-shot smartphone gui nav-\\nigation. arXiv preprint arXiv: 2311.07562.\\n9\\nHui Yang, Sifu Yue, and Yunzhong He. 2023a. Auto-\\ngpt for online decision making: Benchmarks and\\nadditional opinions.\\nZhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng\\nWang, Chung-Ching Lin, Zicheng Liu, and Lijuan\\nWang. 2023b.\\nThe dawn of lmms: Preliminary\\nexplorations with gpt-4v (ision).\\narXiv preprint\\narXiv:2309.17421.\\nZhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang,\\nChung-Ching Lin, Zicheng Liu, and Lijuan Wang.\\n2023c.\\nThe dawn of lmms: Preliminary explo-\\nrations with gpt-4v(ision).\\narXiv preprint arXiv:\\n2309.17421.\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\\nShafran, Karthik Narasimhan, and Yuan Cao. 2023.\\nReAct: Synergizing reasoning and acting in language\\nmodels. In ICLR.\\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\\nWendi Zheng, Xiao Xia, et al. 2022.\\nGlm-130b:\\nAn open bilingual pre-trained model. arXiv preprint\\narXiv:2210.02414.\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\\nZhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,\\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging\\nllm-as-a-judge with mt-bench and chatbot arena.\\n10\\n',\n",
       " '[\\n    {\\n        \"title\": \"joke\",\\n        \"date_created\": \"1974\",\\n        \"art_writing_type\": \"Article\",\\n        \"dimensions\": {\\n            \"pages\": 30,\\n            \"page_format\": \".pdf\",\\n            \"file_size\": \"2.34mb\"\\n        },\\n        \"description\": \"writing\",\\n        \"relevance\": \"no more\",\\n        \"themes\": \"artistic\",\\n        \"keywords\": \"pupils\",\\n        \"abstract\": \"noun\",\\n        \"artworks_or_artists\": \"none\",\\n        \"citation_information\": \"horn\",\\n        \"supplementary_materials\": \"ww.ee.rn\",\\n        \"image_credits\": \"me\",\\n        \"video_credits\": \"me\",\\n        \"related_publications\": \"me\",\\n        \"recommended_readings\": \"me\",\\n        \"references\": \"me\",\\n        \"external_links\": \"me\",\\n        \"journal_details\": {\\n            \"journal_name\": \"me\",\\n            \"publisher\": \"me\",\\n            \"volume_and_issue\": \"me\",\\n            \"page_range\": \"23\",\\n            \"issn_or_isbn\": \"2mr=e223m2\",\\n            \"doi\": \"dd\",\\n            \"date_of_publication\": \"me\"\\n        },\\n        \"key_takeaways\": {\\n            \"1\": \"me\",\\n            \"2\": \"me\",\\n            \"3\": \"me\"\\n        },\\n        \"personal_influence\": \"me\",\\n        \"personal_journey\": \"me\",\\n        \"academic_review\": {\\n            \"reviewer_1_name\": \"me\",\\n            \"reviewer_2_name\": \"me\",\\n            \"review_summary\": \"me\"\\n        },\\n        \"overall_evaluation\": \"me\"\\n    },\\n    {\\n        \"title\": \"joke\",\\n        \"date_created\": \"1974\",\\n        \"art_writing_type\": \"Article\",\\n        \"dimensions\": {\\n            \"pages\": 30,\\n            \"page_format\": \".pdf\",\\n            \"file_size\": \"2.34mb\"\\n        },\\n        \"description\": \"writing\",\\n        \"relevance\": \"no more\",\\n        \"themes\": \"artistic\",\\n        \"keywords\": \"pupils\",\\n        \"abstract\": \"noun\",\\n        \"artworks_or_artists\": \"none\",\\n        \"citation_information\": \"horn\",\\n        \"supplementary_materials\": \"ww.ee.rn\",\\n        \"image_credits\": \"me\",\\n        \"video_credits\": \"me\",\\n        \"related_publications\": \"me\",\\n        \"recommended_readings\": \"me\",\\n        \"references\": \"me\",\\n        \"external_links\": \"me\",\\n        \"journal_details\": {\\n            \"journal_name\": \"me\",\\n            \"publisher\": \"me\",\\n            \"volume_and_issue\": \"me\",\\n            \"page_range\": \"23\",\\n            \"issn_or_isbn\": \"2mr=e223m2\",\\n            \"doi\": \"dd\",\\n            \"date_of_publication\": \"me\"\\n        },\\n        \"key_takeaways\": {\\n            \"1\": \"me\",\\n            \"2\": \"me\",\\n            \"3\": \"me\"\\n        },\\n        \"personal_influence\": \"me\",\\n        \"personal_journey\": \"me\",\\n        \"academic_review\": {\\n            \"reviewer_1_name\": \"me\",\\n            \"reviewer_2_name\": \"me\",\\n            \"review_summary\": \"me\"\\n        },\\n        \"overall_evaluation\": \"me\"\\n    },\\n    {\\n        \"title\": \"joke\",\\n        \"date_created\": \"1974\",\\n        \"art_writing_type\": \"Article\",\\n        \"dimensions\": {\\n            \"pages\": 30,\\n            \"page_format\": \".pdf\",\\n            \"file_size\": \"2.34mb\"\\n        },\\n        \"description\": \"writing\",\\n        \"relevance\": \"no more\",\\n        \"themes\": \"artistic\",\\n        \"keywords\": \"pupils\",\\n        \"abstract\": \"noun\",\\n        \"artworks_or_artists\": \"none\",\\n        \"citation_information\": \"horn\",\\n        \"supplementary_materials\": \"ww.ee.rn\",\\n        \"image_credits\": \"me\",\\n        \"video_credits\": \"me\",\\n        \"related_publications\": \"me\",\\n        \"recommended_readings\": \"me\",\\n        \"references\": \"me\",\\n        \"external_links\": \"me\",\\n        \"journal_details\": {\\n            \"journal_name\": \"me\",\\n            \"publisher\": \"me\",\\n            \"volume_and_issue\": \"me\",\\n            \"page_range\": \"23\",\\n            \"issn_or_isbn\": \"2mr=e223m2\",\\n            \"doi\": \"dd\",\\n            \"date_of_publication\": \"me\"\\n        },\\n        \"key_takeaways\": {\\n            \"1\": \"me\",\\n            \"2\": \"me\",\\n            \"3\": \"me\"\\n        },\\n        \"personal_influence\": \"me\",\\n        \"personal_journey\": \"me\",\\n        \"academic_review\": {\\n            \"reviewer_1_name\": \"me\",\\n            \"reviewer_2_name\": \"me\",\\n            \"review_summary\": \"me\"\\n        },\\n        \"overall_evaluation\": \"me\"\\n    }\\n]',\n",
       " 'ChatART\\nGet immediate insights, find artistic inspiration, acquire new\\nknowledge, discover the perfect artwork.\\nChatART simplifies art research. Ask questions, seek\\nrecommendations, or engage in discussions about new artistic\\nmovements – all within the comfort of a conversational interface.\\nOverview\\nChatART is an expert chatbot designed to simplify art research. It provides a\\nconversational interface for users to ask questions, seek recommendations, and\\nengage in discussions about new artistic movements. Powered by expert training\\ndata curated by professional art researchers, ChatART stands as the ﬁrst art expert\\nchatbot, offering precision and niche insights beyond the ordinary.\\nKey Features:\\n●\\n🎨 Personalized Insights: ChatART provides personalized, intelligent insights\\ninto high-quality artistic knowledge, new artworks, young artists\\' practices,\\ntrending aesthetics and emerging art movements.\\n●\\n🤖 Expert Training Data: Powered by expert training data curated, veriﬁed, and\\nregularly updated by professional art researchers, ChatART ensures accuracy\\nand relevance in every conversation.\\n●\\n🌐 Conversational Interface: Engage in seamless, intelligent conversations\\nabout art through an intuitive conversational interface.\\nEmpowering Art Professionals:\\nNew Base empowers users and clients to revolutionize art labeling, management,\\nand data recreation. For young artists, it facilitates visibility within the AI industry,\\nwhile assisting art professionals in making informed decisions and enhancing their\\nbusinesses with curated, high-quality art data.\\nMission\\nOur mission is to verify, curate, update, and maintain AI training datasets, ensuring\\nthe highest quality and relevance in the ﬁeld of ﬁne arts. We aim to seamlessly\\nintegrate young artists with the AI industry, fostering creativity and innovation.\\nGoal\\nChatART\\'s goal is to empower art professionals, from collectors to curators and art\\ntherapists, by providing personalized insights, trending art practices, and aesthetics.\\nWe aim to shape the future of art labeling, management, and data recreation, making\\nart history more inclusive, transparent, and accessible.\\nHow It Works\\nChatART facilitates an easy onboarding process for artists and art writers through\\nmetadata templates. Artists can apply artworks, while art writers can contribute\\narticles and publications. This structured data is crucial for building a high-quality\\ntraining database for AI, enhancing the accuracy and relevance of ChatART\\'s\\nresponses.\\nBeneﬁts Examples:\\n●\\nDiscover New Artists\\n●\\nDiscuss Art Movements\\n●\\nEvaluate Artwork Authenticity\\n●\\nCurate Art Exhibitions\\n●\\nStay Informed about Art Market Trends\\n●\\nExplore Art Therapy Techniques\\n●\\nDecode Symbolism in Art\\n●\\nReceive Art Material Recommendations\\n●\\nExplore Art and Technology Integration\\n●\\nContribute to Digital Art Preservation\\n●\\nAccess Art Educational Resources\\n●\\nEnhance Art World Networking\\nTarget Users\\nChatART is tailored for art professionals, including curators, art collectors, museum\\ndirectors, art therapists, urban planners, creatives, journalists, teachers, and\\nresearchers.\\nMarket Opportunity\\nThe market opportunity lies in catering to art enthusiasts, students, educators, and\\ninstitutions seeking innovative ways to explore and understand ﬁne arts.\\nCollaborations with galleries, museums, and technology companies interested in AI\\nand the arts present additional potential.\\nSubscription Model:\\nChatART offers a subscription-based service, allowing users to pay a recurring fee\\nfor access to ﬁne-tuning capabilities. This model includes various tiers based on\\ncustomization, storage, or processing power.\\nChatART envisions a future where AI-driven insights and curated data redeﬁne the\\nlandscape of ﬁne arts, making it more accessible, inclusive, and engaging for users\\nworldwide.\\nAdditional Information: Metadata about artists,\\nartworks, art writer, publications\\nArtist Information\\n●\\nArtist Name: `<Your Name>`\\n●\\nDescription: `<Brief description of yourself and your artistic focus>`\\n●\\nArtist Website: `<Your website URL, e.g. https://yourname.example.com>`\\n●\\nArtist Biography: `<A comprehensive biography detailing your art journey,\\neducation, and notable achievements, milestones, exhibitions, and projects>`\\n●\\nArtist Statement: `<A statement outlining your artistic philosophy, intent,\\ninterests, and vision, creative process, inspiration, and conceptual approach to\\nyour work. Incorporate elements that hold personal meaning for you, e.g. from\\nnature.>`\\n●\\nArtist Media: `<List of your favorite magazines, blogs, proﬁles, forums, pages,\\nstreams, youtube channels or exhibitions that have inﬂuenced your work, e.g.\\n[Blog](https://www.blog.com/), Etc.>`\\n●\\nArtist Interests: `<List any interests or passions that may inﬂuence or inform\\nyour artistic work, especially in the ﬁelds of design, architecture, music,\\nfashion, etc., e.g. metabolism, jazz, sustainable fashion>`\\n●\\nArtistic Process: <Outline your creative process, from initial idea to ﬁnished\\nartwork, explaining the steps, decisions, and challenges you encountered\\nalong the way>\\n●\\nPersonal Signiﬁcance: \\\\`<Share any personal connections or meanings that\\nyour work holds for you as the artist>\\\\`\\n●\\nReviews and Critical Reception: <Include notable reviews, critiques, or analysis\\nof your artwork from art critics, journalists, or scholars, and any positive or\\nnegative feedback that you have received or that you can remember>\\n●\\nArtist Contact Information: `<Your preferred contact details, like email\\naddress, phone number or social media handles>`\\n●\\nReferences or external links that provide additional context and background\\ninformation: `<Relevant publications, interviews, articles, or resources where\\nyour work has been featured, e.g. [Article\\nTitle](https://example.com/article-url)>`\\nArtwork Information\\n●\\nArtwork Title: `<Artwork Title>`\\n●\\nDate Created: `<Year of Creation>`\\n●\\nArt Medium: `<Art Medium e.g. \"Oil on Canvas\", “Sculpture”>`\\n●\\nArtform: `<Speciﬁc Artform or Technique e.g. \"Photography\", \"Sculpture\">`\\n●\\nDimensions: `<Height x Width x Depth (in inches or cm) for physical works or\\nﬁle size and resolution for digital works>`\\n- Width: `<Width in cm, in, or virtual units (e.g., px)>`\\n- Height: `<Height in cm, in, or virtual units (e.g., px)>`\\n- Depth: `<Depth in cm, in, or an indication such as \"N/A\" if not applicable>`\\n●\\nDescription of Artwork: `<A brief visual description of your artwork from a\\nnaive perspective, including its details, concept, colors, unique features,\\ntechniques, subject matter, themes, goals, contextual elements etc.>`\\n●\\nArt Movement: `<The art movement or style your work belongs to or is\\ninﬂuenced by, e.g., \"Bio Art\", \"Surrealism\", \"Ai Art\", \"Post Digital Art\", \"Mixed\\nMedia\", “Immersive Art”, “Land Art”>`\\n●\\nInspirations: `<List music, artists, works, events or other sources of inspiration\\nthat you like or that inﬂuenced your artwork>\\\\`\\n●\\nArtistic Techniques: `<Describe key techniques, materials, tools or processes\\nused in the creation of your artwork>\\\\`\\n●\\nMaterials: `<List the materials used in the creation of your artwork, such as\\npaint, clay, wood, metal, etc.>`\\n●\\nSubject Matter : `<Brieﬂy describe the subject matter of your artwork and its\\nsigniﬁcance within the context of your artistic practice>`\\n●\\nContextual Elements: `<Describe any objects, themes, images, networks,\\nhistorical, cultural, social, or political context that informed or inﬂuenced your\\nartwork>`\\n●\\nMotifs: `<List any recurring motifs and metaphors or patterns present in your\\nartistic process and explain their meaning or signiﬁcance>`\\n●\\nCollaborators: `<If your artwork involved collaborations with other artists or\\nprofessionals, provide their names and their roles in the project>`\\n●\\nSymbols: `<List any symbols used in your artwork and explain their meaning\\nor signiﬁcance>`\\n●\\nColor Palette: `<Describe the color palette and the reasoning behind your\\nchoice of colors>`\\n●\\nAttributes: `<List of attributes of your work, such as hand crafted, synthetic,\\nnatural, customizable, limited edition, one-of-a-kind, or scalable>\\\\`\\n●\\nCharacteristics: `<List the distinctive characteristics of your artwork, such as\\ntexture, form, scale, style, or visual elements that contribute to its overall look\\nand feel>`\\n●\\nMood: \\\\`<Describe the mood, emotional or mental state you were in while\\ncreating the artwork, and how it inﬂuenced the ﬁnal piece>\\\\`\\n●\\nTextures:`<Describe the textures present in your artwork, such as rough,\\nsmooth, layered, or tactile, and explain their importance or contribution to the\\noverall experience of the piece>`\\n●\\nFraming: `<Indicate whether your artwork is framed or unframed, and provide\\ndetails about the frame, such as materials, colors, and dimensions>`\\n●\\nStyle Inﬂuences: `<List general styles, artists, or works that have inﬂuenced\\nyour artistic style and approach to your artwork>`\\n●\\nTechnique Training: `<Mention any courses, workshops, or mentors who have\\nguided your development of the techniques used in your artwork>`\\n●\\nThemes:`<List the themes explored in your artwork, such as identity,\\nsimulation, nature, spirituality, technology, healing, meditation, algorithms,\\nnetworking, smart materials, sustainability, ecosystems, stock market or\\nsocial issues>`\\n●\\nConcept: `<Explain the central concept or idea behind your artwork, and how it\\nrelates to your artistic practice and vision>`\\n●\\nEvolution: `<Describe how your artwork has evolved over time, and the factors\\nthat have shaped its development>`\\n●\\nConservation: `<Information on the artwork\\'s durability, maintenance, and\\nrecommended conservation techniques to preserve its longevity and beauty>`\\n●\\nScenarios: `<Describe how your artwork behaves in different lighting\\nconditions or its interactions with viewers, e.g. when it is not observed, and\\nwhat are scenario-related concepts in your artwork such as the relationship\\nbetween presence and absence, the role of the viewer in experiencing the\\nartwork, and the transformative nature of different environments?>`\\n●\\nSettings:`<Discuss how your artwork looks, feels, or interacts in various\\nsettings, such as galleries, homes, outdoors, or public spaces, and the impact\\nthese settings could have on the viewer\\'s experience and perception of the\\npiece>\\\\`\\n●\\nExhibition History:`<Details of previous exhibitions or venues where your\\nartwork has been presented>\\\\`\\n●\\nAwards and Recognitions: `<List any awards, honors or recognitions your\\nartwork has received>`\\n●\\nCollections: `<Details of any collectors or institutions that own or have\\nexhibited your artwork>`\\n●\\nAdditional Keywords: `<List relevant keywords related to your artwork, helping\\nviewers to understand its content and themes>`\\n●\\nAdditional Information: `<Provide any other relevant information that might\\nhelp viewers better understand, appreciate, or engage with your artwork>`\\n●\\nReferences or external links that provide additional context and background\\ninformation: `<Relevant publications, interviews, articles, or resources where\\nyour work has been featured, e.g. [Article\\nTitle](https://example.com/article-url)>`\\n●\\nSound:`<If your artwork incorporates sound, describe the type of sound, its\\npurpose, and how it contributes to the overall experience of the piece>\\\\`\\n●\\nPlot:`<If your artwork tells a story or has a narrative, brieﬂy describe the plot\\nand how it unfolds within the artwork, as well as its signiﬁcance or connection\\nto the themes and concepts explored in the piece>\\\\`\\n●\\nSetting: The time, place, and environment in which the artwork was created,\\nproviding context and inﬂuencing the events and atmosphere.\\n●\\nConﬂict: The central problem or struggle that drives the narrative of the\\nartwork forward and creates tension, often involving obstacles or opposing\\nforces.\\n●\\nTone: The overall mood or attitude conveyed by the artwork, inﬂuencing the\\nobservers\\'s emotional response and perception of the art.\\n●\\nPoint of View: The perspective from which the artworks unfold, determining\\nthe level of access to thoughts, feelings, and the reliability of the narrator.\\n●\\nForeshadowing: Hints or clues about future events in the artworks narrative,\\ncreating anticipation and building suspense.\\n●\\nSymbolism: Objects, actions, or elements that represent deeper meanings or\\nconcepts within the artwork.\\n●\\nMotif: Recurring thematic elements or patterns that contribute to the overall\\nmeaning and coherence of the artwork.\\n●\\nLeitmotif: A recurring musical or thematic phrase associated with a particular\\nelement, idea, or situation in the artwork.\\n●\\nSubplots: Secondary themes or narratives that run parallel to the main theme,\\noften involving supporting associations.\\n●\\nMetaphor: A ﬁgure of perception or an aesthetic form that establishes a\\ncomparison between two unrelated objects or ideas, adding depth and layers\\nof meaning.\\n●\\nAnalogy: A comparison that explains an unfamiliar or abstract concept by\\ndrawing parallels to a more familiar or concrete concept.\\n●\\nMeta-Language: A contrast between what is shown and what actually comes\\nto the mind, often used to create absurd elements or evoke an act of\\nreﬂection, like meme references in your artwork.\\n●\\nFlashback: An aesthetic device that interrupts the chronological order of\\nevents to provide background information or reveal past experiences.\\n●\\nResolution: The ﬁnal artistic form or solution to the conﬂicts presented in the\\nartwork, tying up loose ends and providing conceptual closure.\\n●\\nClimax: The turning point or highest point of tension in the artwork, where the\\nconﬂicts reach their peak and the outcome becomes imminent.\\n●\\nDialogue: Interactions between elements like motifs or metaphors within the\\nartwork, revealing their appearance, emotional response, and advancing the\\nnarrative of the artwork.\\n●\\nNarrative Structure: The overall organization and framework of the artwork,\\nincluding the arrangement of events and the pacing of the narrative elements.\\nSales and Pricing Information\\n●\\nAsking Price: `<Artwork\\'s asking price, in your preferred currency>`\\n●\\nPayment Options: `<List accepted payment methods such as credit card,\\nPayPal, cryptocurrency, etc.>`\\n●\\nDiscounts or Offers: `<Include details about any discounts, bundle offers, or\\nother promotions related to your artwork>`\\n●\\nShipping Information: `<Specify shipping options, insurance, cost, estimated\\ndelivery time, handling fees, etc.>`\\n●\\nReturns Policy: `<Provide details about your return policy, including\\ntimeframes, conditions for accepting returns, and refund process>`\\n●\\nLimited Editions: `<If your artwork is part of a limited edition series, specify\\nthe edition size, numbering, and any special features, such as signed\\ncertiﬁcatesof authenticity>\\\\`\\n●\\nPurchase Information: \\\\`<Provide details on how interested parties can\\npurchase or inquire about your artwork, including contact information, pricing,\\nand any available editions>```\\n●\\nAuthentication: \\\\`<Describe the process for ensuring the authenticity of your\\nartwork, such as hand-signed certiﬁcates, holographic seals, or online\\nveriﬁcation systems>\\\\`\\n●\\nAvailability: \\\\`<State the current availability of your artwork, whether it\\'s in\\nstock, on backorder, or sold out>\\\\`\\n●\\nLicensing Options: \\\\`<If you offer licensing options for your artwork, include a\\nbrief description and pricing information for available licenses>\\\\`\\n●\\nInquiries: \\\\`<Provide contact information for sales inquiries, such as an email\\naddress, contact form, or phone number>\\\\`\\nExhibitions and Projects\\n●\\nExhibition Dates: \\\\`<List details of any upcoming exhibitions or events\\nfeaturing your artwork>\\\\`\\nWork Presentation\\n●\\nEditions and Variants: `<Details of any different editions, variants, or versions\\nof your artwork and how they differ from the original>`\\n●\\nInstallation Instructions: `<Provide instructions on how to properly install,\\ndisplay, or interact with your artwork, if applicable>`\\n●\\nMaintenance and Preservation: `<Recommendations for maintaining and\\npreserving your artwork, including proper handling, storage, and cleaning\\nprocedures>`\\n●\\nEdition Details: `<For editioned works, provide information on edition size,\\nnumbering, availability, and pricing>`\\n●\\nAvailability and Pricing: `<Provide information on the availability and pricing of\\nyour artwork, including any information on discounts, promotions, or special\\noffers>`\\nAdditional Supporting Materials\\n○\\nImages: `<Provide high-resolution images or speciﬁcations for the\\nartwork, including detailed shots or images showing the artwork in\\ndifferent contexts>`\\n○\\nVideo: `<If applicable, provide a link to a video showcasing the artwork\\nand/or the process of creating it>`\\n○\\nAudio: `<If applicable, provide a link to an audio component that is part\\nof or complements the artwork>`\\n○\\nAdditional Materials: `<Provide links to supporting materials, such as\\nvideo or audio documentation, digital versions of your artwork, critical\\nreviews, or related research>`\\n○\\nAccessibility Info: `<Provide any relevant accessibility information, such\\nas image alt text, video/audio transcripts, or ways viewers with\\ndisabilities can engage with your artwork>`\\nCopyright and Licensing Information\\n●\\nCopyright: `<Indicate the copyright holder(s) and any relevant information\\nregarding copyright ownership, e.g., \"© 2021 Your Name. All rights reserved.\">`\\n●\\nLicensing: `<Specify any terms and conditions regarding how the artwork can\\nbe used, reproduced, or distributed, e.g., \"This artwork is licensed under the\\nCreative Commons Attribution-NonCommercial-NoDerivatives 4.0\\nInternational License. Link:\\n[http://creativecommons.org/licenses/by-nc-nd/4.0/]\")>`\\n●\\nUsage Permissions: `<Clearly state permissions and restrictions on how the\\nartwork may be used or shared, including permission for educational,\\ncommercial, or public display purposes>`\\n●\\nContact Information: `<Provide contact information for- Inquiries: \\\\`<Provide\\nan email address or other method for viewers, collectors, or institutions to\\ncontact you with any questions, requests, or inquiries about the artwork, e.g.,\\n\"Please direct all inquiries to\\n[email@example.com](mailto:email@example.com)\">\\\\`\\n●\\nSocial Media: \\\\`<List any social media platforms where your artwork is\\nshowcased or where followers can stay updated on your work, e.g., \"Follow\\nme on Instagram:\\n[https://www.instagram.com/yourusername](https://www.instagram.com/you\\nrusername)\">\\\\`\\n●\\nImage Credits: `<Provide appropriate image credits for photos, videos, or other\\nmedia used within your artwork or metadata, including the name of the\\nphotographer or creator and a link to their webpage if applicable>`\\nArt Criteria and Parameters\\nFor each criterion, provide a brief statement that addresses how your artwork meets\\nthe requirements:\\n●\\nAesthetic Quality: \\\\`<Statement about the aesthetic quality of your artwork>\\\\`\\n●\\nConceptual Relevance: \\\\`<Statement about the conceptual relevance of your\\nartwork>\\\\`\\n●\\nTechnological Innovation: \\\\`<Statement about the technological innovation of\\nyour artwork>\\\\`\\n●\\nInterdisciplinary Approach: \\\\`<Statement about the interdisciplinary approach\\nof your artwork>\\\\`\\n●\\nAudience Interaction: \\\\`<Statement about the audience interaction with your\\nartwork>\\\\`\\n●\\nInteractive Elements: `<If applicable, describe any interactive features or\\ncomponents in the artwork>`\\n●\\naesthetic quality: \\\\`<Describe the aesthetic qualities of your artwork, such as\\nharmony, balance, contrast, or rhythm, and explain how these features\\ncontribute to the overall visual appeal and impact>\\\\`\\n●\\nconceptual relevance: `<Describe the conceptual relevance of your artwork,\\nsuch as how it addresses current issues, challenges conventions, or connects\\nto broader artistic or cultural themes>`\\n●\\ntechnological innovation: \\\\`<Explain the types of technology or technological\\ninnovation used in the creation, presentation, or promotion of your artwork,\\nand how this enhances its creative or conceptual value>\\\\`\\n●\\ninterdisciplinary approach:\\\\`<Describe the interdisciplinary approach taken in\\nyour artwork, such as how it combines various disciplines, mediums, or\\nperspectives to create a unique and innovative artistic experience>\\\\`\\n●\\naudience interaction: `\\\\`<Explain how your artwork encourages audience\\ninteraction, whether through physical engagement, emotional connection, or\\nintellectual stimulation, and discuss the intended impact or response you\\nhope to elicit from viewers>\\\\``\\n●\\nInteractive Elements:`<List any interactive elements of your artwork, such as\\ntouchscreens, sensors, or augmented reality features, and describe their\\npurpose or function in the context of the work>\\\\`\\n●\\nAccessibility: \\\\`<Explain how you have considered accessibility in your\\nartwork, such as accommodating viewers with disabilities or ensuring the\\nwork is inclusive and engaging for a diverse audience>\\\\`\\n●\\nCritical Reception: \\\\`<Describe how you feel about any critical response your\\nartwork has received, including reviews, press coverage, commentary on\\nsocial media, or notable discussions among art professionals>\\\\`\\n●\\nContextual Analysis: \\\\`<Provide an analysis of your artwork within the context\\nof your artistic practice, your inﬂuences, and the contemporary art world>\\\\`\\n●\\nChallenges Overcome: \\\\`<Discuss any challenges or obstacles you faced\\nduring the creation of your artwork, and how you addressed or resolved\\nthem>\\\\`\\n●\\nPersonal Signiﬁcance: \\\\`<Share any personal or emotional signiﬁcance your\\nartwork holds for you,such as its connection to your life experiences, personal\\ngrowth, or self-reﬂection, and how this has informed the work\\'s meaning and\\nimpact>```\\n●\\nFuture Plans: \\\\`<Outline any plans or ambitions you have for the future of this\\nartwork, including potential exhibitions, collaborations, or expansions on its\\nthemes or concepts>```\\nArtistic Questions\\n●\\nThemes and Concepts: `<What themes or concepts drive your artwork\\'s\\ncreation?>`\\n●\\nArt Historical Context: `<Does your work engage with or contribute to the\\ngreater art historical context?>`\\n●\\nCultural, Historical, and Societal Inﬂuences:`<Are there speciﬁc cultural,\\nhistorical, or societal inﬂuences that have shaped your art practice?>`\\n●\\nTheory and Philosophy: `<What theories or philosophies inform your artistic\\npractice and artwork?>`\\n●\\nArtistic Practice Evolution: `<How does your artistic practice evolve or change\\nover time? Is there a speciﬁc trajectory that you can trace (e.g., collaboration\\nwith other artists, using new technologies)?>`\\n●\\nTechniques, Materials, and Processes: `<What speciﬁc techniques, materials,\\nor processes have you employed in your work, and how do these choices\\ncontribute to its overall meaning or effect?>`\\n●\\nAudience Interaction: `<In what ways does your artwork invite audience\\ninteraction, and how critical is this interaction to the overall experience or\\ninterpretation of the piece?>`\\n●\\nInﬂuences and Inspirations: `<Can you provide any examples of other artists\\nor artworks that have inﬂuenced your practice or approach to your work?>`\\n●\\nCurrent Art Landscape: `<How do you see your artwork ﬁtting within the\\ncurrent art landscape or engaging in larger art-related conversations?>`\\n●\\nChallenges and Obstacles: `<Are there any challenges or obstacles you faced\\nwhile creating this artwork, and how did these experiences shape the ﬁnal\\npiece?>`\\n●\\nMessage and Intention: Is there a message or intention behind your artwork\\nthat you hope to communicate to your audience, and how do you hope they\\nengage with it?\\n●\\nInterdisciplinary Approaches: Does your artwork incorporate any\\ninterdisciplinary approaches, such as collaborations with professionals from\\ndifferent ﬁelds, research in various disciplines, or the application of knowledge\\nfrom multiple sources?\\n●\\nFuture Projects and Collaborations: Are there any upcoming projects,\\nexhibitions, or collaborations that you are currently working on or plan to work\\non in the future? If so, how do these endeavors connect to your existing body\\nof work?\\n●\\nArtwork Understanding: How has your understanding of your artwork shifted\\nor evolved throughout the creation process, and what insights have you\\ngained as a result?\\n●\\nProvoking Conversation: In what ways do you hope your artwork provokes\\nconversation, sparks debate, or inspires thought among your audience?\\n●\\nArtistic Practice Future: How do you envision the future of your artistic\\npractice, and what areas or themes do you hope to explore moving forward?\\n●\\nCommunity and Network Support: Are there any communities, organizations,\\nor networks that have supported you or your work, and how have these\\nconnections impacted your artistic practice and experiences?\\nUser Proﬁle\\nType: <Which 3 User Types do you want to reach with your art?\\nIdentify the top 3 user types of your art that your work is\\nmost likely to attract, such as Museum Director, Urban\\nPlanner, Galerist, Art Therapist, Art Collector, Journalist,\\nGallery, A Specific Collection, Museum, Art Critic, Art\\nResearcher, etc. This helps in targeting your marketing\\nefforts and understanding your audience\\'s preferences and\\nneeds.>\\n',\n",
       " '']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "import pickle\n",
    "from PyPDF2 import PdfReader\n",
    "from streamlit_extras.add_vertical_space import add_vertical_space\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for page in data_Call:\n",
    "    text += page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MAKE-A-VIDEO:\\nTEXT-TO-VIDEO\\nGENERATION\\nWITHOUT TEXT-VIDEO DATA\\nUriel Singer +\\nAdam Polyak +\\nThomas Hayes +\\nXi Yin +\\nJie An\\nSongyang Zhang\\nQiyuan Hu\\nHarry Yang\\nOron Ashual\\nOran Gafni\\nDevi Parikh +\\nSonal Gupta +\\nYaniv Taigman +\\nMeta AI\\nABSTRACT\\nWe propose Make-A-Video – an approach for directly translating the tremendous\\nrecent progress in Text-to-Image (T2I) generation to Text-to-Video (T2V). Our\\nintuition is simple: learn what the world looks like and how it is described from\\npaired text-image data, and learn how the world moves from unsupervised video\\nfootage. Make-A-Video has three advantages: (1) it accelerates training of the\\nT2V model (it does not need to learn visual and multimodal representations from\\nscratch), (2) it does not require paired text-video data, and (3) the generated\\nvideos inherit the vastness (diversity in aesthetic, fantastical depictions, etc.)\\nof today’s image generation models. We design a simple yet effective way to\\nbuild on T2I models with novel and effective spatial-temporal modules. First, we\\ndecompose the full temporal U-Net and attention tensors and approximate them\\nin space and time. Second, we design a spatial temporal pipeline to generate\\nhigh resolution and frame rate videos with a video decoder, interpolation model\\nand two super resolution models that can enable various applications besides\\nT2V. In all aspects, spatial and temporal resolution, faithfulness to text, and\\nquality, Make-A-Video sets the new state-of-the-art in text-to-video generation,\\nas determined by both qualitative and quantitative measures.\\n1\\nINTRODUCTION\\nThe Internet has fueled collecting billions of (alt-text, image) pairs from HTML pages (Schuhmann\\net al., 2022), enabling the recent breakthroughs in Text-to-Image (T2I) modeling. However, repli-\\ncating this success for videos is limited since a similarly sized (text, video) dataset cannot be easily\\ncollected. It would be wasteful to train Text-to-Video (T2V) models from scratch when there already\\nexist models that can generate images. Moreover, unsupervised learning enables networks to learn\\nfrom orders of magnitude more data. This large quantity of data is important to learn representa-\\ntions of more subtle, less common concepts in the world. Unsupervised learning has long had great\\nsuccess in advancing the ﬁeld of natural language processing (NLP) (Liu et al., 2019a; Brown et al.,\\n2020). Models pre-trained this way yield considerably higher performance than when solely trained\\nin a supervised manner.\\nInspired by these motivations, we propose Make-A-Video. Make-A-Video leverages T2I models\\nto learn the correspondence between text and the visual world, and uses unsupervised learning on\\nunlabeled (unpaired) video data, to learn realistic motion. Together, Make-A-Video generates videos\\nfrom text without leveraging paired text-video data.\\nClearly, text describing images does not capture the entirety of phenomena observed in videos. That\\nsaid, one can often infer actions and events from static images (e.g. a woman drinking coffee, or an\\n+ Core Contributors. Corresponding author: urielsinger@meta.com. Jie and Songyang are from\\nUniversity of Rochester (work done during internship at Meta).\\n1\\narXiv:2209.14792v1  [cs.CV]  29 Sep 2022\\n(a) A dog wearing a superhero outfit with red cape flying through the sky. \\n(b) There is a table by a window with sunlight streaming through illuminating a pile of books. \\n(c) Robot dancing in times square.\\n(d) Unicorns running along a beach, highly detailed.\\nFigure 1: T2V generation examples. Our model can generate high-quality videos with coherent\\nmotion for a diverse set of visual concepts. In example (a), there are large and realistic motion for\\nthe dog. In example (b), the books are almost static but the scene changes with the camera motion.\\nVideo samples are available at make-a-video.github.io\\nelephant kicking a football) as done in image-based action recognition systems (Girish et al., 2020).\\nMoreover, even without text descriptions, unsupervised videos are sufﬁcient to learn how different\\nentities in the world move and interact (e.g. the motion of waves at the beach, or of an elephant’s\\ntrunk). As a result, a model that has only seen text describing images is surprisingly effective at\\ngenerating short videos, as demonstrated by our temporal diffusion-based method. Make-A-Video\\nsets the new state-of-the-art in T2V generation.\\nUsing function-preserving transformations, we extend the spatial layers at the model initialization\\nstage, to include temporal information. The extended spatial-temporal network includes new at-\\ntention modules that learn temporal world dynamics from a collection of videos. This procedure\\nsigniﬁcantly accelerates the T2V training process by instantaneously transferring the knowledge\\nfrom a previously trained T2I network to a new T2V one. To enhance the visual quality, we train\\nspatial super-resolution models as well as frame interpolation models. This increases the resolution\\nof the generated videos, as well as enables a higher (controllable) frame rate.\\nOur main contributions are:\\n• We present Make-A-Video – an effective method that extends a diffusion-based T2I model\\nto T2V through a spatiotemporally factorized diffusion model.\\n• We leverage joint text-image priors to bypass the need for paired text-video data, which in\\nturn allows us to potentially scale to larger quantities of video data.\\n• We present super-resolution strategies in space and time that, for the ﬁrst time, generate\\nhigh-deﬁnition, high frame-rate videos given a user-provided textual input.\\n• We evaluate Make-A-Video against existing T2V systems and present: (a) State-of-the-art\\nresults in quantitative as well as qualitative measures, and (b) A more thorough evaluation\\nthan existing literature in T2V. We also collect a test set of 300 prompts for zero-shot T2V\\nhuman evaluation which we plan to release.\\n2\\n2\\nPREVIOUS WORK\\nText-to-Image Generation.\\n(Reed et al., 2016) is among the ﬁrst methods to extend uncondi-\\ntional Generative Adversairal Network (GAN) (Goodfellow et al., 2014) to T2I generation. Later\\nGAN variants have focused on progressive generation (Zhang et al., 2017; Hong et al., 2018), or\\nbetter text-image alignment (Xu et al., 2018; Zhang et al., 2021). The pioneering work of DALL-\\nE (Ramesh et al., 2021) considers T2I generation as a sequence-to-sequence translation problem us-\\ning a discrete variational auto-encoder (VQVAE) and Transformer (Vaswani et al., 2017). Additional\\nvariants (Ding et al., 2022) have been proposed since then. For example, Make-A-Scene (Gafni\\net al., 2022) explores controllable T2I generation using semantic maps. Parti (Yu et al., 2022a)\\naims for more diverse content generation through an encoder-decoder architecture and an improved\\nimage tokenizer (Yu et al., 2021). On the other hand, Denoising Diffusion Probabilistic Models\\n(DDPMs) (Ho et al., 2020) are successfully leveraged for T2I generation. GLIDE (Nichol et al.,\\n2021) trained a T2I and an upsampling diffusion model for cascade generation. GLIDE’s proposed\\nclassiﬁer-free guidance has been widely adopted in T2I generation to improve image quality and\\ntext faithfulness. DALLE-2 (Ramesh et al., 2022) leverages the CLIP (Radford et al., 2021) latent\\nspace and a prior model. VQ-diffusion (Gu et al., 2022) and stable diffusion (Rombach et al., 2022)\\nperforms T2I generation in the latent space instead of pixel space to improve efﬁciency.\\nText-to-Video Generation. While there is remarkable progress in T2I generation, the progress of\\nT2V generation lags behind largely due to two main reasons: the lack of large-scale datasets with\\nhigh-quality text-video pairs, and the complexity of modeling higher-dimensional video data. Early\\nworks (Mittal et al., 2017; Pan et al., 2017; Marwah et al., 2017; Li et al., 2018; Gupta et al., 2018;\\nLiu et al., 2019b) are mainly focused on video generation in simple domains, such as moving digits\\nor speciﬁc human actions. To our knowledge, Sync-DRAW (Mittal et al., 2017) is the ﬁrst T2V\\ngeneration approach that leverages a VAE with recurrent attention. (Pan et al., 2017) and (Li et al.,\\n2018) extend GANs from image generation to T2V generation.\\nMore recently, GODIVA (Wu et al., 2021a) is the ﬁrst to use 2D VQVAE and sparse attention for\\nT2V generation supporting more realistic scenes. N ¨UWA (Wu et al., 2021b) extends GODIVA, and\\npresents a uniﬁed representation for various generation tasks in a multitask learning scheme. To\\nfurther improve the performance of T2V generation, CogVideo (Hong et al., 2022) is built on top of\\na frozen CogView-2 (Ding et al., 2022) T2I model by adding additional temporal attention modules.\\nVideo Diffusion Models (VDM) (Ho et al., 2022) uses a space-time factorized U-Net with joint\\nimage and video data training. While both CogVideo and VDM collected 10M private text-video\\npairs for training, our work uses solely open-source datasets, making it easier to reproduce.\\nLeveraging Image Priors for Video Generation. Due to the complexity of modeling videos and the\\nchallenges in high-quality video data collection, it is natural to consider leveraging image priors for\\nvideos to simplifying the learning process. After all, an image is a video with a single frame (Bain\\net al., 2021). In unconditional video generation, MoCoGAN-HD (Tian et al., 2021) formulates\\nvideo generation as the task of ﬁnding a trajectory in the latent space of a pre-trained and ﬁxed image\\ngeneration model. In T2V generation, N ¨UWA (Wu et al., 2021b) combines image and video datasets\\nin a multitask pre-training stage to improve model generalization for ﬁne-tuning. CogVideo (Hong\\net al., 2022) uses a pre-trained and ﬁxed T2I model for T2V generation with only a small number\\nof trainable parameters to reduce memory usage during training. But the ﬁxed autoencoder and T2I\\nmodels can be restrictive for T2V generation. The architecture of VDM (Ho et al., 2022) can enable\\njoint image and video generation. However, they sample random independent images from random\\nvideos as their source of images, and do not leverage the massive text-image datasets.\\nMake-A-Video differs from previous works in several aspects. First, our architecture breaks the\\ndependency on text-video pairs for T2V generation. This is a signiﬁcant advantage compared to\\nprior work, that has to be restricted to narrow domains (Mittal et al., 2017; Gupta et al., 2018; Ge\\net al., 2022; Hayes et al., 2022), or require large-scale paired text-video data (Hong et al., 2022;\\nHo et al., 2022). Second, we ﬁne-tune the T2I model for video generation, gaining the advantage\\nof adapting the model weights effectively, compared to freezing the weights as in CogVideo (Hong\\net al., 2022). Third, motivated from prior work on efﬁcient architectures for video and 3D vision\\ntasks (Ye et al., 2019; Qiu et al., 2017; Xie et al., 2018), our use of pseudo-3D convolution (Qiu\\net al., 2017) and temporal attention layers not only better leverage a T2I architecture, it also allows\\nfor better temporal information fusion compared to VDM (Ho et al., 2022).\\n3\\nFigure 2: Make-A-Video high-level architecture. Given input text x translated by the prior P into\\nan image embedding, and a desired frame rate fps, the decoder Dt generates 16 64 × 64 frames,\\nwhich are then interpolated to a higher frame rate by ↑F , and increased in resolution to 256 × 256\\nby SRt\\nl and 768 × 768 by SRh, resulting in a high-spatiotemporal-resolution generated video ˆy.\\n3\\nMETHOD\\nMake-A-Video consists of three main components: (i) A base T2I model trained on text-image pairs\\n(Sec. 3.1), (ii) spatiotemporal convolution and attention layers that extend the networks’ building\\nblocks to the temporal dimension (Sec. 3.2), and (iii) spatiotemporal networks that consist of both\\nspatiotemporal layers, as well as another crucial element needed for T2V generation - a frame inter-\\npolation network for high frame rate generation (Sec. 3.3).\\nMake-A-Video’s ﬁnal T2V inference scheme (depicted in Fig. 2) can be formulated as:\\nˆyt = SRh ◦ SRt\\nl ◦ ↑F ◦ Dt ◦ P ◦(ˆx, Cx(x)),\\n(1)\\nwhere ˆyt is the generated video, SRh, SRl are the spatial and spatiotemporal super-resolution net-\\nworks (Sec. 3.2), ↑F is a frame interpolation network (Sec. 3.3), Dt is the spatiotemporal decoder\\n(Sec. 3.2), P is the prior (Sec. 3.1), ˆx is the BPE-encoded text, Cx is the CLIP text encoder (Rad-\\nford et al., 2021), and x is the input text. The three main components are described in detail in the\\nfollowing sections.\\n3.1\\nTEXT-TO-IMAGE MODEL\\nPrior to the addition of the temporal components, we train the backbone of our method: a T2I model\\ntrained on text-image pairs, sharing the core components with the work of (Ramesh et al., 2022).\\nWe use the following networks to produce high-resolution images from text: (i) A prior network P,\\nthat during inference generates image embeddings ye given text embeddings xe and BPE encoded\\ntext tokens ˆx, (ii) a decoder network D that generates a low-resolution 64 × 64 RGB image ˆyl,\\nconditioned on the image embeddings ye, and (iii) two super-resolution networks SRl,SRh that\\nincrease the generated image ˆyl resolution to 256 × 256 and 768 × 768 pixels respectively, resulting\\nin the ﬁnal1 generated image ˆy.\\n3.2\\nSPATIOTEMPORAL LAYERS\\nIn order to expand the two-dimensional (2D) conditional network into the temporal dimension, we\\nmodify the two key building blocks that now require not just spatial but also temporal dimensions in\\norder to generate videos: (i) Convolutional layers (Sec. 3.2.1), and (ii) attention layers (Sec. 3.2.2),\\ndiscussed in the following two subsections. Other layers, such as fully-connected layers, do not\\nrequire speciﬁc handling when adding an additional dimension, as they are agnostic to structured\\nspatial and temporal information. Temporal modiﬁcations are made in most U-Net-based diffusion\\nnetworks: the spatiotemporal decoder Dt now generating 16 RGB frames, each of size 64 × 64, the\\n1We then downsample to 512 using bicubic interpolation for a cleaner aesthetic. Maintaining a clean aes-\\nthetic for high deﬁnition videos is part of future work.\\n4\\nFigure 3: The architecture and initialization scheme of the Pseudo-3D convolutional and at-\\ntention layers, enabling the seamless transition of a pre-trained Text-to-Image model to the\\ntemporal dimension. (left) Each spatial 2D conv layer is followed by a temporal 1D conv layer.\\nThe temporal conv layer is initialized with an identity function. (right) Temporal attention layers are\\napplied following the spatial attention layers by initializing the temporal projection to zero, resulting\\nin an identity function of the temporal attention blocks.\\nnewly added frame interpolation network ↑F , increasing the effective frame rate by interpolating\\nbetween the 16 generated frames (as depicted in Fig. 2), and the super-resolution networks SRt\\nl.\\nNote that super resolution involves hallucinating information. In order to not have ﬂickering ar-\\ntifacts, the hallucination must be consistent across frames. As a result, our SRt\\nl module operates\\nacross spatial and temporal dimensions. In qualitative inspection we found this to signiﬁcantly out-\\nperform per-frame super resolution. It is challenging to extend SRh to the temporal dimension due\\nto memory and compute constraints, as well as a scarcity of high resolution video data. So SRh\\noperates only along the spatial dimensions. But to encourage consistent detail hallucination across\\nframes, we use the same noise initialization for each frame.\\n3.2.1\\nPSEUDO-3D CONVOLUTIONAL LAYERS\\nMotivated by separable convolutions (Chollet, 2017), we stack a 1D convolution following each\\n2D convolutional (conv) layer, as shown in Fig. 3. This facilitates information sharing between\\nthe spatial and temporal axes, without succumbing to the heavy computational load of 3D conv\\nlayers. In addition, it creates a concrete partition between the pre-trained 2D conv layers and the\\nnewly initialized 1D conv layers, allowing us to train the temporal convolutions from scratch, while\\nretaining the previously learned spatial knowledge in the spatial convolutions’ weights.\\nGiven an input tensor h ∈ RB×C×F ×H×W , where B, C, F, H, W are the batch, channels, frames,\\nheight, and width dimensions respectively, the Pseudo-3D convolutional layer is deﬁned as:\\nConvP 3D(h) := Conv1D(Conv2D(h) ◦ T) ◦ T,\\n(2)\\nwhere the transpose operator ◦T swaps between the spatial and temporal dimensions. For smooth\\ninitialization, while the Conv2D layer is initialized from the pre-trained T2I model, the Conv1D\\nlayer is initialized as the identity function, enabling a seamless transition from training spatial-only\\nlayers, to spatiotemporal layers. Note that at initialization, the network will generate K different\\nimages (due to random noise), each faithful to the input text but lacking temporal coherence.\\n3.2.2\\nPSEUDO-3D ATTENTION LAYERS\\nA crucial component of T2I networks is the attention layer, where in addition to self-attending to ex-\\ntracted features, text information is injected to several network hierarchies, alongside other relevant\\ninformation, such as the diffusion time-step. While using 3D convolutional layers is computationally\\nheavy, adding the temporal dimension to attention layers is outright infeasible in terms of memory\\nconsumption. Inspired by the work of (Ho et al., 2022), we extend our dimension decomposition\\nstrategy to attention layers as well. Following each (pre-trained) spatial attention layer, we stack a\\ntemporal attention layer, which as with the convolutional layers, approximates a full spatiotemporal\\nattention layer. Speciﬁcally, given an input tensor h, we deﬁne flatten as a matrix operator that\\n5\\nﬂattens the spatial dimension into h′ ∈ RB×C×F ×HW . unflatten is deﬁned as the inverse matrix\\noperator. The Pseudo-3D attention layer therefore is therefore deﬁned as:\\nATTNP 3D(h) = unflatten(ATTN1D(ATTN2D(flatten(h)) ◦ T) ◦ T).\\n(3)\\nSimilarly to ConvP 3D, to allow for smooth spatiotemporal initialization, the ATTN2D layer is ini-\\ntialized from the pre-trained T2I model and the ATTN1D layer is initialized as the identity function.\\nFactorized space-time attention layers have also been used in VDM (Ho et al., 2022) and\\nCogVideo (Hong et al., 2022). CogVideo has added temporal layers to each (frozen) spatial layers\\nwhereas we train them jointly. In order to force their network to train for images and videos inter-\\nchangeably, VDM has extended their 2D U-Net to 3D through unﬂattened 1x3x3 convolution ﬁlters,\\nsuch that the subsequent spatial attention remains 2D, and added 1D temporal attention through rel-\\native position embeddings. In contrast, we apply an additional 3x1x1 convolution projection (after\\neach 1x3x3) such that the temporal information will also be passed through each convolution layer.\\nFrame rate conditioning. In addition to the T2I conditionings, similar to CogVideo (Hong et al.,\\n2022), we add an additional conditioning parameter fps, representing the number of frames-per-\\nsecond in a generated video. Conditioning on a varying number of frames-per-second, enables an\\nadditional augmentation method to tackle the limited volume of available videos at training time,\\nand provides additional control on the generated video at inference time.\\n3.3\\nFRAME INTERPOLATION NETWORK\\nIn addition to the spatiotemporal modiﬁcations discussed in Sec. 3.2, we train a new masked frame\\ninterpolation and extrapolation network ↑F , capable of increasing the number of frames of the gen-\\nerated video either by frame interpolation for a smoother generated video, or by pre/post frame\\nextrapolation for extending the video length. In order to increase the frame rate within memory and\\ncompute constraints, we ﬁne-tune a spatiotemporal decoder Dt on the task of masked frame inter-\\npolation, by zero-padding the masked input frames, enabling video upsampling. When ﬁne-tuning\\non masked frame interpolation, we add an additional 4 channels to the input of the U-Net: 3 chan-\\nnels for the RGB masked video input and an additional binary channel indicating which frames are\\nmasked. We ﬁne-tune with variable frame-skips and fps conditioning to enable multiple temporal\\nupsample rates at inference time. We denote ↑F as the operator that expands the given video tensor\\nthrough masked frame interpolation. For all of our experiments we applied ↑F with frame skip 5 to\\nupsample a 16 frame video to 76 frames ((16-1)×5+1). Note that we can use the same architecture\\nfor video extrapolation or image animation by masking frames at the beginning or end of a video.\\n3.4\\nTRAINING\\nThe different components of Make-A-Video described above are trained independently. The only\\ncomponent that receives text as input is the prior P. We train it on paired text-image data and do not\\nﬁne-tune it on videos. The decoder, prior, and two super-resolution components are ﬁrst trained on\\nimages alone (no aligned text). Recall that the decoder receives CLIP image embedding as input,\\nand the super-resolution components receive downsampled images as input during training. After\\ntraining on images, we add and initialize the new temporal layers and ﬁne-tune them over unlabeled\\nvideo data. 16 frames are sampled from the original video with random fps ranging from 1 to\\n30. We use the beta function for sampling and while training the decoder, start from higher FPS\\nranges (less motion) and then transition to lower FPS ranges (more motion). The masked-frame-\\ninterpolation component is ﬁne-tuned from the temporal decoder.\\n4\\nEXPERIMENTS\\n4.1\\nDATASETS AND SETTINGS\\nDatasets. To train the image models, we use a 2.3B subset of the dataset from (Schuhmann et al.)\\nwhere the text is English. We ﬁlter out sample pairs with NSFW images 2, toxic words in the text,\\nor images with a watermark probability larger than 0.5. We use WebVid-10M (Bain et al., 2021)\\n2We used this model: https://github.com/GantMan/nsfw model\\n6\\nTable 1: T2V generation evaluation on MSR-VTT. Zero-Shot means no training is conducted on\\nMSR-VTT. Samples/Input means how many samples are generated (and then ranked) for each input.\\nMethod\\nZero-Shot\\nSamples/Input\\nFID (↓)\\nCLIPSIM (↑)\\nGODIVA (Wu et al., 2021a)\\nNo\\n30\\n−\\n0.2402\\nN ¨UWA (Wu et al., 2021b)\\nNo\\n−\\n47.68\\n0.2439\\nCogVideo (Hong et al., 2022) (Chinese)\\nYes\\n1\\n24.78\\n0.2614\\nCogVideo (Hong et al., 2022) (English)\\nYes\\n1\\n23.59\\n0.2631\\nMake-A-Video (ours)\\nYes\\n1\\n13.17\\n0.3049\\nand a 10M subset from HD-VILA-100M (Xue et al., 2022) 3 to train our video generation models.\\nNote that only the videos (no aligned text) are used. The decoder Dt and the interpolation model\\nis trained on WebVid-10M. SRt\\nl is trained on both WebVid-10M and HD-VILA-10M. While prior\\nwork (Hong et al., 2022; Ho et al., 2022) have collected private text-video pairs for T2V generation,\\nwe use only public datasets (and no paired text for videos). We conduct automatic evaluation on\\nUCF-101 (Soomro et al., 2012) and MSR-VTT (Xu et al., 2016) in a zero-shot setting.\\nAutomatic Metrics. For UCF-101, we write one template sentence for each class (without generat-\\ning any video) and ﬁx it for evaluation. We report Frechet Video Distance (FVD) and Inception Score\\n(IS) on 10K samples following (Ho et al., 2022). We generate samples that follow the same class\\ndistribution as the training set. For MSR-VTT, we report Frechet Inception Distance (FID) (Parmar\\net al., 2022) and CLIPSIM (average CLIP similarity between video frames and text) (Wu et al.,\\n2021a), where all 59, 794 captions from the test set are used, following (Wu et al., 2021b).\\nHuman Evaluation Set and Metrics. We collect an evaluation set from Amazon Mechanical Turk\\n(AMT) that consists of 300 prompts. We asked annotators what they would be interested in gener-\\nating if there were a T2V system. We ﬁltered out prompts that were incomplete (e.g., “jump into\\nwater”), too abstract (e.g., “climate change”), or offensive. We then identiﬁed 5 categories (animals,\\nfantasy, people, nature and scenes, food and beverage) and selected prompts for these categories.\\nThese prompts were selected without generating any videos for them, and were kept ﬁxed. In addi-\\ntion, we also used the DrawBench prompts from Imagen (Saharia et al., 2022) for human evaluation.\\nWe evaluate video quality and text-video faithfulness. For video quality, we show two videos in ran-\\ndom order and ask annotators which one is of higher quality. For faithfulness, we additionally show\\nthe text and ask annotators which video has a better correspondence with the text (we suggest them\\nto ignore quality issues). In addition, we also conducted human evaluation to compare video motion\\nrealism of our interpolation model and FILM (Reda et al., 2022). For each comparison, we use the\\nmajority vote from 5 different annotators as the ﬁnal result.\\n4.2\\nQUANTITATIVE RESULTS\\nAutomatic Evaluation on MSR-VTT. In addition to GODIVA and N ¨UWA that report on MSR-\\nVTT, we also perform inference on the ofﬁcially released CogVideo model with both Chinese and\\nEnglish inputs for comparison. For CogVideo and Make-A-Video, we only generate one sample\\nfor each prompt in a zero-shot setting. We only generate videos that are at 16 × 256 × 256 as the\\nevaluation models do not expect higher resolutions and frame rate. The results are shown in Table 1.\\nMake-A-Video’s zero-shot performance is much better than GODIVA and N ¨UWA which are trained\\non MSR-VTT. We also outperform CogVideo in both Chinese and English settings. Thus, Make-A-\\nVideo has signiﬁcantly better generalization capabilities than prior work.\\nAutomatic Evaluation on UCF-101. UCF-101 is a popular benchmark to evaluate video generation\\nand has been recently used in T2V models. CogVideo performed ﬁnetuning of their pretrained\\nmodel for class-conditional video generation. VDM (Ho et al., 2022) performed unconditional video\\ngeneration and trained from scratch on UCF-101. We argue that both settings are not ideal and is\\nnot a direct evaluation of the T2V generation capabilities. Moreover, the FVD evaluation model\\nexpects the videos to be 0.5 second (16 frames), which is too short to be used for video generation in\\npractice. Nevertheless, in order to compare to prior work, we conducted evaluation on UCF-101 in\\n3These 100M clips are sourced from 3.1M videos. We randomly downloaded 3 clips per video to form our\\nHD-VILA-10M subset.\\n7\\nTable 2: Video generation evaluation on UCF-101 for both zero-shot and ﬁne-tuning settings.\\nMethod\\nPretrain\\nClass\\nResolution\\nIS (↑)\\nFVD (↓)\\nZero-Shot Setting\\nCogVideo (Chinese)\\nNo\\nYes\\n480 × 480\\n23.55\\n751.34\\nCogVideo (English)\\nNo\\nYes\\n480 × 480\\n25.27\\n701.59\\nMake-A-Video (ours)\\nNo\\nYes\\n256 × 256\\n33.00\\n367.23\\nFinetuning Setting\\nTGANv2(Saito et al., 2020)\\nNo\\nNo\\n128 × 128\\n26.60 ± 0.47\\n-\\nDIGAN(Yu et al., 2022b)\\nNo\\nNo\\n32.70 ± 0.35\\n577 ± 22\\nMoCoGAN-HD(Tian et al., 2021)\\nNo\\nNo\\n256 × 256\\n33.95 ± 0.25\\n700 ± 24\\nCogVideo (Hong et al., 2022)\\nYes\\nYes\\n160 × 160\\n50.46\\n626\\nVDM (Ho et al., 2022)\\nNo\\nNo\\n64 × 64\\n57.80 ± 1.3\\n-\\nTATS-base(Ge et al., 2022)\\nNo\\nYes\\n128 × 128\\n79.28 ± 0.38\\n278 ± 11\\nMake-A-Video (ours)\\nYes\\nYes\\n256 × 256\\n82.55\\n81.25\\nTable 3: Human evaluation results compared to CogVideo (Hong et al., 2022) on DrawBench and\\nour test set, and to VDM (Ho et al., 2022) on the 28 examples from their website. The numbers\\nshow the percentage of raters that prefer the results of our Make-A-Video model.\\nComparison\\nBenchmark\\nQuality\\nFaithfulness\\nMake-A-Video (ours) vs. VDM\\nVDM prompts (28)\\n84.38\\n78.13\\nMake-A-Video (ours) vs. CogVideo (Chinese)\\nDrawBench (200)\\n76.88\\n73.37\\nMake-A-Video (ours) vs. CogVideo (English)\\nDrawBench (200)\\n74.48\\n68.75\\nMake-A-Video (ours) vs. CogVideo (Chinese)\\nOur Eval. Set (300)\\n73.44\\n75.74\\nMake-A-Video (ours) vs. CogVideo (English)\\nOur Eval. Set (300)\\n77.15\\n71.19\\nboth zero-shot and ﬁnetuning settings. As shown in Table 2, Make-A-Video’s zero-shot performance\\nis already competitive than other approaches that are trained on UCF-101, and is much better than\\nCogVideo, which indicates that Make-A-Video can generalize better even to such a speciﬁc domain.\\nOur ﬁnetuning setting achieves state-of-the-art results with a signiﬁcant reduction in FVD, which\\nsuggests that Make-A-Video can generate more coherent videos than prior work.\\nHuman Evaluation. We compare to CogVideo (the only public zero-shot T2V generation model) on\\nDrawBench and our test set. We also evaluate on the 28 videos shown on the webpage of VDM (Ho\\net al., 2022) (which may be biased towards showcasing the model’s strengths). Since this is a very\\nsmall test set, we randomly generate 8 videos for each input and perform evaluation 8 times and\\nreport the average results. We generate videos at 76 × 256 × 256 resolution for human evaluation.\\nThe results are shown in Table 3. Make-A-Video achieves much better performance in both video\\nquality and text-video faithfulness in all benchmarks and comparisons. For CogVideo, the results are\\nsimilar on DrawBench and our evaluation set. For VDM, it is worth noting that we have achieved\\nsigniﬁcantly better results without any cherry-picking. We also evaluate our frame interpolation\\nnetwork in comparison to FILM (Reda et al., 2022). We ﬁrst generate low frame rate videos (1 FPS)\\nfrom text prompts in DrawBench and our evaluation set, then use each method to upsample to 4\\nFPS. Raters choose our method for more realistic motion 62% of the time on our evaluation set and\\n54% of the time on DrawBench. We observe that our method excels when there are large differences\\nbetween frames where having real-world knowledge of how objects move is crucial.\\n4.3\\nQUALITATIVE RESULTS\\nExamples of Make-A-Video’s generations are shown in Figure 1. In this section, we will show\\nT2V generation comparison to CogVideo (Hong et al., 2022) and VDM (Ho et al., 2022), and video\\ninterpolation comparison to FILM (Reda et al., 2022). In addition, our models can be used for\\na variety of other tasks such as image animation, video variation, etc. Due to space constraint,\\nwe only show a single example of each. Figure 4 (a) shows the comparison of Make-A-Video\\nto CogVideo and VDM. Make-A-Video can generate richer content with motion consistency and\\n8\\n(a) T2V Generation: comparison between VDM (top), CogVideo (mid), and Ours (bottom) for input “Busy freeway at night”. \\n(b) Image Animation: leftmost shows the input image, and we animated it to be a video.\\n(c) Image Interpolation: given two images (leftmost and rightmost), we interpolate frames. Comparing FILM (left) and Ours (right).\\n(d) Video Variation: we can generate a new video (bottom) as a variant to the original video (top).\\nFigure 4: Qualitative results for various comparisons and applications.\\ntext correspondence. Figure 4 (b) shows an example of image animation where we condition the\\nmasked frame interpolation and extrapolation network ↑F on the image and CLIP image embedding\\nto extrapolate the rest of the video. This allows a user to generate a video using their own image\\n– giving them the opportunity to personalize and directly control the generated video. Figure 4\\n(c) shows a comparison of our approach to FILM (Reda et al., 2022) on the task of interpolation\\nbetween two images. We achieve this by using the interpolation model that takes the two images as\\nthe beginning and end frames and masks 14 frames in between for generation. Our model generates\\nmore semantically meaningful interpolation while FILM seems to primarily smoothly transition\\nbetween frames without semantic real-world understanding of what is moving. Figure 4 (d) shows\\nan example for video variation. We take the average CLIP embedding of all frames from a video\\nas the condition to generate a semantically similar video. More video generation examples and\\napplications can be found here: make-a-video.github.io.\\n5\\nDISCUSSION\\nLearning from the world around us is one of the greatest strengths of human intelligence. Just as we\\nquickly learn to recognize people, places, things, and actions through observation, generative sys-\\ntems will be more creative and useful if they can mimic the way humans learn. Learning world dy-\\nnamics from orders of magnitude more videos using unsupervised learning helps researchers break\\n9\\naway from the reliance on labeled data. The presented work has shown how labeled images com-\\nbined effectively with unlabeled video footage can achieve that.\\nAs a next step we plan to address several of the technical limitations. As discussed earlier, our\\napproach can not learn associations between text and phenomenon that can only be inferred in\\nvideos. How to incorporate these (e.g., generating a video of a person waving their hand left-to-right\\nor right-to-left), along with generating longer videos, with multiple scenes and events, depicting\\nmore detailed stories, is left for future work.\\nAs with all large-scale models trained on data from the web, our models have learnt and likely\\nexaggerated social biases, including harmful ones. Our T2I generation model was trained on data\\nthat removed NSFW content and toxic words. All our data (image as well as videos) is publicly\\navailable, adding a layer of transparency to our models, and making it possible for the community\\nto reproduce our work.\\nACKNOWLEDGMENTS\\nMustafa Said Mehmetoglu, Jacob Xu, Katayoun Zand, Jia-Bin-Huang, Jiebo Luo, Shelly Sheynin,\\nAngela Fan, Kelly Freed. Thank you for your contributions!\\nREFERENCES\\nMax Bain, Arsha Nagrani, G¨ul Varol, and Andrew Zisserman. Frozen in time: A joint video and\\nimage encoder for end-to-end retrieval. In ICCV, pp. 1728–1738, 2021.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\\nAriel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz\\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\\nRadford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. CoRR,\\nabs/2005.14165, 2020. URL https://arxiv.org/abs/2005.14165.\\nFranc¸ois Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings\\nof the IEEE conference on computer vision and pattern recognition, pp. 1251–1258, 2017.\\nMing Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image\\ngeneration via hierarchical transformers. arXiv preprint arXiv:2204.14217, 2022.\\nOran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-\\nscene: Scene-based text-to-image generation with human priors, 2022. URL https://arxiv.\\norg/abs/2203.13131.\\nSongwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and\\nDevi Parikh. Long video generation with time-agnostic vqgan and time-sensitive transformer.\\nECCV, 2022.\\nDeeptha Girish, Vineeta Singh, and Anca Ralescu. Understanding action recognition in still images.\\npp. 1523–1529, 06 2020. doi: 10.1109/CVPRW50498.2020.00193.\\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\\nAaron Courville, and Yoshua Bengio. Generative adversarial networks. NIPS, 2014.\\nShuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and\\nBaining Guo. Vector quantized diffusion model for text-to-image synthesis. In CVPR, pp. 10696–\\n10706, 2022.\\nTanmay Gupta, Dustin Schwenk, Ali Farhadi, Derek Hoiem, and Aniruddha Kembhavi. Imagine\\nthis! scripts to compositions to videos. In ECCV, pp. 598–613, 2018.\\nThomas Hayes, Songyang Zhang, Xi Yin, Guan Pang, Sasha Sheng, Harry Yang, Songwei Ge, Is-\\nabelle Hu, and Devi Parikh. Mugen: A playground for video-audio-text multimodal understanding\\nand generation. ECCV, 2022.\\n10\\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020. URL\\nhttps://arxiv.org/abs/2006.11239.\\nJonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J.\\nFleet. Video diffusion models, 2022. URL https://arxiv.org/abs/2204.03458.\\nSeunghoon Hong, Dingdong Yang, Jongwook Choi, and Honglak Lee. Inferring semantic layout for\\nhierarchical text-to-image synthesis. In CVPR, pp. 7986–7994, 2018.\\nWenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pre-\\ntraining for text-to-video generation via transformers, 2022. URL https://arxiv.org/\\nabs/2205.15868.\\nYitong Li, Martin Min, Dinghan Shen, David Carlson, and Lawrence Carin. Video generation from\\ntext. In AAAI, volume 32, 2018.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining\\napproach. CoRR, abs/1907.11692, 2019a. URL http://arxiv.org/abs/1907.11692.\\nYue Liu, Xin Wang, Yitian Yuan, and Wenwu Zhu. Cross-modal dual learning for sentence-to-\\nvideo generation. In Proceedings of the 27th ACM International Conference on Multimedia, pp.\\n1239–1247, 2019b.\\nTanya Marwah, Gaurav Mittal, and Vineeth N Balasubramanian. Attentive semantic video genera-\\ntion using captions. In ICCV, pp. 1426–1434, 2017.\\nGaurav Mittal, Tanya Marwah, and Vineeth N Balasubramanian. Sync-draw: Automatic video gen-\\neration using deep recurrent attentive architectures. In Proceedings of the 25th ACM international\\nconference on Multimedia, pp. 1096–1104, 2017.\\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with\\ntext-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\\nYingwei Pan, Zhaofan Qiu, Ting Yao, Houqiang Li, and Tao Mei. To create what you tell: Generat-\\ning videos from captions. In Proceedings of the 25th ACM international conference on Multime-\\ndia, pp. 1789–1798, 2017.\\nGaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On aliased resizing and surprising subtleties in\\ngan evaluation. In CVPR, 2022.\\nZhaofan Qiu, Ting Yao, and Tao Mei. Learning spatio-temporal representation with pseudo-3d\\nresidual networks. In ICCV, pp. 5533–5541, 2017.\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\\nmodels from natural language supervision. In ICML, pp. 8748–8763. PMLR, 2021.\\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\\nand Ilya Sutskever. Zero-shot text-to-image generation. In ICML, pp. 8821–8831. PMLR, 2021.\\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\\nconditional image generation with clip latents, 2022.\\nURL https://arxiv.org/abs/\\n2204.06125.\\nFitsum Reda, Janne Kontkanen, Eric Tabellion, Deqing Sun, Caroline Pantofaru, and Brian Curless.\\nFilm: Frame interpolation for large motion. arXiv preprint arXiv:2202.04901, 2022.\\nScott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee.\\nGenerative adversarial text to image synthesis. In ICML, pp. 1060–1069. PMLR, 2016.\\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High-\\nresolution image synthesis with latent diffusion models. In CVPR, pp. 10684–10695, 2022.\\n11\\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kam-\\nyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Sal-\\nimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image dif-\\nfusion models with deep language understanding, 2022. URL https://arxiv.org/abs/\\n2205.11487.\\nMasaki Saito, Shunta Saito, Masanori Koyama, and Sosuke Kobayashi. Train sparsely, generate\\ndensely: Memory-efﬁcient unsupervised training of high-resolution temporal gan. International\\nJournal of Computer Vision, 128(10):2586–2606, 2020.\\nChristoph Schuhmann, Romain Beaumont, Cade W Gordon, Ross Wightman, Theo Coombes,\\nAarush Katta, Clayton Mullis, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson,\\net al. Laion-5b: An open large-scale dataset for training next generation image-text models.\\nChristoph Schuhmann,\\nRichard Vencu,\\nRomain Beaumont,\\nTheo Coombes,\\nCade Gor-\\ndon,\\nAarush\\nKatta,\\nRobert\\nKaczmarczyk,\\nand\\nJenia\\nJitsev.\\nLAION-5B:\\nlaion-\\n5b:\\nA new era of open large-scale multi-modal datasets.\\nhttps://laion.ai/\\nlaion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/,\\n2022.\\nKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions\\nclasses from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.\\nYu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N Metaxas, and Sergey\\nTulyakov. A good image generator is what you need for high-resolution video synthesis. ICLR,\\n2021.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2017. URL https://arxiv.\\norg/abs/1706.03762.\\nChenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and\\nNan Duan. Godiva: Generating open-domain videos from natural descriptions. arXiv preprint\\narXiv:2104.14806, 2021a.\\nChenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. N ¨Uwa: Visual\\nsynthesis pre-training for neural visual world creation, 2021b. URL https://arxiv.org/\\nabs/2111.12417.\\nSaining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethinking spatiotem-\\nporal feature learning: Speed-accuracy trade-offs in video classiﬁcation. In ECCV, pp. 305–321,\\n2018.\\nJun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging\\nvideo and language. In CVPR, pp. 5288–5296, 2016.\\nTao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong\\nHe. Attngan: Fine-grained text to image generation with attentional generative adversarial net-\\nworks. In CVPR, pp. 1316–1324, 2018.\\nHongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and\\nBaining Guo. Advancing high-resolution video-language representation with large-scale video\\ntranscriptions. In CVPR, pp. 5036–5045, 2022.\\nRongtian Ye, Fangyu Liu, and Liqiang Zhang. 3d depthwise convolution: Reducing model parame-\\nters in 3d vision tasks. In Canadian Conference on Artiﬁcial Intelligence, pp. 186–199. Springer,\\n2019.\\nJiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong\\nXu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan.\\narXiv preprint arXiv:2110.04627, 2021.\\n12\\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\\nAlexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin\\nLi, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich\\ntext-to-image generation, 2022a. URL https://arxiv.org/abs/2206.10789.\\nSihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin.\\nGenerating videos with dynamics-aware implicit generative adversarial networks. ICLR, 2022b.\\nHan Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dim-\\nitris N Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adver-\\nsarial networks. In ICCV, pp. 5907–5915, 2017.\\nHan Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and Yinfei Yang. Cross-modal contrastive\\nlearning for text-to-image generation. In CVPR, pp. 833–842, 2021.\\n13\\nAppAgent: Multimodal Agents as Smartphone Users\\nChi Zhang∗\\nZhao Yang∗\\nJiaxuan Liu∗\\nYucheng Han\\nXin Chen\\nZebiao Huang\\nBin Fu\\nGang Yu†\\nTencent\\n{johnczhang, jayzyang, jiaxuanliu, yuchenghan, shingxchen, zebiaohuang, brianfu, skicyyu}@tencent.com\\nhttps://appagent-official.github.io/\\nClock\\nTEMU\\nObservation: The screenshot shows a photo editing app on \\na smartphone with an image displayed on the screen...\\nThought: To complete this task, I should adjust the noise \\nreduction level using the slider. \\nAction: Action: swipe(23, \"right\", \"medium\")\\nObservation: The image shows a screenshot of an \\nalarm application on a smartphone...\\nThought: To complete this task, I should disable the \\nvibration feature by tapping on the UI element. \\nAction: tap(17)\\nObservation: The screenshot displays a shopping app \\nwith a variety of headsets listed ...\\nThought: To complete this task, I should add the \\nselected headset into my shopping cart.\\nAction: tap(14)\\nImprove the quality of \\nthis image until it looks \\ngood to you\\nObservation: The screenshot shows the Telegram app, \\na conversation from \"Agent Liu\" with a message...\\nThought: To complete this task, I should input the text with \\nthis answer into the input field labeled with the tag ‘7’\\nAction: text(\"The Toronto Raptors won the 2019 NBA ... \")\\nSet an alarm at 12:30 pm \\nevery Friday and Sunday, \\nand disable the vibration\\nSearch for a gaming \\nheadset and add it to my \\nshopping cart.\\nSend an email to \\njanedoe@email.com to \\nask her about her new job\\nSearch music video song \\nWonderful Tonight and \\nleave a praising comment \\nChat with the user \\nAgent Liu and \\nanswer his question\\nObservation: The screenshot shows a YouTube search \\nresult for \"Wonderful Tonight\" ...\\nThought: To complete this task, I should insert a \\npraising comment into the text input field labeled ‘2’.\\nAction: text(“This is such a timeless piece...\")\\nObservation: The screenshot shows a smartphone\\'s \\nemail application ...\\nThought: To complete this task, I should type the \\ncontent of the email asking Jane Doe about her new job.\\nAction: text(“Dear Jane Doe,\\\\n\\\\nI am writing to ...\")\\nFigure 1: Diverse applications of our multimodal agent framework for smartphone App operation. We\\nevaluate the effectiveness of our agent model on 50 tasks across 10 different Apps, highlighting its adaptability and\\neffectiveness in a real-world context.\\nAbstract\\nRecent advancements in large language models\\n(LLMs) have led to the creation of intelligent\\nagents capable of performing complex tasks.\\nThis paper introduces a novel LLM-based mul-\\ntimodal agent framework designed to operate\\nsmartphone applications. Our framework en-\\nables the agent to operate smartphone applica-\\n*Equal contributions.\\n†Corresponding Author.\\ntions through a simplified action space, mim-\\nicking human-like interactions such as tapping\\nand swiping. This novel approach bypasses\\nthe need for system back-end access, thereby\\nbroadening its applicability across diverse apps.\\nCentral to our agent’s functionality is its in-\\nnovative learning method. The agent learns\\nto navigate and use new apps either through\\nautonomous exploration or by observing hu-\\nman demonstrations. This process generates\\n1\\narXiv:2312.13771v1  [cs.CV]  21 Dec 2023\\na knowledge base that the agent refers to for\\nexecuting complex tasks across different appli-\\ncations. To demonstrate the practicality of our\\nagent, we conducted extensive testing over 50\\ntasks in 10 different applications, including so-\\ncial media, email, maps, shopping, and sophis-\\nticated image editing tools. The results affirm\\nour agent’s proficiency in handling a diverse\\narray of high-level tasks.\\n1\\nIntroduction\\nThe emergence of large language models (LLMs),\\nsuch as ChatGPT (OpenAI, 2021) and GPT-4 (Ope-\\nnAI, 2023), marks a significant milestone in the\\nfield of artificial intelligence and natural language\\nprocessing. These advanced models represent a\\nfundamental change in how machines understand\\nand generate human language, exhibiting a level of\\nsophistication and versatility previously unattain-\\nable. One of the most exciting developments in\\nthis field is the capability of LLMs to function not\\njust as language processors, but as agents capable\\nof performing complex tasks. This evolution is ev-\\nident in initiatives such as AutoGPT (Yang et al.,\\n2023a) and MetaGPT (Hong et al., 2023), which\\nshowcase the practical applications of LLMs in\\ntasks requiring advanced cognitive functions like\\nreasoning, planning, and collaboration. The signifi-\\ncance of these developments cannot be overstated,\\nas they extend the utility of LLMs beyond simple\\nlanguage tasks, revolutionizing various aspects of\\ntechnology and daily life.\\nHowever, a key limitation of these LLM-based\\nagents has been their reliance solely on text-based\\ninformation. This restriction has historically cur-\\ntailed their perception and interaction with their\\nenvironment. The introduction of models equipped\\nwith vision capabilities, such as the latest itera-\\ntion of GPT-4, marks a pivotal breakthrough. By\\nintegrating the ability to process and interpret vi-\\nsual information, these models can now understand\\naspects of their surroundings that are difficult or\\nimpossible to convey through text alone. This ex-\\ntended capability enables LLMs to interpret con-\\ntext, recognize patterns, and respond to visual cues,\\nthus providing a more holistic and interactive expe-\\nrience with the world.\\nIn our work, we focus on building a multimodal\\nagent leveraging the vision capabilities of multi-\\nmodal large language models to undertake tasks\\npreviously unachievable by text-only agents. In\\nparticular, we explore an interesting but challeng-\\ning application that builds an agent to operate any\\nsmartphone application (App) in the mobile oper-\\nating system. Our approach differs significantly\\nfrom existing intelligent phone assistants like Siri,\\nwhich operate through system back-end access and\\nfunction calls. Instead, our agent interacts with\\nsmartphone apps in a human-like manner, using\\nlow-level operations such as tapping and swiping\\non the graphical user interface (GUI). The proposed\\nagent offers multiple advantages. Firstly, it elimi-\\nnates the need for system back-end access, making\\nour agent universally applicable across various ap-\\nplications. Additionally, this approach enhances\\nsecurity and privacy, as the agent does not require\\ndeep system integration. Furthermore, by operating\\non the GUI level, our agent can adapt to changes\\nin app interfaces and updates, ensuring long-term\\napplicability and flexibility.\\nHowever, creating a multimodal agent capable\\nof operating diverse smartphone apps presents sig-\\nnificant challenges. Existing research indicates that\\nadapting current models for embodied tasks ne-\\ncessitates extensive training data, and collecting a\\nlarge dataset of app demonstrations for training is\\na formidable task. Moreover, different apps have\\nunique GUIs with varying icon meanings and op-\\nerational logic, and it remains uncertain whether\\nthese adapted models can effectively generalize to\\nunseen apps.\\nIn this paper, we introduce a multimodal agent\\nframework aimed at operating any smartphone app\\nlike human users. The learning of our framework\\ninvolves an exploration phase where the agent in-\\nteracts autonomously with apps through a set of\\npre-defined actions and learns from their outcomes.\\nThese interactions are documented, which assists\\nthe agent in navigating and operating the apps. This\\nlearning process can be accelerated by observing\\na few human demonstrations. Following this ex-\\nploratory phase, the agent can operate the app by\\nconsulting the constructed document based on its\\ncurrent state, eliminating the need to adapt the pa-\\nrameters of the LLMs or collect extensive training\\ndata for each app.\\nTo validate its effectiveness, we tested our agent\\non 50 tasks across 10 different apps, ranging from\\nsocial media and messaging to email, maps, shop-\\nping, and even complex image editing apps. Both\\nquantitative results and user studies underscore the\\nadvantages of our design, particularly its adapt-\\nability, user-friendliness, and efficient learning and\\n2\\noperating capabilities across a wide range of appli-\\ncations. This underlines the potential of our agent\\nas a versatile and effective tool in the realm of\\nsmartphone app operation.\\nIn summary, this paper makes the following con-\\ntributions:\\n• We open-source a multimodal agent frame-\\nwork, focusing on operating smartphone ap-\\nplications with our developed action space.\\n• We propose an innovative exploration strategy,\\nwhich enables the agent to learn to use novel\\napps.\\n• Through extensive experiments across multi-\\nple apps, we validate the advantages of our\\nframework, demonstrating its potential in the\\nrealm of AI-assisted smartphone app opera-\\ntion.\\n2\\nRelated Work\\n2.1\\nLarge language models\\nThe development of ChatGPT (OpenAI, 2021) and\\nGPT-4 (OpenAI, 2023) represents a crucial ad-\\nvancement in natural language processing. Unlike\\nearlier large language models (LLMs), these new\\nmodels (Touvron et al., 2023a,b; Zeng et al., 2022;\\nTaori et al., 2023; Zheng et al., 2023) enable multi-\\nround conversations and have the impressive ability\\nto follow complex instructions. The integration of\\nvision capabilities in GPT-4V (Yang et al., 2023b)\\nis a further milestone, enabling the language model\\nto process and interpret visual data. This addition\\nhas broadened the scope of potential AI applica-\\ntions, allowing GPT-4 to undertake diverse tasks\\nsuch as problem-solving, logical reasoning, tool\\nusage, API calls, and coding. Recent studies (Yang\\net al., 2023c; Yan et al., 2023) have shown that\\nGPT-4V can understand various types of images,\\nincluding simple user interfaces (UIs) in popular\\nsmartphone apps. However, challenges arise when\\nthe apps are new and their UIs are less typical,\\nwhich highlights a major problem that our work\\naims to address. Among open-source efforts from\\nthe industry and research community, the LLaMA\\nseries (Touvron et al., 2023a,b) are the most pop-\\nular equivalents and have been fine-tuned to ac-\\nquire conversational abilities, employing a decoder-\\nonly architecture similar to ChatGPT (Taori et al.,\\n2023; Zheng et al., 2023). Building upon LLaMA,\\nmany multimodal LLMs, such as LLaVA (Liu et al.,\\n2023b,a), ChartLlama (Han et al., 2023), and Sta-\\nbleLLaVA (Li et al., 2023), also demonstrate vision\\nunderstanding capabilities akin to those of GPT-4V.\\nNevertheless, a performance gap persists between\\nthese open-source models and GPT-4V, suggesting\\npotential areas for further development.\\n2.2\\nLLMs as agents\\nThe use of LLMs as agents for executing com-\\nplex tasks has gained increasing attention. Ini-\\ntiatives like AutoGPT (Yang et al., 2023a), Hug-\\ngingGPT (Shen et al., 2023), and MetaGPT (Hong\\net al., 2023) illustrate this trend, and many projects\\ndemonstrate impressive capabilities, moving be-\\nyond basic language tasks to engaging in activi-\\nties requiring higher cognitive functions, such as\\nsoftware development (Qian et al., 2023; Chen\\net al., 2021) and gaming (FAIR et al., 2022; Park\\net al., 2023; Xu et al., 2023).\\nIn this context,\\nYao et al. (Yao et al., 2023) introduce an innova-\\ntive approach that synergizes reasoning and acting\\nin LLMs, significantly enhancing their decision-\\nmaking and interactive capabilities. LLM-based\\nagents are designed to utilize the advanced lan-\\nguage and reasoning skills of LLMs to interact\\nwith and manipulate their environment (Liu et al.,\\n2023c; Gur et al., 2023; Xie et al., 2023). This in-\\ncludes performing tasks that require understanding\\ncontext, making decisions, and learning from inter-\\nactions (Xi et al., 2023; Hu and Shu, 2023). Such\\nagents are pivotal in applications where human-like\\ncognitive abilities are essential.\\nThe\\nemergence\\nof\\nmultimodal\\nLLM\\nagents (Wang et al., 2023; Furuta et al., 2023;\\nBrohan et al., 2022, 2023; Reed et al., 2022),\\ncapable of processing various inputs including text,\\nimages, audio, and video, has further broadened\\nthe scope of LLM applications. This versatility\\nis particularly beneficial for LLM-based agents,\\nenabling them to interact more effectively with\\ntheir environment and complete more complex\\ntasks, be it completing household tasks in a\\nphysical world (Ahn et al., 2022), generating 3D\\nassets via procedural tool use (Sun et al., 2023), or\\nmastering over 600 tasks across different domains\\nat the same time (Reed et al., 2022). Our research\\ncontributes to this area by focusing on an agent\\ndesigned to operate smartphone applications. This\\nagent’s ability to interpret screenshots from the\\noperating system demonstrates its flexibility and\\nadaptability, making it a valuable tool in a wide\\n3\\nTap\\n👀\\n👀\\n“Tapping this UI element opens \\nthe color settings to adjust the \\ncolor of the image.”\\n“Swiping this UI element allows the \\nuser to adjust the color \\ntemperature of the image.”\\n🤔\\n…\\n…\\nExploration Phase\\nDeployment Phase: “beautify this photo for me…”\\n…\\n…\\nObserve, Think, Act, and Summarize\\nSwipe\\nDoc for App\\nFigure 2: Overview of our multimodal agent framework designed to operate smartphone applications. The\\nfigure illustrates the two-phase approach of our framework. In the exploration phase, the agent interacts with a\\nsmartphone application and learns from their outcomes to create a comprehensive reference document. In the\\ndeployment phase, the agent utilizes the information compiled in this document to operate and navigate the apps\\neffectively.\\nrange of applications.\\n3\\nMethod\\nThis section details the methodology behind our in-\\nnovative multimodal agent framework. This frame-\\nwork enables an agent to interact with smartphone\\napplications in a manner akin to human behavior.\\nWe first describe the experimental environment and\\naction space, which are foundational elements of\\nour system. Next, we discuss the exploration phase,\\nwhere the agent learns app functionalities either\\nthrough autonomous interactions or by observing\\nhuman demonstrations. Finally, we outline the de-\\nployment phase, explaining how the agent applies\\nits acquired knowledge to execute high-level tasks.\\n3.1\\nEnvironment and Action Space\\nExperimental Environment: Our experimental\\nenvironment is built on a command-line interface\\n(CLI), allowing the agent to interact with smart-\\nphone apps. We chose the Android operating sys-\\ntem for our experiments. The agent receives two\\nkey inputs: a real-time screenshot showing the\\napp’s interface and an XML file detailing the in-\\nteractive elements. To enhance the agent’s ability\\nto identify and interact with these elements seam-\\nlessly, we assign each element a unique identifier.\\nThese identifiers are derived either from the re-\\nsource ID in the XML file (if provided) or are con-\\nstructed by combining the class name, size, and\\ncontent of the element. These elements are over-\\nlaid as semi-transparent numbers on the screenshot.\\nThis helps the agent to interact accurately without\\nneeding to specify exact positions on the screen\\nand enhances the agent’s precision in controlling\\nthe phone.\\nAction Space: Our agent’s action space mir-\\nrors common human interactions with smartphones:\\ntaps and swipes. We designed four basic functions:\\n• Tap(element : int) : This function simu-\\nlates a tap on the UI element numbered on\\nthe screen. For example, tap(5) would tap\\nthe element labeled ‘5’.\\n• Long_press(element : int) : This func-\\ntion emulates a long press (for 1 second) on a\\nUI element.\\n• Swipe ( element : int, direction : str,\\ndist : str): It allows the agent to swipe on\\nan element in a specified direction (up, down,\\nleft, right) and distance (short, medium, long).\\nFor instance,\\nswipe(21, “up”, “medium”)\\nwould swipe up on element ‘21’ for a medium\\ndistance.\\n• Text(text : str) : To bypass inefficient vir-\\ntual keyboard typing, this function inputs\\ntext directly into an input field when a\\nvirtual keyboard is visible.\\nFor exam-\\nple, text(“Hello, world!”) inputs the string\\n“Hello, world!\".\\n• Back() : A system-level function that helps\\nthe agent return to the previous UI page, espe-\\ncially useful for exiting irrelevant pages.\\n4\\n• Exit() : A specialized function is employed\\nto conclude processes, typically invoked upon\\nsuccessful task completion.\\nThese predefined actions are designed to sim-\\nplify the agent’s interactions, particularly by elim-\\ninating the need for precise screen coordinates,\\nwhich can pose challenges for language models\\nin accurately predicting.\\n3.2\\nExploration Phase\\nExploring by autonomous interactions. The Ex-\\nploration Phase is central to our framework. Here,\\nthe agent learns about the functionalities and fea-\\ntures of smartphone apps through trial and error. In\\nthis phase, the agent is assigned a task and starts\\ninteracting autonomously with the UI elements. It\\nuses different actions and observes the resulting\\nchanges in the app interface to understand how\\nit works. The agent, driven by a large language\\nmodel, attempts to figure out the functions of UI\\nelements and the effects of specific actions by an-\\nalyzing screenshots before and after each action.\\nThis information is compiled into a document that\\nrecords the effects of actions applied to different el-\\nements. When a UI element is acted upon multiple\\ntimes, the agent will update the document based\\non past documents and current observations to im-\\nprove quality. To make exploration more efficient,\\nthe agent stops further exploring UI elements if the\\ncurrent UI page seems unrelated to the main tasks\\nof the app, like advertisement pages. In such cases,\\nit uses the Android system’s Back() function to\\nreturn to the previous UI page. Compared with\\nrandom exploration, such as Depth-First Search\\nand Breadth-First Search, this goal-oriented explo-\\nration approach ensures that the agent focuses on\\nelements crucial for the effective operation of the\\napp. The agent also utilizes the LLM’s existing\\nknowledge about user interfaces to improve explo-\\nration efficiency. The exploration stops when the\\nagent completes the assigned task.\\nExploring by watching demos. An alternative\\nand often more effective exploration method in-\\nvolves the agent observing human demonstrations.\\nThese demonstrations provide the agent with ex-\\namples of efficient app usage, especially for un-\\nderstanding complex functionalities that might be\\nchallenging to discover through autonomous inter-\\nactions. In this method, a human user operates\\nthe apps while the agent observes, recording only\\nthe elements and actions employed by the human.\\nThis strategy narrows down the exploration space\\nand prevents the agent from engaging with irrele-\\nvant app pages, making it a more streamlined and\\nefficient approach compared to autonomous inter-\\nactions.\\n3.3\\nDeployment Phase\\nFollowing the exploration phase, the agent is well-\\nequipped to execute complex tasks based on its\\naccrued experience. The agent adheres to a step-\\nby-step approach when given a task, with each step\\nencompassing access to a screenshot of the current\\nUI and a dynamically generated document detail-\\ning the functions of UI elements and the actions’\\neffects on the current UI page. The prompts also\\nprovide detailed explanations of all available ac-\\ntions. In each step, the agent is first tasked with\\nproviding its observations of the current UI, fol-\\nlowed by articulating its thought process concern-\\ning the task and current observations. Subsequently,\\nthe agent proceeds to execute actions by invoking\\navailable functions. After each action, the agent\\nsummarizes the interaction history and the actions\\ntaken during the current step. This information is\\nincorporated into the next prompt, which provides\\nthe agent with a form of memory. This meticulous\\napproach enhances the reliability and interpretabil-\\nity of the agent’s actions, thereby facilitating more\\ninformed decision-making. The deployment phase\\nstops when the agent determines that the task has\\nbeen accomplished, at which point it can exit the\\nprocess by taking the Exit() action.\\n4\\nExperiments\\nIn this section, we will present our evaluation of\\nthe multimodal agent framework through a combi-\\nnation of quantitative and qualitative experiments.\\nOur primary goal is to assess the agent’s perfor-\\nmance and its ability to operate a diverse set of\\nsmartphone applications effectively.\\n4.1\\nExperimental Setup\\nTo comprehensively evaluate our method, we con-\\nstruct a benchmark that includes 10 popular appli-\\ncations, each serving various purposes. These ap-\\nplications include Google Maps, Twitter, Telegram,\\nYouTube, Spotify, Yelp, Gmail, TEMU, Clock, and\\nLightroom. We have intentionally chosen this di-\\nverse set of apps to test the agent’s adaptability\\nacross various functions and interfaces. In particu-\\nlar, to gain a more comprehensive insight into the\\n5\\nThe search field is focused and a keyboard is \\npresent on the lower half of the screen. UI \\nelement labeled \\'6\\' appears to be the search input \\nfield where text can be input to find locations, \\naddresses, or points of interest.\\nObservation\\nThought\\nAction\\ntext(\"Tencent Shanghai Branch\")\\nTo complete the given task, I should enter the \\ntext \"Tencent Shanghai Branch\" in the search \\nfield to proceed with the search for the \\ndestination.\\nA keyboard and a search result for \"Tencent Shanghai \\nBranch\" with address details and a distance measurement \\nare displayed.\\nObservation\\nThought\\nAction\\ntap(1)\\nTo complete the given task, I should confirm the search \\nfor \"Tencent Shanghai Branch\". In this screenshot, the \\nsearch confirm button is represented by the blue button\\nwith the text “\\\\u641c\\\\u7d22” (“Search” in Chinese) on it.\\nThe screenshot shows a map application with the \\nlocation \"Tencent Shanghai Branch\" pinpointed \\nwith a red marker. There are two buttons at the \\nbottom of the screen, one labeled \"Directions\" and \\nthe other labeled \"Start\", with numeric tags 1 and \\n3 respectively.\\nObservation\\nThought\\nAction\\ntap(2)\\nTo complete the given task, which is to \\nnavigate to Tencent Shanghai Branch, I should \\ntap the \"Directions\" button to initiate the \\nroute finding process.\\nObservation\\nThe recipient\\'s email address is already filled \\nin, and the subject line reads \"Inquiry About \\nYour New Position.\" The body of the email is\\nnot visible, and there\\'s a UI element labeled \\n\\'11\\' which is used to collapse the keyboard.\\nThought\\nAction\\ntap(11)\\nTo complete the task, the next step should be \\nto collapse the keyboard so that I can access \\nthe body of the email to type the message to \\nJane Doe asking about her new job.\\nObservation\\nThe recipient\\'s email address is already filled in as \\njanedoe@email.com and the subject line is \"Inquiry \\nAbout Your New Position.\" The cursor is blinking in \\nthe main body area of the email, indicating that it is \\nready to receive text input. The on-screen keyboard is \\nalso visible.\\nThought\\nAction\\ntext(\"Dear Jane Doe,\\\\n\\\\nI am writing to inquire \\nabout your new position that you recently started…\")\\nTo complete the given task, the next step I should do is \\nto type the content of the email asking Jane Doe about \\nher new job.\\nObservation\\nThe email body is typed with a message asking \\nJane Doe about her recent job and expressing \\neagerness to hear from her. There is a send \\nbutton labeled with the numeric tag \\'3’.\\nThought\\nAction\\ntap(3)\\nTo complete the given task, the next step I \\nshould take is to send the email by tapping \\nthe send button.\\nObservation\\nThe screenshot shows a photo editing application \\ninterface with various adjustment settings for \\nexposure, contrast, highlights, and shadows. \\nThese adjustments have numeric tags 20, 21, 22, \\nand 23 respectively. The settings are currently at \\ntheir default values, and the image appears \\nunderexposed, needing improvement in lighting.\\nThought\\nAction\\nswipe(20, \"right\", \"medium\")\\nTo improve the image quality and correct the \\nunderexposure, I need to increase the \\nexposure. The slider for the exposure setting \\nis labeled with numeric tag 20, which I can \\ninteract with to adjust the exposure level. I will \\nswipe right on this slider to increase the \\nexposure.\\nObservation\\nThe exposure level has been increased, as indicated by the \\nslider tagged with number 20 being to the right. The \\ncontrast, highlights, and shadows are yet to be adjusted, \\nwith their sliders at the neutral position. There are also \\nother adjustment options available, such as color and detail, \\nwhich could further improve the image quality if needed.\\nThought\\nAction\\nswipe(21, \"right\", \"short\")\\nThe next step to improve the quality of the image could be \\nto adjust the contrast, as increasing contrast can often \\nmake an image appear more vivid and can complement \\nthe increased exposure. Based on the documentation, \\ninteracting with UI element labeled 21 will adjust the \\ncontrast level of the image.\\nObservation\\nThe exposure and contrast sliders are visible, \\nwith the exposure increased to +2.20 and the \\ncontrast increased to +24. The highlights and\\nshadows sliders are at 0, indicating no \\nadjustment has been made to them.\\nThought\\nAction\\ntap(22)\\nSince I have previously increased the exposure \\nand adjusted the contrast slightly, the next step \\ncould be to adjust the highlights or shadows to \\nfurther improve the image quality. Adjusting \\nthe highlights might help in bringing out more \\ndetail in the brighter parts of the image \\nwithout affecting the overall exposure too much.\\nGoogle Maps: navigate to Tencent Shanghai Branch\\nGmail: send an email to janedoe@email.com to ask her about her new job\\nLightroom: improve the quality of this image until it looks good to you\\nFigure 3: Qualitative Task Evaluation Across Three Apps.\\nThis figure presents qualitative results for three\\ndistinct tasks conducted on Google Maps, Gmail, and Lightroom. It showcases AppAgent’s ability to accurately\\nperceive, reason, and execute tasks, demonstrating its competence in various application contexts. Due to space\\nconstraints, some less critical details have been omitted from the description.\\nvision capabilities of our agent, we conducted an\\nin-depth case study using Adobe Lightroom, an\\nimage-editing application. This specific case study\\nallowed us to evaluate the agent’s proficiency in\\nhandling visual tasks and its ability to interpret\\nand manipulate images within the app. For the\\nexploration phase, we capped the maximum num-\\nber of steps at 40. During testing, we limited the\\nmaximum number of steps to 10. For these experi-\\nments, we utilized the state-of-the-art multimodal\\nlarge language model, GPT-4. GPT-4 is equipped\\nto process interleaved image-and-text inputs effec-\\ntively. This unique capability enables our agent to\\ninterpret and interact with both visual and textual\\ninformation seamlessly within the applications.\\n4.2\\nDesign and Analysis\\nBaselines. To comprehensively evaluate our mul-\\ntimodal agent framework, we considered various\\ndesign choices and their impact on performance.\\nWe conducted experiments using different configu-\\nrations to provide valuable insights into the agent’s\\nbehavior. We started with GPT-4 without any ref-\\nerence documents during testing and examined its\\nperformance both with the raw action API and\\nour simplified action space. Next, we explored\\ndifferent ways to generate guiding documents for\\nthe agent. These included documents generated\\nthrough autonomous exploration, watching human\\ndemonstrations, and the manually crafted docu-\\nment as an oracle benchmark.\\nTo effectively compare the performance of dif-\\n6\\nMethod\\nDocument\\nAction Space\\nSR ↑\\nReward ↑\\nAvg. Steps\\nGPT4 (Baseline)\\nNone\\nRaw\\n2.2%\\n0.6\\n4.0\\nNone\\nOurs\\n48.9%\\n3.5\\n6.9\\nAppAgent\\nAuto. Exploration\\nOurs\\n73.3%\\n5.1\\n4.4\\nWatching Demos\\nOurs\\n84.4%\\n4.7\\n5.1\\nManually Crafted\\nOurs\\n95.6%\\n5.5\\n5.5\\nTable 1: Evaluating Design Choices in AppAgent Performance. This table contrasts different design elements\\nwithin AppAgent. Key findings include: our custom-developed action space surpasses the raw action space in effi-\\nciency; the exploration phase, incorporating both autonomous interaction and observation of human demonstrations,\\nsignificantly enhances agent performance; and the auto-generated documentation yields outcomes on par with those\\nderived from manually crafted documents.\\nMethod\\nDocument\\nAction Space\\nAvg. Rank ↓\\nNum. Tools\\nGPT4 (Baseline)\\nNone\\nOurs\\n2.30\\n2.4\\nAppAgent\\nWatching Demos\\nOurs\\n1.95\\n5.8\\nManually Crafted\\nOurs\\n1.75\\n4.0\\nTable 2: Case study on image editing tasks with Lightroom App. We conduct a user study to rank the image\\nediting results of different methods. Our agents produce better results than the GPT-4 baseline.\\nferent methods, we employed three key metrics:\\nSuccessful Rate (SR): This metric measures the\\naverage rate at which the agent successfully com-\\npletes tasks within an app. If the agent fails to\\nfinish the task in 10 steps, it is considered a failure.\\nReward: To provide a more fine-grained measure-\\nment, we developed a reward model to assess per-\\nformance. For each task within an app, we scored\\ndifferent UI pages. The closer the UI page was to\\nthe objective, the higher the score received. This\\nmeans that even if the agent failed to complete the\\ntask, it would still receive credit based on its final\\nstate.\\nAverage Steps: We also reported the average num-\\nber of steps required to successfully finish tasks\\nacross the selected applications.\\nResults. The comparison of our experimental re-\\nsults is presented in Table 1. We report the average\\nperformance of 45 tasks on 9 of the 10 previously\\ndescribed apps. Notably, we excluded Lightroom\\nfrom this evaluation, as assessing task completion\\nin this application presented inherent ambiguities.\\nAs demonstrated, our simplified action space sig-\\nnificantly improves the performance of the GPT-4\\nbaseline. Our observations indicate that LLM strug-\\ngles with producing accurate xy coordinates, while\\nour simplified action space eliminates this challeng-\\ning requirement. Additionally, documents gener-\\nated through autonomous exploration and observ-\\ning human demonstrations proved to be highly ef-\\nfective. Their results consistently outperformed the\\nGPT-4 baseline and are comparable to the results\\nof human-written documents, which highlights the\\nefficacy of our design in enhancing the agent’s per-\\nformance across a diverse set of applications.\\nQualitative results. In Fig. 3, we provide ex-\\namples showcasing the agent’s execution process\\nfor various tasks. This qualitative analysis serves\\nto demonstrate the agent’s capacity to accurately\\nperceive, reason, and act in response to given tasks.\\nFor a more comprehensive understanding of our\\nagent’s capabilities, please refer to our project page,\\nwhich includes additional demonstration videos.\\n4.3\\nCase Study\\nTo gain deeper insights into the vision capabili-\\nties of our agent, we conducted an extensive case\\nstudy using Adobe Lightroom, an image-editing\\napplication. This specific case study allowed us\\nto evaluate the agent’s proficiency in handling vi-\\nsual tasks, which was previously impossible for\\ntext-only agent models. Lightroom, as an image-\\nediting app with various editing tools, demands a\\nwide range of operations, such as selecting appro-\\npriate tools and manipulating image parameters.\\nThis case study provides a robust evaluation of the\\nagent’s overall capabilities. Additionally, the open-\\nended nature of image editing tasks allows us to\\n7\\nassess the agent’s problem-solving abilities. We\\nprepared five images with visual issues, such as\\nlow contrast and overexposure. Various variants\\nof our model, as previously illustrated, were used\\nto edit these images. A user study was conducted\\nto rank the editing results produced by different\\nmethods. We also reported the average number of\\ntools used for image editing, providing an addi-\\ntional reference to the editing process’s complexity.\\nAll models were assigned the task of “fix this image\\nuntil it looks good to you” without specifying the\\nimage’s problems. The comparison of the results\\nis presented in Table 2. As we can see, our agent\\nmodel with documents yields consistently better\\nresults than the GPT-4 baseline, which emphasizes\\nthe influence of documents in our design. The gen-\\nerated documents by watching the demonstration\\nproduced comparable results with the results of\\nmanually crafted documents, which suggests the\\neffectiveness of the exploration phase. We also\\nfind that with a document, the agent tends to use\\nvarious tools to improve the image quality, while\\nthe GPT-4 baseline uses fewer tools.\\n5\\nConclusion\\nIn this paper, we have introduced a novel multi-\\nmodal agent framework that leverages the vision\\ncapabilities of large language models to operate\\nsmartphone applications in a human-like manner.\\nOur approach eliminates the need for system back-\\nend access and offers security, adaptability, and\\nflexibility advantages. Our exploration-based learn-\\ning strategy allows the agent to quickly adapt to\\nnew applications with unfamiliar user interfaces,\\nmaking it a versatile tool for various tasks. Our\\nextensive experiments across various apps high-\\nlight our agent’s ability to handle diverse high-level\\ntasks and underscore its adaptability and learning\\nefficiency.\\nLimitation. We have adopted a simplified action\\nspace for smartphone operations, which means that\\nadvanced controls such as multi-touch and irregu-\\nlar gestures are not supported. This limitation may\\nrestrict the agent’s applicability in some challeng-\\ning scenarios. Nevertheless, we recognize this as\\nan avenue for future research and development.\\nReferences\\nMichael Ahn, Anthony Brohan, Noah Brown, Yev-\\ngen Chebotar, Omar Cortes, Byron David, Chelsea\\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol\\nHausman, Alex Herzog, Daniel Ho, Jasmine Hsu,\\nJulian Ibarz, Brian Ichter, Alex Irpan, Eric Jang,\\nRosario Jauregui Ruano, Kyle Jeffrey, Sally Jes-\\nmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalash-\\nnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey\\nLevine, Yao Lu, Linda Luu, Carolina Parada, Pe-\\nter Pastor, Jornell Quiambao, Kanishka Rao, Jarek\\nRettinghouse, Diego Reyes, Pierre Sermanet, Nico-\\nlas Sievers, Clayton Tan, Alexander Toshev, Vincent\\nVanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,\\nMengyuan Yan, and Andy Zeng. 2022. Do as i can\\nand not as i say: Grounding language in robotic af-\\nfordances. In arXiv preprint arXiv:2204.01691.\\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen\\nChebotar, Xi Chen, Krzysztof Choromanski, Tianli\\nDing, Danny Driess, Avinava Dubey, Chelsea Finn,\\net al. 2023. Rt-2: Vision-language-action models\\ntransfer web knowledge to robotic control. arXiv\\npreprint arXiv:2307.15818.\\nAnthony Brohan, Noah Brown, Justice Carbajal, Yev-\\ngen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana\\nGopalakrishnan, Karol Hausman, Alex Herzog, Jas-\\nmine Hsu, et al. 2022. Rt-1: Robotics transformer\\nfor real-world control at scale.\\narXiv preprint\\narXiv:2212.06817.\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\\nGreg Brockman, et al. 2021.\\nEvaluating large\\nlanguage models trained on code. arXiv preprint\\narXiv:2107.03374.\\nMeta FAIR, Anton Bakhtin, Noam Brown, Emily Di-\\nnan, Gabriele Farina, Colin Flaherty, Daniel Fried,\\nAndrew Goff, Jonathan Gray, Hengyuan Hu, et al.\\n2022. Human-level play in the game of diplomacy\\nby combining language models with strategic reason-\\ning. Science, 378(6624):1067–1074.\\nHiroki Furuta, Kuang-Huei Lee, Ofir Nachum, Yutaka\\nMatsuo, Aleksandra Faust, Shixiang Shane Gu, and\\nIzzeddin Gur. 2023. Multimodal web navigation with\\ninstruction-finetuned foundation models.\\nIzzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa\\nSafdari, Yutaka Matsuo, Douglas Eck, and Aleksan-\\ndra Faust. 2023. A real-world webagent with plan-\\nning, long context understanding, and program syn-\\nthesis.\\nYucheng Han, Chi Zhang, Xin Chen, Xu Yang, Zhibin\\nWang, Gang Yu, Bin Fu, and Hanwang Zhang. 2023.\\nChartllama: A multimodal llm for chart understand-\\ning and generation.\\nSirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu\\nZheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang,\\nZili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang\\nZhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu,\\nand Jürgen Schmidhuber. 2023. Metagpt: Meta pro-\\ngramming for a multi-agent collaborative framework.\\n8\\nZhiting Hu and Tianmin Shu. 2023. Language mod-\\nels, agent models, and world models: The law for\\nmachine reasoning and planning.\\narXiv preprint\\narXiv:2312.05230.\\nYanda Li, Chi Zhang, Gang Yu, Zhibin Wang, Bin\\nFu, Guosheng Lin, Chunhua Shen, Ling Chen, and\\nYunchao Wei. 2023. Stablellava: Enhanced visual\\ninstruction tuning with synthesized image-dialogue\\ndata.\\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae\\nLee. 2023a. Improved baselines with visual instruc-\\ntion tuning.\\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\\nLee. 2023b. Visual instruction tuning.\\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu\\nLei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen\\nMen, Kejuan Yang, Shudan Zhang, Xiang Deng, Ao-\\nhan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng\\nShen, Tianjun Zhang, Yu Su, Huan Sun, Minlie\\nHuang, Yuxiao Dong, and Jie Tang. 2023c. Agent-\\nBench: Evaluating LLMs as agents. arXiv preprint\\narXiv: 2308.03688.\\nOpenAI. 2021.\\nChatgpt.\\nhttps://openai.com/\\nresearch/chatgpt.\\nOpenAI. 2023. Gpt-4 technical report.\\nJoon Sung Park, Joseph O’Brien, Carrie Jun Cai, Mered-\\nith Ringel Morris, Percy Liang, and Michael S Bern-\\nstein. 2023. Generative agents: Interactive simulacra\\nof human behavior. In Proceedings of the 36th An-\\nnual ACM Symposium on User Interface Software\\nand Technology, pages 1–22.\\nChen Qian, Xin Cong, Cheng Yang, Weize Chen,\\nYusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong\\nSun. 2023. Communicative agents for software de-\\nvelopment. arXiv preprint arXiv:2307.07924.\\nScott Reed, Konrad Zolna, Emilio Parisotto, Ser-\\ngio Gomez Colmenarejo,\\nAlexander Novikov,\\nGabriel Barth-Maron, Mai Gimenez, Yury Sulsky,\\nJackie Kay, Jost Tobias Springenberg, et al. 2022. A\\ngeneralist agent. arXiv preprint arXiv:2205.06175.\\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\\nWeiming Lu, and Yueting Zhuang. 2023. Hugging-\\ngpt: Solving ai tasks with chatgpt and its friends\\nin huggingface. In Advances in Neural Information\\nProcessing Systems.\\nChunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang,\\nZishan Qin, and Stephen Gould. 2023. 3d-gpt: Proce-\\ndural 3d modeling with large language models. arXiv\\npreprint arXiv:2310.12945.\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\\nAn instruction-following llama model.\\nhttps://\\ngithub.com/tatsu-lab/stanford_alpaca.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. 2023a. Llama: Open\\nand efficient foundation language models.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\\nScialom. 2023b. Llama 2: Open foundation and\\nfine-tuned chat models.\\nZihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jin-\\nbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He,\\nZilong Zheng, Yaodong Yang, Xiaojian Ma, and\\nYitao Liang. 2023.\\nJarvis-1: Open-world multi-\\ntask agents with memory-augmented multimodal lan-\\nguage models.\\nZhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen\\nDing, Boyang Hong, Ming Zhang, Junzhe Wang,\\nSenjie Jin, Enyu Zhou, et al. 2023. The rise and\\npotential of large language model based agents: A\\nsurvey. arXiv preprint arXiv:2309.07864.\\nTianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Lu-\\noxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao,\\nQian Liu, Che Liu, Leo Z. Liu, Yiheng Xu, Hongjin\\nSu, Dongchan Shin, Caiming Xiong, and Tao Yu.\\n2023. Openagents: An open platform for language\\nagents in the wild.\\nYuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xi-\\naolong Wang, Weidong Liu, and Yang Liu. 2023.\\nExploring large language models for communica-\\ntion games: An empirical study on werewolf. arXiv\\npreprint arXiv:2309.04658.\\nAn Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin,\\nLinjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong,\\nJulian McAuley, Jianfeng Gao, Zicheng Liu, and\\nLijuan Wang. 2023. Gpt-4v in wonderland: Large\\nmultimodal models for zero-shot smartphone gui nav-\\nigation. arXiv preprint arXiv: 2311.07562.\\n9\\nHui Yang, Sifu Yue, and Yunzhong He. 2023a. Auto-\\ngpt for online decision making: Benchmarks and\\nadditional opinions.\\nZhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng\\nWang, Chung-Ching Lin, Zicheng Liu, and Lijuan\\nWang. 2023b.\\nThe dawn of lmms: Preliminary\\nexplorations with gpt-4v (ision).\\narXiv preprint\\narXiv:2309.17421.\\nZhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang,\\nChung-Ching Lin, Zicheng Liu, and Lijuan Wang.\\n2023c.\\nThe dawn of lmms: Preliminary explo-\\nrations with gpt-4v(ision).\\narXiv preprint arXiv:\\n2309.17421.\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\\nShafran, Karthik Narasimhan, and Yuan Cao. 2023.\\nReAct: Synergizing reasoning and acting in language\\nmodels. In ICLR.\\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\\nWendi Zheng, Xiao Xia, et al. 2022.\\nGlm-130b:\\nAn open bilingual pre-trained model. arXiv preprint\\narXiv:2210.02414.\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\\nZhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,\\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging\\nllm-as-a-judge with mt-bench and chatbot arena.\\n10\\n[\\n    {\\n        \"title\": \"joke\",\\n        \"date_created\": \"1974\",\\n        \"art_writing_type\": \"Article\",\\n        \"dimensions\": {\\n            \"pages\": 30,\\n            \"page_format\": \".pdf\",\\n            \"file_size\": \"2.34mb\"\\n        },\\n        \"description\": \"writing\",\\n        \"relevance\": \"no more\",\\n        \"themes\": \"artistic\",\\n        \"keywords\": \"pupils\",\\n        \"abstract\": \"noun\",\\n        \"artworks_or_artists\": \"none\",\\n        \"citation_information\": \"horn\",\\n        \"supplementary_materials\": \"ww.ee.rn\",\\n        \"image_credits\": \"me\",\\n        \"video_credits\": \"me\",\\n        \"related_publications\": \"me\",\\n        \"recommended_readings\": \"me\",\\n        \"references\": \"me\",\\n        \"external_links\": \"me\",\\n        \"journal_details\": {\\n            \"journal_name\": \"me\",\\n            \"publisher\": \"me\",\\n            \"volume_and_issue\": \"me\",\\n            \"page_range\": \"23\",\\n            \"issn_or_isbn\": \"2mr=e223m2\",\\n            \"doi\": \"dd\",\\n            \"date_of_publication\": \"me\"\\n        },\\n        \"key_takeaways\": {\\n            \"1\": \"me\",\\n            \"2\": \"me\",\\n            \"3\": \"me\"\\n        },\\n        \"personal_influence\": \"me\",\\n        \"personal_journey\": \"me\",\\n        \"academic_review\": {\\n            \"reviewer_1_name\": \"me\",\\n            \"reviewer_2_name\": \"me\",\\n            \"review_summary\": \"me\"\\n        },\\n        \"overall_evaluation\": \"me\"\\n    },\\n    {\\n        \"title\": \"joke\",\\n        \"date_created\": \"1974\",\\n        \"art_writing_type\": \"Article\",\\n        \"dimensions\": {\\n            \"pages\": 30,\\n            \"page_format\": \".pdf\",\\n            \"file_size\": \"2.34mb\"\\n        },\\n        \"description\": \"writing\",\\n        \"relevance\": \"no more\",\\n        \"themes\": \"artistic\",\\n        \"keywords\": \"pupils\",\\n        \"abstract\": \"noun\",\\n        \"artworks_or_artists\": \"none\",\\n        \"citation_information\": \"horn\",\\n        \"supplementary_materials\": \"ww.ee.rn\",\\n        \"image_credits\": \"me\",\\n        \"video_credits\": \"me\",\\n        \"related_publications\": \"me\",\\n        \"recommended_readings\": \"me\",\\n        \"references\": \"me\",\\n        \"external_links\": \"me\",\\n        \"journal_details\": {\\n            \"journal_name\": \"me\",\\n            \"publisher\": \"me\",\\n            \"volume_and_issue\": \"me\",\\n            \"page_range\": \"23\",\\n            \"issn_or_isbn\": \"2mr=e223m2\",\\n            \"doi\": \"dd\",\\n            \"date_of_publication\": \"me\"\\n        },\\n        \"key_takeaways\": {\\n            \"1\": \"me\",\\n            \"2\": \"me\",\\n            \"3\": \"me\"\\n        },\\n        \"personal_influence\": \"me\",\\n        \"personal_journey\": \"me\",\\n        \"academic_review\": {\\n            \"reviewer_1_name\": \"me\",\\n            \"reviewer_2_name\": \"me\",\\n            \"review_summary\": \"me\"\\n        },\\n        \"overall_evaluation\": \"me\"\\n    },\\n    {\\n        \"title\": \"joke\",\\n        \"date_created\": \"1974\",\\n        \"art_writing_type\": \"Article\",\\n        \"dimensions\": {\\n            \"pages\": 30,\\n            \"page_format\": \".pdf\",\\n            \"file_size\": \"2.34mb\"\\n        },\\n        \"description\": \"writing\",\\n        \"relevance\": \"no more\",\\n        \"themes\": \"artistic\",\\n        \"keywords\": \"pupils\",\\n        \"abstract\": \"noun\",\\n        \"artworks_or_artists\": \"none\",\\n        \"citation_information\": \"horn\",\\n        \"supplementary_materials\": \"ww.ee.rn\",\\n        \"image_credits\": \"me\",\\n        \"video_credits\": \"me\",\\n        \"related_publications\": \"me\",\\n        \"recommended_readings\": \"me\",\\n        \"references\": \"me\",\\n        \"external_links\": \"me\",\\n        \"journal_details\": {\\n            \"journal_name\": \"me\",\\n            \"publisher\": \"me\",\\n            \"volume_and_issue\": \"me\",\\n            \"page_range\": \"23\",\\n            \"issn_or_isbn\": \"2mr=e223m2\",\\n            \"doi\": \"dd\",\\n            \"date_of_publication\": \"me\"\\n        },\\n        \"key_takeaways\": {\\n            \"1\": \"me\",\\n            \"2\": \"me\",\\n            \"3\": \"me\"\\n        },\\n        \"personal_influence\": \"me\",\\n        \"personal_journey\": \"me\",\\n        \"academic_review\": {\\n            \"reviewer_1_name\": \"me\",\\n            \"reviewer_2_name\": \"me\",\\n            \"review_summary\": \"me\"\\n        },\\n        \"overall_evaluation\": \"me\"\\n    }\\n]ChatART\\nGet immediate insights, find artistic inspiration, acquire new\\nknowledge, discover the perfect artwork.\\nChatART simplifies art research. Ask questions, seek\\nrecommendations, or engage in discussions about new artistic\\nmovements – all within the comfort of a conversational interface.\\nOverview\\nChatART is an expert chatbot designed to simplify art research. It provides a\\nconversational interface for users to ask questions, seek recommendations, and\\nengage in discussions about new artistic movements. Powered by expert training\\ndata curated by professional art researchers, ChatART stands as the ﬁrst art expert\\nchatbot, offering precision and niche insights beyond the ordinary.\\nKey Features:\\n●\\n🎨 Personalized Insights: ChatART provides personalized, intelligent insights\\ninto high-quality artistic knowledge, new artworks, young artists\\' practices,\\ntrending aesthetics and emerging art movements.\\n●\\n🤖 Expert Training Data: Powered by expert training data curated, veriﬁed, and\\nregularly updated by professional art researchers, ChatART ensures accuracy\\nand relevance in every conversation.\\n●\\n🌐 Conversational Interface: Engage in seamless, intelligent conversations\\nabout art through an intuitive conversational interface.\\nEmpowering Art Professionals:\\nNew Base empowers users and clients to revolutionize art labeling, management,\\nand data recreation. For young artists, it facilitates visibility within the AI industry,\\nwhile assisting art professionals in making informed decisions and enhancing their\\nbusinesses with curated, high-quality art data.\\nMission\\nOur mission is to verify, curate, update, and maintain AI training datasets, ensuring\\nthe highest quality and relevance in the ﬁeld of ﬁne arts. We aim to seamlessly\\nintegrate young artists with the AI industry, fostering creativity and innovation.\\nGoal\\nChatART\\'s goal is to empower art professionals, from collectors to curators and art\\ntherapists, by providing personalized insights, trending art practices, and aesthetics.\\nWe aim to shape the future of art labeling, management, and data recreation, making\\nart history more inclusive, transparent, and accessible.\\nHow It Works\\nChatART facilitates an easy onboarding process for artists and art writers through\\nmetadata templates. Artists can apply artworks, while art writers can contribute\\narticles and publications. This structured data is crucial for building a high-quality\\ntraining database for AI, enhancing the accuracy and relevance of ChatART\\'s\\nresponses.\\nBeneﬁts Examples:\\n●\\nDiscover New Artists\\n●\\nDiscuss Art Movements\\n●\\nEvaluate Artwork Authenticity\\n●\\nCurate Art Exhibitions\\n●\\nStay Informed about Art Market Trends\\n●\\nExplore Art Therapy Techniques\\n●\\nDecode Symbolism in Art\\n●\\nReceive Art Material Recommendations\\n●\\nExplore Art and Technology Integration\\n●\\nContribute to Digital Art Preservation\\n●\\nAccess Art Educational Resources\\n●\\nEnhance Art World Networking\\nTarget Users\\nChatART is tailored for art professionals, including curators, art collectors, museum\\ndirectors, art therapists, urban planners, creatives, journalists, teachers, and\\nresearchers.\\nMarket Opportunity\\nThe market opportunity lies in catering to art enthusiasts, students, educators, and\\ninstitutions seeking innovative ways to explore and understand ﬁne arts.\\nCollaborations with galleries, museums, and technology companies interested in AI\\nand the arts present additional potential.\\nSubscription Model:\\nChatART offers a subscription-based service, allowing users to pay a recurring fee\\nfor access to ﬁne-tuning capabilities. This model includes various tiers based on\\ncustomization, storage, or processing power.\\nChatART envisions a future where AI-driven insights and curated data redeﬁne the\\nlandscape of ﬁne arts, making it more accessible, inclusive, and engaging for users\\nworldwide.\\nAdditional Information: Metadata about artists,\\nartworks, art writer, publications\\nArtist Information\\n●\\nArtist Name: `<Your Name>`\\n●\\nDescription: `<Brief description of yourself and your artistic focus>`\\n●\\nArtist Website: `<Your website URL, e.g. https://yourname.example.com>`\\n●\\nArtist Biography: `<A comprehensive biography detailing your art journey,\\neducation, and notable achievements, milestones, exhibitions, and projects>`\\n●\\nArtist Statement: `<A statement outlining your artistic philosophy, intent,\\ninterests, and vision, creative process, inspiration, and conceptual approach to\\nyour work. Incorporate elements that hold personal meaning for you, e.g. from\\nnature.>`\\n●\\nArtist Media: `<List of your favorite magazines, blogs, proﬁles, forums, pages,\\nstreams, youtube channels or exhibitions that have inﬂuenced your work, e.g.\\n[Blog](https://www.blog.com/), Etc.>`\\n●\\nArtist Interests: `<List any interests or passions that may inﬂuence or inform\\nyour artistic work, especially in the ﬁelds of design, architecture, music,\\nfashion, etc., e.g. metabolism, jazz, sustainable fashion>`\\n●\\nArtistic Process: <Outline your creative process, from initial idea to ﬁnished\\nartwork, explaining the steps, decisions, and challenges you encountered\\nalong the way>\\n●\\nPersonal Signiﬁcance: \\\\`<Share any personal connections or meanings that\\nyour work holds for you as the artist>\\\\`\\n●\\nReviews and Critical Reception: <Include notable reviews, critiques, or analysis\\nof your artwork from art critics, journalists, or scholars, and any positive or\\nnegative feedback that you have received or that you can remember>\\n●\\nArtist Contact Information: `<Your preferred contact details, like email\\naddress, phone number or social media handles>`\\n●\\nReferences or external links that provide additional context and background\\ninformation: `<Relevant publications, interviews, articles, or resources where\\nyour work has been featured, e.g. [Article\\nTitle](https://example.com/article-url)>`\\nArtwork Information\\n●\\nArtwork Title: `<Artwork Title>`\\n●\\nDate Created: `<Year of Creation>`\\n●\\nArt Medium: `<Art Medium e.g. \"Oil on Canvas\", “Sculpture”>`\\n●\\nArtform: `<Speciﬁc Artform or Technique e.g. \"Photography\", \"Sculpture\">`\\n●\\nDimensions: `<Height x Width x Depth (in inches or cm) for physical works or\\nﬁle size and resolution for digital works>`\\n- Width: `<Width in cm, in, or virtual units (e.g., px)>`\\n- Height: `<Height in cm, in, or virtual units (e.g., px)>`\\n- Depth: `<Depth in cm, in, or an indication such as \"N/A\" if not applicable>`\\n●\\nDescription of Artwork: `<A brief visual description of your artwork from a\\nnaive perspective, including its details, concept, colors, unique features,\\ntechniques, subject matter, themes, goals, contextual elements etc.>`\\n●\\nArt Movement: `<The art movement or style your work belongs to or is\\ninﬂuenced by, e.g., \"Bio Art\", \"Surrealism\", \"Ai Art\", \"Post Digital Art\", \"Mixed\\nMedia\", “Immersive Art”, “Land Art”>`\\n●\\nInspirations: `<List music, artists, works, events or other sources of inspiration\\nthat you like or that inﬂuenced your artwork>\\\\`\\n●\\nArtistic Techniques: `<Describe key techniques, materials, tools or processes\\nused in the creation of your artwork>\\\\`\\n●\\nMaterials: `<List the materials used in the creation of your artwork, such as\\npaint, clay, wood, metal, etc.>`\\n●\\nSubject Matter : `<Brieﬂy describe the subject matter of your artwork and its\\nsigniﬁcance within the context of your artistic practice>`\\n●\\nContextual Elements: `<Describe any objects, themes, images, networks,\\nhistorical, cultural, social, or political context that informed or inﬂuenced your\\nartwork>`\\n●\\nMotifs: `<List any recurring motifs and metaphors or patterns present in your\\nartistic process and explain their meaning or signiﬁcance>`\\n●\\nCollaborators: `<If your artwork involved collaborations with other artists or\\nprofessionals, provide their names and their roles in the project>`\\n●\\nSymbols: `<List any symbols used in your artwork and explain their meaning\\nor signiﬁcance>`\\n●\\nColor Palette: `<Describe the color palette and the reasoning behind your\\nchoice of colors>`\\n●\\nAttributes: `<List of attributes of your work, such as hand crafted, synthetic,\\nnatural, customizable, limited edition, one-of-a-kind, or scalable>\\\\`\\n●\\nCharacteristics: `<List the distinctive characteristics of your artwork, such as\\ntexture, form, scale, style, or visual elements that contribute to its overall look\\nand feel>`\\n●\\nMood: \\\\`<Describe the mood, emotional or mental state you were in while\\ncreating the artwork, and how it inﬂuenced the ﬁnal piece>\\\\`\\n●\\nTextures:`<Describe the textures present in your artwork, such as rough,\\nsmooth, layered, or tactile, and explain their importance or contribution to the\\noverall experience of the piece>`\\n●\\nFraming: `<Indicate whether your artwork is framed or unframed, and provide\\ndetails about the frame, such as materials, colors, and dimensions>`\\n●\\nStyle Inﬂuences: `<List general styles, artists, or works that have inﬂuenced\\nyour artistic style and approach to your artwork>`\\n●\\nTechnique Training: `<Mention any courses, workshops, or mentors who have\\nguided your development of the techniques used in your artwork>`\\n●\\nThemes:`<List the themes explored in your artwork, such as identity,\\nsimulation, nature, spirituality, technology, healing, meditation, algorithms,\\nnetworking, smart materials, sustainability, ecosystems, stock market or\\nsocial issues>`\\n●\\nConcept: `<Explain the central concept or idea behind your artwork, and how it\\nrelates to your artistic practice and vision>`\\n●\\nEvolution: `<Describe how your artwork has evolved over time, and the factors\\nthat have shaped its development>`\\n●\\nConservation: `<Information on the artwork\\'s durability, maintenance, and\\nrecommended conservation techniques to preserve its longevity and beauty>`\\n●\\nScenarios: `<Describe how your artwork behaves in different lighting\\nconditions or its interactions with viewers, e.g. when it is not observed, and\\nwhat are scenario-related concepts in your artwork such as the relationship\\nbetween presence and absence, the role of the viewer in experiencing the\\nartwork, and the transformative nature of different environments?>`\\n●\\nSettings:`<Discuss how your artwork looks, feels, or interacts in various\\nsettings, such as galleries, homes, outdoors, or public spaces, and the impact\\nthese settings could have on the viewer\\'s experience and perception of the\\npiece>\\\\`\\n●\\nExhibition History:`<Details of previous exhibitions or venues where your\\nartwork has been presented>\\\\`\\n●\\nAwards and Recognitions: `<List any awards, honors or recognitions your\\nartwork has received>`\\n●\\nCollections: `<Details of any collectors or institutions that own or have\\nexhibited your artwork>`\\n●\\nAdditional Keywords: `<List relevant keywords related to your artwork, helping\\nviewers to understand its content and themes>`\\n●\\nAdditional Information: `<Provide any other relevant information that might\\nhelp viewers better understand, appreciate, or engage with your artwork>`\\n●\\nReferences or external links that provide additional context and background\\ninformation: `<Relevant publications, interviews, articles, or resources where\\nyour work has been featured, e.g. [Article\\nTitle](https://example.com/article-url)>`\\n●\\nSound:`<If your artwork incorporates sound, describe the type of sound, its\\npurpose, and how it contributes to the overall experience of the piece>\\\\`\\n●\\nPlot:`<If your artwork tells a story or has a narrative, brieﬂy describe the plot\\nand how it unfolds within the artwork, as well as its signiﬁcance or connection\\nto the themes and concepts explored in the piece>\\\\`\\n●\\nSetting: The time, place, and environment in which the artwork was created,\\nproviding context and inﬂuencing the events and atmosphere.\\n●\\nConﬂict: The central problem or struggle that drives the narrative of the\\nartwork forward and creates tension, often involving obstacles or opposing\\nforces.\\n●\\nTone: The overall mood or attitude conveyed by the artwork, inﬂuencing the\\nobservers\\'s emotional response and perception of the art.\\n●\\nPoint of View: The perspective from which the artworks unfold, determining\\nthe level of access to thoughts, feelings, and the reliability of the narrator.\\n●\\nForeshadowing: Hints or clues about future events in the artworks narrative,\\ncreating anticipation and building suspense.\\n●\\nSymbolism: Objects, actions, or elements that represent deeper meanings or\\nconcepts within the artwork.\\n●\\nMotif: Recurring thematic elements or patterns that contribute to the overall\\nmeaning and coherence of the artwork.\\n●\\nLeitmotif: A recurring musical or thematic phrase associated with a particular\\nelement, idea, or situation in the artwork.\\n●\\nSubplots: Secondary themes or narratives that run parallel to the main theme,\\noften involving supporting associations.\\n●\\nMetaphor: A ﬁgure of perception or an aesthetic form that establishes a\\ncomparison between two unrelated objects or ideas, adding depth and layers\\nof meaning.\\n●\\nAnalogy: A comparison that explains an unfamiliar or abstract concept by\\ndrawing parallels to a more familiar or concrete concept.\\n●\\nMeta-Language: A contrast between what is shown and what actually comes\\nto the mind, often used to create absurd elements or evoke an act of\\nreﬂection, like meme references in your artwork.\\n●\\nFlashback: An aesthetic device that interrupts the chronological order of\\nevents to provide background information or reveal past experiences.\\n●\\nResolution: The ﬁnal artistic form or solution to the conﬂicts presented in the\\nartwork, tying up loose ends and providing conceptual closure.\\n●\\nClimax: The turning point or highest point of tension in the artwork, where the\\nconﬂicts reach their peak and the outcome becomes imminent.\\n●\\nDialogue: Interactions between elements like motifs or metaphors within the\\nartwork, revealing their appearance, emotional response, and advancing the\\nnarrative of the artwork.\\n●\\nNarrative Structure: The overall organization and framework of the artwork,\\nincluding the arrangement of events and the pacing of the narrative elements.\\nSales and Pricing Information\\n●\\nAsking Price: `<Artwork\\'s asking price, in your preferred currency>`\\n●\\nPayment Options: `<List accepted payment methods such as credit card,\\nPayPal, cryptocurrency, etc.>`\\n●\\nDiscounts or Offers: `<Include details about any discounts, bundle offers, or\\nother promotions related to your artwork>`\\n●\\nShipping Information: `<Specify shipping options, insurance, cost, estimated\\ndelivery time, handling fees, etc.>`\\n●\\nReturns Policy: `<Provide details about your return policy, including\\ntimeframes, conditions for accepting returns, and refund process>`\\n●\\nLimited Editions: `<If your artwork is part of a limited edition series, specify\\nthe edition size, numbering, and any special features, such as signed\\ncertiﬁcatesof authenticity>\\\\`\\n●\\nPurchase Information: \\\\`<Provide details on how interested parties can\\npurchase or inquire about your artwork, including contact information, pricing,\\nand any available editions>```\\n●\\nAuthentication: \\\\`<Describe the process for ensuring the authenticity of your\\nartwork, such as hand-signed certiﬁcates, holographic seals, or online\\nveriﬁcation systems>\\\\`\\n●\\nAvailability: \\\\`<State the current availability of your artwork, whether it\\'s in\\nstock, on backorder, or sold out>\\\\`\\n●\\nLicensing Options: \\\\`<If you offer licensing options for your artwork, include a\\nbrief description and pricing information for available licenses>\\\\`\\n●\\nInquiries: \\\\`<Provide contact information for sales inquiries, such as an email\\naddress, contact form, or phone number>\\\\`\\nExhibitions and Projects\\n●\\nExhibition Dates: \\\\`<List details of any upcoming exhibitions or events\\nfeaturing your artwork>\\\\`\\nWork Presentation\\n●\\nEditions and Variants: `<Details of any different editions, variants, or versions\\nof your artwork and how they differ from the original>`\\n●\\nInstallation Instructions: `<Provide instructions on how to properly install,\\ndisplay, or interact with your artwork, if applicable>`\\n●\\nMaintenance and Preservation: `<Recommendations for maintaining and\\npreserving your artwork, including proper handling, storage, and cleaning\\nprocedures>`\\n●\\nEdition Details: `<For editioned works, provide information on edition size,\\nnumbering, availability, and pricing>`\\n●\\nAvailability and Pricing: `<Provide information on the availability and pricing of\\nyour artwork, including any information on discounts, promotions, or special\\noffers>`\\nAdditional Supporting Materials\\n○\\nImages: `<Provide high-resolution images or speciﬁcations for the\\nartwork, including detailed shots or images showing the artwork in\\ndifferent contexts>`\\n○\\nVideo: `<If applicable, provide a link to a video showcasing the artwork\\nand/or the process of creating it>`\\n○\\nAudio: `<If applicable, provide a link to an audio component that is part\\nof or complements the artwork>`\\n○\\nAdditional Materials: `<Provide links to supporting materials, such as\\nvideo or audio documentation, digital versions of your artwork, critical\\nreviews, or related research>`\\n○\\nAccessibility Info: `<Provide any relevant accessibility information, such\\nas image alt text, video/audio transcripts, or ways viewers with\\ndisabilities can engage with your artwork>`\\nCopyright and Licensing Information\\n●\\nCopyright: `<Indicate the copyright holder(s) and any relevant information\\nregarding copyright ownership, e.g., \"© 2021 Your Name. All rights reserved.\">`\\n●\\nLicensing: `<Specify any terms and conditions regarding how the artwork can\\nbe used, reproduced, or distributed, e.g., \"This artwork is licensed under the\\nCreative Commons Attribution-NonCommercial-NoDerivatives 4.0\\nInternational License. Link:\\n[http://creativecommons.org/licenses/by-nc-nd/4.0/]\")>`\\n●\\nUsage Permissions: `<Clearly state permissions and restrictions on how the\\nartwork may be used or shared, including permission for educational,\\ncommercial, or public display purposes>`\\n●\\nContact Information: `<Provide contact information for- Inquiries: \\\\`<Provide\\nan email address or other method for viewers, collectors, or institutions to\\ncontact you with any questions, requests, or inquiries about the artwork, e.g.,\\n\"Please direct all inquiries to\\n[email@example.com](mailto:email@example.com)\">\\\\`\\n●\\nSocial Media: \\\\`<List any social media platforms where your artwork is\\nshowcased or where followers can stay updated on your work, e.g., \"Follow\\nme on Instagram:\\n[https://www.instagram.com/yourusername](https://www.instagram.com/you\\nrusername)\">\\\\`\\n●\\nImage Credits: `<Provide appropriate image credits for photos, videos, or other\\nmedia used within your artwork or metadata, including the name of the\\nphotographer or creator and a link to their webpage if applicable>`\\nArt Criteria and Parameters\\nFor each criterion, provide a brief statement that addresses how your artwork meets\\nthe requirements:\\n●\\nAesthetic Quality: \\\\`<Statement about the aesthetic quality of your artwork>\\\\`\\n●\\nConceptual Relevance: \\\\`<Statement about the conceptual relevance of your\\nartwork>\\\\`\\n●\\nTechnological Innovation: \\\\`<Statement about the technological innovation of\\nyour artwork>\\\\`\\n●\\nInterdisciplinary Approach: \\\\`<Statement about the interdisciplinary approach\\nof your artwork>\\\\`\\n●\\nAudience Interaction: \\\\`<Statement about the audience interaction with your\\nartwork>\\\\`\\n●\\nInteractive Elements: `<If applicable, describe any interactive features or\\ncomponents in the artwork>`\\n●\\naesthetic quality: \\\\`<Describe the aesthetic qualities of your artwork, such as\\nharmony, balance, contrast, or rhythm, and explain how these features\\ncontribute to the overall visual appeal and impact>\\\\`\\n●\\nconceptual relevance: `<Describe the conceptual relevance of your artwork,\\nsuch as how it addresses current issues, challenges conventions, or connects\\nto broader artistic or cultural themes>`\\n●\\ntechnological innovation: \\\\`<Explain the types of technology or technological\\ninnovation used in the creation, presentation, or promotion of your artwork,\\nand how this enhances its creative or conceptual value>\\\\`\\n●\\ninterdisciplinary approach:\\\\`<Describe the interdisciplinary approach taken in\\nyour artwork, such as how it combines various disciplines, mediums, or\\nperspectives to create a unique and innovative artistic experience>\\\\`\\n●\\naudience interaction: `\\\\`<Explain how your artwork encourages audience\\ninteraction, whether through physical engagement, emotional connection, or\\nintellectual stimulation, and discuss the intended impact or response you\\nhope to elicit from viewers>\\\\``\\n●\\nInteractive Elements:`<List any interactive elements of your artwork, such as\\ntouchscreens, sensors, or augmented reality features, and describe their\\npurpose or function in the context of the work>\\\\`\\n●\\nAccessibility: \\\\`<Explain how you have considered accessibility in your\\nartwork, such as accommodating viewers with disabilities or ensuring the\\nwork is inclusive and engaging for a diverse audience>\\\\`\\n●\\nCritical Reception: \\\\`<Describe how you feel about any critical response your\\nartwork has received, including reviews, press coverage, commentary on\\nsocial media, or notable discussions among art professionals>\\\\`\\n●\\nContextual Analysis: \\\\`<Provide an analysis of your artwork within the context\\nof your artistic practice, your inﬂuences, and the contemporary art world>\\\\`\\n●\\nChallenges Overcome: \\\\`<Discuss any challenges or obstacles you faced\\nduring the creation of your artwork, and how you addressed or resolved\\nthem>\\\\`\\n●\\nPersonal Signiﬁcance: \\\\`<Share any personal or emotional signiﬁcance your\\nartwork holds for you,such as its connection to your life experiences, personal\\ngrowth, or self-reﬂection, and how this has informed the work\\'s meaning and\\nimpact>```\\n●\\nFuture Plans: \\\\`<Outline any plans or ambitions you have for the future of this\\nartwork, including potential exhibitions, collaborations, or expansions on its\\nthemes or concepts>```\\nArtistic Questions\\n●\\nThemes and Concepts: `<What themes or concepts drive your artwork\\'s\\ncreation?>`\\n●\\nArt Historical Context: `<Does your work engage with or contribute to the\\ngreater art historical context?>`\\n●\\nCultural, Historical, and Societal Inﬂuences:`<Are there speciﬁc cultural,\\nhistorical, or societal inﬂuences that have shaped your art practice?>`\\n●\\nTheory and Philosophy: `<What theories or philosophies inform your artistic\\npractice and artwork?>`\\n●\\nArtistic Practice Evolution: `<How does your artistic practice evolve or change\\nover time? Is there a speciﬁc trajectory that you can trace (e.g., collaboration\\nwith other artists, using new technologies)?>`\\n●\\nTechniques, Materials, and Processes: `<What speciﬁc techniques, materials,\\nor processes have you employed in your work, and how do these choices\\ncontribute to its overall meaning or effect?>`\\n●\\nAudience Interaction: `<In what ways does your artwork invite audience\\ninteraction, and how critical is this interaction to the overall experience or\\ninterpretation of the piece?>`\\n●\\nInﬂuences and Inspirations: `<Can you provide any examples of other artists\\nor artworks that have inﬂuenced your practice or approach to your work?>`\\n●\\nCurrent Art Landscape: `<How do you see your artwork ﬁtting within the\\ncurrent art landscape or engaging in larger art-related conversations?>`\\n●\\nChallenges and Obstacles: `<Are there any challenges or obstacles you faced\\nwhile creating this artwork, and how did these experiences shape the ﬁnal\\npiece?>`\\n●\\nMessage and Intention: Is there a message or intention behind your artwork\\nthat you hope to communicate to your audience, and how do you hope they\\nengage with it?\\n●\\nInterdisciplinary Approaches: Does your artwork incorporate any\\ninterdisciplinary approaches, such as collaborations with professionals from\\ndifferent ﬁelds, research in various disciplines, or the application of knowledge\\nfrom multiple sources?\\n●\\nFuture Projects and Collaborations: Are there any upcoming projects,\\nexhibitions, or collaborations that you are currently working on or plan to work\\non in the future? If so, how do these endeavors connect to your existing body\\nof work?\\n●\\nArtwork Understanding: How has your understanding of your artwork shifted\\nor evolved throughout the creation process, and what insights have you\\ngained as a result?\\n●\\nProvoking Conversation: In what ways do you hope your artwork provokes\\nconversation, sparks debate, or inspires thought among your audience?\\n●\\nArtistic Practice Future: How do you envision the future of your artistic\\npractice, and what areas or themes do you hope to explore moving forward?\\n●\\nCommunity and Network Support: Are there any communities, organizations,\\nor networks that have supported you or your work, and how have these\\nconnections impacted your artistic practice and experiences?\\nUser Proﬁle\\nType: <Which 3 User Types do you want to reach with your art?\\nIdentify the top 3 user types of your art that your work is\\nmost likely to attract, such as Museum Director, Urban\\nPlanner, Galerist, Art Therapist, Art Collector, Journalist,\\nGallery, A Specific Collection, Museum, Art Critic, Art\\nResearcher, etc. This helps in targeting your marketing\\nefforts and understanding your audience\\'s preferences and\\nneeds.>\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_text(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'OpenAI'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m VectorStore \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mfrom_texts(chunks, embedding\u001b[38;5;241m=\u001b[39membeddings)\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pydantic\\v1\\main.py:339\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mCreate a new model by parsing and validating input data from keyword arguments.\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03mRaises ValidationError if the input data cannot be parsed to form a valid model.\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;66;03m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[39;00m\n\u001b[1;32m--> 339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__pydantic_self__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pydantic\\v1\\main.py:1102\u001b[0m, in \u001b[0;36mvalidate_model\u001b[1;34m(model, input_data, cls)\u001b[0m\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1102\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAssertionError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m   1104\u001b[0m     errors\u001b[38;5;241m.\u001b[39mappend(ErrorWrapper(exc, loc\u001b[38;5;241m=\u001b[39mROOT_KEY))\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\embeddings\\openai.py:350\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.validate_environment\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    339\u001b[0m client_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai_api_key\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morganization\u001b[39m\u001b[38;5;124m\"\u001b[39m: values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai_organization\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    348\u001b[0m }\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m values\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 350\u001b[0m     values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpenAI\u001b[49m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclient_params)\u001b[38;5;241m.\u001b[39membeddings\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m values\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masync_client\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    352\u001b[0m     values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masync_client\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mAsyncOpenAI(\n\u001b[0;32m    353\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclient_params\n\u001b[0;32m    354\u001b[0m     )\u001b[38;5;241m.\u001b[39membeddings\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'openai' has no attribute 'OpenAI'"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "VectorStore = FAISS.from_texts(chunks, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
